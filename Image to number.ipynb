{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "KZmNRi243MPG",
    "outputId": "bbdd504e-85eb-4166-9993-ac57c61e8573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvy5H8h5xapu"
   },
   "source": [
    "Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "QnlHocd_32BT",
    "outputId": "68c163f9-5a27-40fc-d37e-8455aa4b8da4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pt\n",
    "import seaborn as sns\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TstmxAnE4nR_"
   },
   "outputs": [],
   "source": [
    "h5f=h5py.File('/content/drive/My Drive/SVHN_single_grey1.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl1NPyOq5SyQ"
   },
   "outputs": [],
   "source": [
    "X_train= h5f['X_train'][:]\n",
    "Y_train=h5f['y_train'][:]\n",
    "\n",
    "X_val= h5f['X_val'][:]\n",
    "Y_val=h5f['y_val'][:]\n",
    "\n",
    "X_test= h5f['X_test'][:]\n",
    "Y_test=h5f['y_test'][:]\n",
    "\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdSiZ0_K6e5V"
   },
   "source": [
    "## Flattening the images for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3sBRAqCE602s",
    "outputId": "e311a7aa-9464-4fd1-aeae-9e48ab03f30d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 32, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFhehp6q6cR4"
   },
   "outputs": [],
   "source": [
    "fX_train=X_train.reshape(X_train.shape[0],1024)\n",
    "fX_test=X_test.reshape(X_test.shape[0],1024)\n",
    "fX_val=X_val.reshape(X_val.shape[0],1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jg7lecI07qK7"
   },
   "source": [
    "# Normalising the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRXIwl4z7NKl"
   },
   "outputs": [],
   "source": [
    "nfX_train=fX_train/255.0\n",
    "nfX_test=fX_test/255.0\n",
    "nfX_val=fX_val/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dn0yadeR8Co5"
   },
   "source": [
    "# Converting Class metric to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KamRdYwl8B3r"
   },
   "outputs": [],
   "source": [
    "HY_train=tf.keras.utils.to_categorical(Y_train,num_classes=10)\n",
    "HY_test=tf.keras.utils.to_categorical(Y_test,num_classes=10)\n",
    "HY_val=tf.keras.utils.to_categorical(Y_val,num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSHKaidd83uO"
   },
   "source": [
    "# Printing train,test,val size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "vwwa8nnV73Uh",
    "outputId": "a9f5cc36-8db1-436c-9480-0cc0a3716f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set after flattening (42000, 1024)\n",
      "number of samples in train set 42000\n",
      "Size of a image in train set (32, 32)\n",
      "Size of test set after flattening (18000, 1024)\n",
      "number of samples in test set 18000\n",
      "Size of a image in test set (32, 32)\n",
      "Size of Validaiton set after flattening (60000, 1024)\n",
      "number of samples in Validation set 60000\n",
      "Size of a image in Validation set (32, 32)\n"
     ]
    }
   ],
   "source": [
    "print('Size of train set after flattening',nfX_train.shape)\n",
    "print('number of samples in train set',X_train.shape[0])\n",
    "print('Size of a image in train set',X_train.shape[1:])\n",
    "\n",
    "print('Size of test set after flattening',nfX_test.shape)\n",
    "print('number of samples in test set',X_test.shape[0])\n",
    "print('Size of a image in test set',X_test.shape[1:])\n",
    "\n",
    "print('Size of Validaiton set after flattening',nfX_val.shape)\n",
    "print('number of samples in Validation set',X_val.shape[0])\n",
    "print('Size of a image in Validation set',X_val.shape[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WT6-cNNYxiKA"
   },
   "source": [
    "Here the size of validation dataset is higher than the training dataser, which is usually not the case. Training dataset will be higher than validation. \n",
    "First, lets train the model for the given dataset and then as new trial lets combine the train and val set and the split it in 70:30 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hA6bGwyR-cI3"
   },
   "source": [
    "Visualising first 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "colab_type": "code",
    "id": "ecfsdW729dy_",
    "outputId": "ce9cd9c5-07ea-4675-a6e5-d0f57f7f7e52"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('label for each of the above image: %s' % (Y_train[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZelryrG_UBJ"
   },
   "source": [
    "The images are not clear. Images contain multiple numbers. Model has to learn to classify the image to the number which is more predominant (clear). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GrPoJwyANUA"
   },
   "source": [
    "# Creating a Keras Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48TbD1wpPROu"
   },
   "outputs": [],
   "source": [
    "def train_val_loop(X,y,Xval,yval,n,act,epoch,verb=True,LR=0.01,L2=0,opti='sgd',m=0):\n",
    "    \n",
    "  Learning_rate=LR\n",
    "  neurons=n\n",
    "  Regularize=L2\n",
    "  Activation=act\n",
    "  momentum=m\n",
    "\n",
    "  modelx=tf.keras.models.Sequential()\n",
    "  modelx.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "  for i in range(0,len(neurons)):\n",
    "    modelx.add(tf.keras.layers.Dense(n[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "    \n",
    "  if opti=='sgd':\n",
    "    Optimizer = tf.keras.optimizers.SGD(lr=Learning_rate,momentum=m)\n",
    "  if opti=='adam':\n",
    "    Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "    \n",
    "  modelx.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  modelx.fit(X,y,validation_data=(Xval,yval),batch_size=30,epochs=epoch,verbose=verb)\n",
    "    \n",
    "  return modelx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "GhRYWREe6oNW",
    "outputId": "698b1dd2-296d-4795-d7e5-352aa6faccb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "42000/42000 [==============================] - 7s 178us/sample - loss: 2.6091 - acc: 0.0990 - val_loss: 2.5795 - val_acc: 0.0995\n"
     ]
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[10,10],['relu','softmax'],1,True,0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlUTuQJK6oNb"
   },
   "source": [
    "The accuracy is 9% and output class is 10 so it seems somewhat reasonable model. and Loss is 2.6 which is also reasonable. so model makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRauf1N26oNd"
   },
   "source": [
    "Testing by setting high value for lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "x_plg43g6oNf",
    "outputId": "3516baa7-607f-4496-8fc9-538d9c0543ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 537.5397 - acc: 0.1003 - val_loss: 2.3026 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[10,10],['relu','softmax'],1,True,0.00001,1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzcTvV9V6oNm"
   },
   "source": [
    "As known with high value of l2 regularizer the loss increases to very high value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YtebLEJ6oNo"
   },
   "source": [
    "# Over fit on a small batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2B-xfhp6oNp"
   },
   "outputs": [],
   "source": [
    "nfX_train=nfX_train[:20]\n",
    "HY_train=HY_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SHe8DA0E6oNs",
    "outputId": "a39bff7e-30e3-4630-e336-1420e6aa9597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 3.4214 - acc: 0.1000 - val_loss: 2.5091 - val_acc: 0.1000\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 3.4094 - acc: 0.1000 - val_loss: 2.5042 - val_acc: 0.1000\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.3975 - acc: 0.1000 - val_loss: 2.4993 - val_acc: 0.1000\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.3856 - acc: 0.1000 - val_loss: 2.4946 - val_acc: 0.1000\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 3.3739 - acc: 0.1000 - val_loss: 2.4901 - val_acc: 0.1000\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 4s 195ms/sample - loss: 3.3622 - acc: 0.1000 - val_loss: 2.4856 - val_acc: 0.1000\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.3506 - acc: 0.1000 - val_loss: 2.4812 - val_acc: 0.1000\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 4s 196ms/sample - loss: 3.3391 - acc: 0.1000 - val_loss: 2.4770 - val_acc: 0.1000\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.3276 - acc: 0.1000 - val_loss: 2.4729 - val_acc: 0.1001\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 4s 196ms/sample - loss: 3.3163 - acc: 0.1000 - val_loss: 2.4689 - val_acc: 0.1001\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.3050 - acc: 0.1000 - val_loss: 2.4650 - val_acc: 0.1000\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.2939 - acc: 0.1000 - val_loss: 2.4612 - val_acc: 0.1000\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.2828 - acc: 0.1000 - val_loss: 2.4575 - val_acc: 0.0999\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.2719 - acc: 0.1000 - val_loss: 2.4539 - val_acc: 0.0998\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.2610 - acc: 0.1000 - val_loss: 2.4504 - val_acc: 0.0998\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.2503 - acc: 0.1000 - val_loss: 2.4470 - val_acc: 0.0998\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 3.2397 - acc: 0.1000 - val_loss: 2.4436 - val_acc: 0.0999\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.2291 - acc: 0.1000 - val_loss: 2.4404 - val_acc: 0.0999\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 3.2186 - acc: 0.1000 - val_loss: 2.4372 - val_acc: 0.0999\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.2083 - acc: 0.1000 - val_loss: 2.4342 - val_acc: 0.0997\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 3.1981 - acc: 0.1000 - val_loss: 2.4312 - val_acc: 0.0998\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 3.1879 - acc: 0.1000 - val_loss: 2.4283 - val_acc: 0.0998\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.1779 - acc: 0.1000 - val_loss: 2.4255 - val_acc: 0.0998\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 4s 203ms/sample - loss: 3.1679 - acc: 0.1000 - val_loss: 2.4228 - val_acc: 0.0997\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 3.1580 - acc: 0.1000 - val_loss: 2.4201 - val_acc: 0.0996\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 3.1482 - acc: 0.1000 - val_loss: 2.4175 - val_acc: 0.0998\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 4s 204ms/sample - loss: 3.1384 - acc: 0.1000 - val_loss: 2.4150 - val_acc: 0.0998\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 4s 208ms/sample - loss: 3.1288 - acc: 0.1000 - val_loss: 2.4125 - val_acc: 0.0997\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 4s 203ms/sample - loss: 3.1192 - acc: 0.1000 - val_loss: 2.4101 - val_acc: 0.0997\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 3.1098 - acc: 0.1000 - val_loss: 2.4078 - val_acc: 0.0999\n"
     ]
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[10,10],['relu','softmax'],30,True,0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I295KY8h6oNw"
   },
   "source": [
    "Model is not overfitting. Lets try increasing the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YVa-TzDq6oNx",
    "outputId": "b650f5f2-b753-489e-92ae-f19e74ba9cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 4s 191ms/sample - loss: 2.6388 - acc: 0.1000 - val_loss: 2.4301 - val_acc: 0.1028\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 4s 187ms/sample - loss: 2.3620 - acc: 0.1500 - val_loss: 2.3881 - val_acc: 0.1040\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 4s 188ms/sample - loss: 2.1952 - acc: 0.1500 - val_loss: 2.3953 - val_acc: 0.1029\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 4s 188ms/sample - loss: 2.0603 - acc: 0.1000 - val_loss: 2.3845 - val_acc: 0.1020\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 4s 192ms/sample - loss: 1.9620 - acc: 0.2000 - val_loss: 2.3793 - val_acc: 0.1021\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 1.8741 - acc: 0.4000 - val_loss: 2.3763 - val_acc: 0.1023\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 4s 189ms/sample - loss: 1.8083 - acc: 0.4500 - val_loss: 2.3802 - val_acc: 0.1014\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 4s 190ms/sample - loss: 1.7470 - acc: 0.4500 - val_loss: 2.3690 - val_acc: 0.1032\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 4s 194ms/sample - loss: 1.6933 - acc: 0.4000 - val_loss: 2.3699 - val_acc: 0.1014\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 4s 190ms/sample - loss: 1.6491 - acc: 0.4500 - val_loss: 2.3828 - val_acc: 0.1032\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 4s 192ms/sample - loss: 1.5982 - acc: 0.5000 - val_loss: 2.3720 - val_acc: 0.1036\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 4s 188ms/sample - loss: 1.5451 - acc: 0.6000 - val_loss: 2.3766 - val_acc: 0.1007\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 4s 190ms/sample - loss: 1.5122 - acc: 0.6000 - val_loss: 2.3745 - val_acc: 0.1045\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 4s 193ms/sample - loss: 1.4737 - acc: 0.6000 - val_loss: 2.3761 - val_acc: 0.1015\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 4s 190ms/sample - loss: 1.4359 - acc: 0.6000 - val_loss: 2.3723 - val_acc: 0.1039\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 4s 195ms/sample - loss: 1.4072 - acc: 0.6500 - val_loss: 2.3757 - val_acc: 0.1022\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 4s 188ms/sample - loss: 1.3737 - acc: 0.6000 - val_loss: 2.3780 - val_acc: 0.1033\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 4s 192ms/sample - loss: 1.3427 - acc: 0.7000 - val_loss: 2.3749 - val_acc: 0.1020\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 4s 189ms/sample - loss: 1.3207 - acc: 0.7000 - val_loss: 2.3827 - val_acc: 0.1019\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 4s 191ms/sample - loss: 1.2950 - acc: 0.7000 - val_loss: 2.3747 - val_acc: 0.1039\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 4s 188ms/sample - loss: 1.2718 - acc: 0.7500 - val_loss: 2.3778 - val_acc: 0.1016\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 4s 199ms/sample - loss: 1.2469 - acc: 0.7000 - val_loss: 2.3794 - val_acc: 0.1026\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 1.2220 - acc: 0.7000 - val_loss: 2.3850 - val_acc: 0.1026\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 4s 191ms/sample - loss: 1.2019 - acc: 0.7500 - val_loss: 2.3816 - val_acc: 0.1037\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 4s 196ms/sample - loss: 1.1854 - acc: 0.8000 - val_loss: 2.3826 - val_acc: 0.1016\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 4s 188ms/sample - loss: 1.1616 - acc: 0.8000 - val_loss: 2.3846 - val_acc: 0.1025\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 4s 194ms/sample - loss: 1.1401 - acc: 0.8500 - val_loss: 2.3868 - val_acc: 0.1036\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 4s 189ms/sample - loss: 1.1206 - acc: 0.8500 - val_loss: 2.3889 - val_acc: 0.1035\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 4s 191ms/sample - loss: 1.1029 - acc: 0.8500 - val_loss: 2.3864 - val_acc: 0.1018\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 4s 190ms/sample - loss: 1.0868 - acc: 0.8500 - val_loss: 2.3903 - val_acc: 0.1019\n"
     ]
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[10,10],['relu','softmax'],30,True,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYcAMnSd6oN0"
   },
   "source": [
    "training accuracy has but still not good enough. lets change the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jMHcEcKH6oN1",
    "outputId": "8b7b013e-229d-4b0c-eb11-c0ffe4e66284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 4s 218ms/sample - loss: 2.5585 - acc: 0.1500 - val_loss: 2.3595 - val_acc: 0.0997\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 4s 202ms/sample - loss: 2.1985 - acc: 0.2000 - val_loss: 2.3674 - val_acc: 0.0997\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 4s 203ms/sample - loss: 1.9328 - acc: 0.4500 - val_loss: 2.3740 - val_acc: 0.0994\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 4s 203ms/sample - loss: 1.7292 - acc: 0.4500 - val_loss: 2.3797 - val_acc: 0.0996\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 1.5597 - acc: 0.5500 - val_loss: 2.3859 - val_acc: 0.0994\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 1.4119 - acc: 0.6500 - val_loss: 2.3916 - val_acc: 0.0991\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 4s 208ms/sample - loss: 1.2834 - acc: 0.7500 - val_loss: 2.3966 - val_acc: 0.0988\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 4s 202ms/sample - loss: 1.1814 - acc: 0.7500 - val_loss: 2.4025 - val_acc: 0.0991\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 4s 206ms/sample - loss: 1.0906 - acc: 0.8000 - val_loss: 2.4086 - val_acc: 0.1004\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 4s 208ms/sample - loss: 1.0108 - acc: 0.9000 - val_loss: 2.4134 - val_acc: 0.1013\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 4s 203ms/sample - loss: 0.9407 - acc: 0.9000 - val_loss: 2.4191 - val_acc: 0.1026\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 4s 197ms/sample - loss: 0.8780 - acc: 0.9000 - val_loss: 2.4248 - val_acc: 0.1034\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 0.8224 - acc: 0.9000 - val_loss: 2.4302 - val_acc: 0.1042\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 4s 198ms/sample - loss: 0.7723 - acc: 0.9000 - val_loss: 2.4344 - val_acc: 0.1050\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 0.7276 - acc: 0.9500 - val_loss: 2.4388 - val_acc: 0.1063\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 0.6870 - acc: 0.9500 - val_loss: 2.4432 - val_acc: 0.1075\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 0.6514 - acc: 0.9500 - val_loss: 2.4473 - val_acc: 0.1080\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 4s 208ms/sample - loss: 0.6182 - acc: 1.0000 - val_loss: 2.4507 - val_acc: 0.1082\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 4s 204ms/sample - loss: 0.5881 - acc: 1.0000 - val_loss: 2.4554 - val_acc: 0.1088\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 0.5603 - acc: 1.0000 - val_loss: 2.4585 - val_acc: 0.1088\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 4s 210ms/sample - loss: 0.5345 - acc: 1.0000 - val_loss: 2.4618 - val_acc: 0.1091\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 0.5103 - acc: 1.0000 - val_loss: 2.4651 - val_acc: 0.1095\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 4s 200ms/sample - loss: 0.4880 - acc: 1.0000 - val_loss: 2.4674 - val_acc: 0.1097\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 4s 201ms/sample - loss: 0.4675 - acc: 1.0000 - val_loss: 2.4707 - val_acc: 0.1102\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 4s 199ms/sample - loss: 0.4475 - acc: 1.0000 - val_loss: 2.4733 - val_acc: 0.1102\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 4s 212ms/sample - loss: 0.4293 - acc: 1.0000 - val_loss: 2.4757 - val_acc: 0.1101\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 4s 210ms/sample - loss: 0.4121 - acc: 1.0000 - val_loss: 2.4787 - val_acc: 0.1107\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 4s 207ms/sample - loss: 0.3962 - acc: 1.0000 - val_loss: 2.4805 - val_acc: 0.1108\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 4s 202ms/sample - loss: 0.3809 - acc: 1.0000 - val_loss: 2.4829 - val_acc: 0.1109\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 4s 204ms/sample - loss: 0.3666 - acc: 1.0000 - val_loss: 2.4848 - val_acc: 0.1112\n"
     ]
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,True,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "putwBqcp6oN6"
   },
   "source": [
    "The above architecture seems good enough but lets try agian by increasing the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1CCbERN36oN8",
    "outputId": "8082318c-9d3b-4244-abd4-07716a9a7d45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "20/20 [==============================] - 5s 225ms/sample - loss: 2.3435 - acc: 0.1500 - val_loss: 2.3150 - val_acc: 0.1037\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 5s 228ms/sample - loss: 2.2085 - acc: 0.2000 - val_loss: 2.3173 - val_acc: 0.1036\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 5s 230ms/sample - loss: 2.1072 - acc: 0.3000 - val_loss: 2.3194 - val_acc: 0.1026\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 5s 226ms/sample - loss: 2.0243 - acc: 0.4500 - val_loss: 2.3214 - val_acc: 0.1020\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 5s 229ms/sample - loss: 1.9522 - acc: 0.5000 - val_loss: 2.3240 - val_acc: 0.1012\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 5s 227ms/sample - loss: 1.8854 - acc: 0.5000 - val_loss: 2.3264 - val_acc: 0.1009\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 4s 218ms/sample - loss: 1.8191 - acc: 0.5000 - val_loss: 2.3299 - val_acc: 0.0996\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 4s 224ms/sample - loss: 1.7554 - acc: 0.6000 - val_loss: 2.3333 - val_acc: 0.0992\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 4s 223ms/sample - loss: 1.6942 - acc: 0.6000 - val_loss: 2.3366 - val_acc: 0.0976\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 4s 216ms/sample - loss: 1.6350 - acc: 0.7000 - val_loss: 2.3407 - val_acc: 0.0972\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 4s 211ms/sample - loss: 1.5797 - acc: 0.8000 - val_loss: 2.3443 - val_acc: 0.0964\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 4s 212ms/sample - loss: 1.5268 - acc: 0.8500 - val_loss: 2.3477 - val_acc: 0.0958\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 4s 219ms/sample - loss: 1.4764 - acc: 0.8500 - val_loss: 2.3510 - val_acc: 0.0956\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 4s 217ms/sample - loss: 1.4302 - acc: 0.9000 - val_loss: 2.3549 - val_acc: 0.0953\n",
      "Epoch 15/30\n",
      "20/20 [==============================] - 4s 211ms/sample - loss: 1.3842 - acc: 0.9000 - val_loss: 2.3589 - val_acc: 0.0948\n",
      "Epoch 16/30\n",
      "20/20 [==============================] - 4s 213ms/sample - loss: 1.3442 - acc: 0.9000 - val_loss: 2.3612 - val_acc: 0.0944\n",
      "Epoch 17/30\n",
      "20/20 [==============================] - 4s 206ms/sample - loss: 1.3034 - acc: 0.9000 - val_loss: 2.3650 - val_acc: 0.0940\n",
      "Epoch 18/30\n",
      "20/20 [==============================] - 4s 207ms/sample - loss: 1.2675 - acc: 0.9500 - val_loss: 2.3689 - val_acc: 0.0935\n",
      "Epoch 19/30\n",
      "20/20 [==============================] - 4s 218ms/sample - loss: 1.2304 - acc: 0.9500 - val_loss: 2.3714 - val_acc: 0.0931\n",
      "Epoch 20/30\n",
      "20/20 [==============================] - 4s 208ms/sample - loss: 1.1965 - acc: 1.0000 - val_loss: 2.3749 - val_acc: 0.0929\n",
      "Epoch 21/30\n",
      "20/20 [==============================] - 4s 209ms/sample - loss: 1.1631 - acc: 1.0000 - val_loss: 2.3775 - val_acc: 0.0926\n",
      "Epoch 22/30\n",
      "20/20 [==============================] - 4s 213ms/sample - loss: 1.1309 - acc: 1.0000 - val_loss: 2.3821 - val_acc: 0.0926\n",
      "Epoch 23/30\n",
      "20/20 [==============================] - 4s 209ms/sample - loss: 1.0996 - acc: 1.0000 - val_loss: 2.3837 - val_acc: 0.0920\n",
      "Epoch 24/30\n",
      "20/20 [==============================] - 4s 215ms/sample - loss: 1.0695 - acc: 1.0000 - val_loss: 2.3870 - val_acc: 0.0918\n",
      "Epoch 25/30\n",
      "20/20 [==============================] - 4s 211ms/sample - loss: 1.0397 - acc: 1.0000 - val_loss: 2.3907 - val_acc: 0.0913\n",
      "Epoch 26/30\n",
      "20/20 [==============================] - 4s 210ms/sample - loss: 1.0102 - acc: 1.0000 - val_loss: 2.3939 - val_acc: 0.0913\n",
      "Epoch 27/30\n",
      "20/20 [==============================] - 4s 210ms/sample - loss: 0.9825 - acc: 1.0000 - val_loss: 2.3972 - val_acc: 0.0911\n",
      "Epoch 28/30\n",
      "20/20 [==============================] - 4s 216ms/sample - loss: 0.9551 - acc: 1.0000 - val_loss: 2.4002 - val_acc: 0.0910\n",
      "Epoch 29/30\n",
      "20/20 [==============================] - 4s 214ms/sample - loss: 0.9281 - acc: 1.0000 - val_loss: 2.4037 - val_acc: 0.0913\n",
      "Epoch 30/30\n",
      "20/20 [==============================] - 4s 222ms/sample - loss: 0.9002 - acc: 1.0000 - val_loss: 2.4071 - val_acc: 0.0910\n"
     ]
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,\n",
    "             [250,200,100,50,10],\n",
    "             ['relu','relu','relu','relu','softmax'],30,True,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zp13AWNs6oN_"
   },
   "source": [
    "It seems model takes higher number of iterations to learn than the previous trial, so lets stick with 3 hidden layers with 200,100,10 neurons after fine tuning we will train this architecture also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9VpaFOTq6oOA"
   },
   "source": [
    "# Reloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFtd2AIH6oOB"
   },
   "outputs": [],
   "source": [
    "h5f=h5py.File('/content/drive/My Drive/SVHN_single_grey1.h5','r')\n",
    "\n",
    "X_train= h5f['X_train'][:]\n",
    "Y_train=h5f['y_train'][:]\n",
    "\n",
    "X_val= h5f['X_val'][:]\n",
    "Y_val=h5f['y_val'][:]\n",
    "\n",
    "X_test= h5f['X_test'][:]\n",
    "Y_test=h5f['y_test'][:]\n",
    "\n",
    "h5f.close()\n",
    "\n",
    "fX_train=X_train.reshape(X_train.shape[0],1024)\n",
    "fX_test=X_test.reshape(X_test.shape[0],1024)\n",
    "fX_val=X_val.reshape(X_val.shape[0],1024)\n",
    "\n",
    "nfX_train=fX_train/255.0\n",
    "nfX_test=fX_test/255.0\n",
    "nfX_val=fX_val/255.0\n",
    "\n",
    "HY_train=tf.keras.utils.to_categorical(Y_train,num_classes=10)\n",
    "HY_test=tf.keras.utils.to_categorical(Y_test,num_classes=10)\n",
    "HY_val=tf.keras.utils.to_categorical(Y_val,num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KIqTGFS-6oOK"
   },
   "source": [
    "Seems good. Lets try with lower value of learning rate and Lambda\n",
    "\n",
    "Learning rate : 1e-7\n",
    "\n",
    "Lamda : 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1zNNK2J26oOM",
    "outputId": "a6cb4b87-75e3-41f1-bf72-6b6eb1a23c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.8387 - acc: 0.1042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8386967637379965, 0.104166664]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,False,1e-7,1e-7)\n",
    "modeln.evaluate(nfX_val,HY_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HMRxIFP_yTq"
   },
   "source": [
    "Loss and accuracy are both low lets try increasing Learning rate a little\n",
    "\n",
    "Learning rate : 1e-4\n",
    "\n",
    "Lamda : 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hlUvpUeM6oOQ",
    "outputId": "8657c684-0277-443a-b0b3-f580eddd6daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.7271 - acc: 0.4873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7270950929323832, 0.48731667]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,False,1e-4,1e-7)\n",
    "modeln.evaluate(nfX_val,HY_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t436OjqBTJS"
   },
   "source": [
    "Lets try high value of Learning rate\n",
    "\n",
    "Learning rate : 1e2\n",
    "\n",
    "Lamda : 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "341ab2DWBDkY",
    "outputId": "83d8bfbe-1b16-4faa-a674-6eb1c9783320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 66us/sample - loss: nan - acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan, 0.1]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeln=train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,False,1e2,1e-7)\n",
    "modeln.evaluate(nfX_val,HY_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98NVBd9TClA3"
   },
   "source": [
    "SO the loss increase to nan\n",
    "\n",
    "lets do a coarse search with\n",
    "\n",
    " learning rate : 1e-4 to 1e2\n",
    "\n",
    " Lambda : 1e-5 to 1e5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Rg0ITCOV6oOZ",
    "outputId": "fb9a49e3-ba46-41d7-8540-518008068e55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 65us/sample - loss: 5.5826 - acc: 0.3651\n",
      "Try 1/50: Best_val_acc: [5.582575669606527, 0.3651], lr: 0.00018236554827874956, Lambda: 0.08208476810007129\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 2.7264 - acc: 0.5694\n",
      "Try 2/50: Best_val_acc: [2.7263506132125856, 0.56943333], lr: 0.00013652024075847173, Lambda: 0.002598472054511446\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 3/50: Best_val_acc: [2.302613067626953, 0.1], lr: 0.0013934480587437861, Lambda: 1.6209805004140698\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.5815 - acc: 0.8931\n",
      "Try 4/50: Best_val_acc: [0.5814547545353571, 0.89306664], lr: 0.011526808765254073, Lambda: 0.0005513519848926906\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: nan - acc: 0.1000\n",
      "Try 5/50: Best_val_acc: [nan, 0.1], lr: 0.40490510954579106, Lambda: 6.441866544870449\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 3.3155 - acc: 0.6948\n",
      "Try 6/50: Best_val_acc: [3.3154658849080403, 0.69475], lr: 0.00031338243404281516, Lambda: 0.005831499001223418\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 7/50: Best_val_acc: [2.3026291370391845, 0.1], lr: 0.010988923763944858, Lambda: 20.437243388410398\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: nan - acc: 0.1000\n",
      "Try 8/50: Best_val_acc: [nan, 0.1], lr: 5.783685382149125, Lambda: 0.0013191764522656876\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 9/50: Best_val_acc: [2.302625250752767, 0.1], lr: 0.010485798657269549, Lambda: 33.450673909938786\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 2.6264 - acc: 0.2450\n",
      "Try 10/50: Best_val_acc: [2.626438991292318, 0.24501666], lr: 0.00036972040771911696, Lambda: 0.07843127920314584\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.8685 - acc: 0.8195\n",
      "Try 11/50: Best_val_acc: [0.8684945532957713, 0.81953335], lr: 0.09225072871795116, Lambda: 0.0014709329612046715\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 2.3404 - acc: 0.1000\n",
      "Try 12/50: Best_val_acc: [2.340351271692912, 0.1], lr: 2.548290896844544, Lambda: 0.12382118325133963\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: nan - acc: 0.1000\n",
      "Try 13/50: Best_val_acc: [nan, 0.1], lr: 0.09366103942745126, Lambda: 3224.512635044212\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.9482 - acc: 0.8524\n",
      "Try 14/50: Best_val_acc: [0.9482490743954977, 0.8524], lr: 0.00882082553891952, Lambda: 0.005491642885289802\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: nan - acc: 0.1000\n",
      "Try 15/50: Best_val_acc: [nan, 0.1], lr: 0.17828256963465441, Lambda: 152.9460248736379\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 16/50: Best_val_acc: [2.3026120423634846, 0.1], lr: 0.001143322428212759, Lambda: 42.949616229930974\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: nan - acc: 0.1000\n",
      "Try 17/50: Best_val_acc: [nan, 0.1], lr: 0.05076885759360406, Lambda: 6828.400143457713\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 18/50: Best_val_acc: [2.3026143074035645, 0.1], lr: 0.002941403997288959, Lambda: 17.35243281999259\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.3027 - acc: 0.1000\n",
      "Try 19/50: Best_val_acc: [2.302696561686198, 0.1], lr: 0.034885042380964314, Lambda: 2.369138689157597\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.3027 - acc: 0.1000\n",
      "Try 20/50: Best_val_acc: [2.302655196126302, 0.1], lr: 0.033312463528507405, Lambda: 1.3091577506500103\n",
      "\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 2.3028 - acc: 0.1000\n",
      "Try 21/50: Best_val_acc: [2.3027606488545738, 0.1], lr: 0.031638706680395624, Lambda: 29.4042287963998\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.8041 - acc: 0.7364\n",
      "Try 22/50: Best_val_acc: [2.8040765740712486, 0.7363833], lr: 0.0005353690644861731, Lambda: 0.005829528839129173\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4836 - acc: 0.8920\n",
      "Try 23/50: Best_val_acc: [0.48362372338374454, 0.89203334], lr: 0.006940065079565686, Lambda: 0.00017661213276959155\n",
      "\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: nan - acc: 0.1000\n",
      "Try 24/50: Best_val_acc: [nan, 0.1], lr: 8.176123721338925, Lambda: 8.182161283462502\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: nan - acc: 0.1000\n",
      "Try 25/50: Best_val_acc: [nan, 0.1], lr: 0.0019251225428635564, Lambda: 1131.4672249647733\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 26/50: Best_val_acc: [2.3026104688008626, 0.1], lr: 0.003813498907503152, Lambda: 14.023652037977106\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: nan - acc: 0.1000\n",
      "Try 27/50: Best_val_acc: [nan, 0.1], lr: 0.00048364203599132597, Lambda: 7702.250902703878\n",
      "\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: nan - acc: 0.1000\n",
      "Try 28/50: Best_val_acc: [nan, 0.1], lr: 1.941349904896551, Lambda: 43.2462737751694\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: nan - acc: 0.1000\n",
      "Try 29/50: Best_val_acc: [nan, 0.1], lr: 0.38507738597902263, Lambda: 4.954364493085995\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.3034 - acc: 0.1000\n",
      "Try 30/50: Best_val_acc: [2.3034478427886964, 0.1], lr: 0.1068708470261377, Lambda: 2.360713501172824\n",
      "\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 31/50: Best_val_acc: [2.3026114701588947, 0.1], lr: 0.000912717665248352, Lambda: 155.02905271908216\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: nan - acc: 0.1000\n",
      "Try 32/50: Best_val_acc: [nan, 0.1], lr: 9.907239572920073, Lambda: 1442.2358137243493\n",
      "\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: nan - acc: 0.1000\n",
      "Try 33/50: Best_val_acc: [nan, 0.1], lr: 0.6361749871542751, Lambda: 2051.0046627152933\n",
      "\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: nan - acc: 0.1000\n",
      "Try 34/50: Best_val_acc: [nan, 0.1], lr: 0.004238401622598359, Lambda: 1594.927973071677\n",
      "\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: nan - acc: 0.1000\n",
      "Try 35/50: Best_val_acc: [nan, 0.1], lr: 0.007702314986672354, Lambda: 1081.405680818118\n",
      "\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 36/50: Best_val_acc: [2.302603244781494, 0.1], lr: 0.00037961842131089635, Lambda: 9.480441069304602\n",
      "\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 1.6195 - acc: 0.5602\n",
      "Try 37/50: Best_val_acc: [1.6194705744743347, 0.5602], lr: 0.0001406423119235599, Lambda: 0.00019758196537733697\n",
      "\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 38/50: Best_val_acc: [2.3026127338409426, 0.1], lr: 0.005050694145343268, Lambda: 0.6976935220559846\n",
      "\n",
      "60000/60000 [==============================] - 5s 75us/sample - loss: 3.4430 - acc: 0.4950\n",
      "Try 39/50: Best_val_acc: [3.4430097807566327, 0.495], lr: 0.00011032997132101512, Lambda: 0.0037961633331017506\n",
      "\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 40/50: Best_val_acc: [2.302609348297119, 0.1], lr: 0.0046334480214851965, Lambda: 0.2919938803097479\n",
      "\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 1.6714 - acc: 0.5615\n",
      "Try 41/50: Best_val_acc: [1.6713683421134948, 0.5614667], lr: 0.0001318277208306362, Lambda: 0.00030307032318691366\n",
      "\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: nan - acc: 0.1000\n",
      "Try 42/50: Best_val_acc: [nan, 0.1], lr: 0.0048177913724631646, Lambda: 1512.4796085997398\n",
      "\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: nan - acc: 0.1000\n",
      "Try 43/50: Best_val_acc: [nan, 0.1], lr: 0.015841121846760536, Lambda: 4952.977742223917\n",
      "\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 44/50: Best_val_acc: [2.3025904893239337, 0.1], lr: 0.0001358025695877294, Lambda: 557.8783268842066\n",
      "\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: nan - acc: 0.1000\n",
      "Try 45/50: Best_val_acc: [nan, 0.1], lr: 1.11399265913758, Lambda: 2.193807171388372\n",
      "\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 2.3035 - acc: 0.1000\n",
      "Try 46/50: Best_val_acc: [2.303518223698934, 0.1], lr: 0.20737575102148345, Lambda: 0.13503294801514693\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.6853 - acc: 0.8734\n",
      "Try 47/50: Best_val_acc: [0.6853366031805674, 0.87338334], lr: 0.01205979034024989, Lambda: 0.00123942778392558\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: nan - acc: 0.1000\n",
      "Try 48/50: Best_val_acc: [nan, 0.1], lr: 0.5844916008611084, Lambda: 113.57126685822598\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 2.3087 - acc: 0.1000\n",
      "Try 49/50: Best_val_acc: [2.30865178120931, 0.1], lr: 0.9600349271222604, Lambda: 0.16915592111462394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,50):\n",
    "    lr = math.pow(10, np.random.uniform(-4.0, 1.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-4,4))\n",
    "    modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,False,lr,Lambda)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 50, modeln.evaluate(nfX_val,HY_val), lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcA1oBdF6oOd"
   },
   "source": [
    "The results shows that accuracy is poor for large values of Lambda and Lr\n",
    "\n",
    "The good results with accuracy above 80% appears for following range of values\n",
    "\n",
    "Learning rate: 0.0071 to 0.0922\n",
    "\n",
    "Lambda: 0.00017 to 0.0015 -\n",
    "\n",
    "now since the lambda is at the extreme of our limits lets try extending it further\n",
    "\n",
    "\n",
    "perform trial for \n",
    "\n",
    "Learning rate =0.0001 to 0.1\n",
    "\n",
    "Lambda- 1e-8 to 1e-2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CF-MohuV6oOg",
    "outputId": "87684735-5321-40fa-9264-3fa093236f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 1.5600 - acc: 0.5494\n",
      "Try 1/50: Best_val_acc: [1.5599735478719075, 0.54943335], lr: 0.00012746000867255232, Lambda: 7.877776769805744e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4182 - acc: 0.8849\n",
      "Try 2/50: Best_val_acc: [0.41822672241131464, 0.88491666], lr: 0.0037585550281953287, Lambda: 1.0579592552310723e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.6327 - acc: 0.8275\n",
      "Try 3/50: Best_val_acc: [0.6326905809322994, 0.82746667], lr: 0.0011812811132555516, Lambda: 3.693093510946086e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.6961 - acc: 0.8789\n",
      "Try 4/50: Best_val_acc: [0.6960772668202718, 0.8788667], lr: 0.010054973560337707, Lambda: 0.0015458167184027672\n",
      "\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.8254 - acc: 0.7644\n",
      "Try 5/50: Best_val_acc: [0.8254190704425176, 0.7644167], lr: 0.0005688351427929636, Lambda: 1.37526775309031e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4721 - acc: 0.8732\n",
      "Try 6/50: Best_val_acc: [0.47212725770076114, 0.8731667], lr: 0.00290437857789559, Lambda: 4.521928206204944e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.0811 - acc: 0.6927\n",
      "Try 7/50: Best_val_acc: [1.081112012910843, 0.69265], lr: 0.0002878882643796186, Lambda: 2.6424319036379913e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3408 - acc: 0.9040\n",
      "Try 8/50: Best_val_acc: [0.34076913526852926, 0.90396667], lr: 0.04495475546443569, Lambda: 3.023278464593094e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.4214 - acc: 0.8801\n",
      "Try 9/50: Best_val_acc: [0.421357650154829, 0.8801], lr: 0.003537246697008306, Lambda: 8.499912690858392e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3676 - acc: 0.8963\n",
      "Try 10/50: Best_val_acc: [0.36760998877684276, 0.89625], lr: 0.03166610734396441, Lambda: 3.1546557245432217e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.4315 - acc: 0.8745\n",
      "Try 11/50: Best_val_acc: [0.4314677702456713, 0.8745], lr: 0.0039807660985430315, Lambda: 1.2937379377417326e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4490 - acc: 0.8782\n",
      "Try 12/50: Best_val_acc: [0.44900428326527275, 0.8782333], lr: 0.0034657594436330688, Lambda: 3.364983059840469e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4023 - acc: 0.8842\n",
      "Try 13/50: Best_val_acc: [0.40233662222226463, 0.88415], lr: 0.006527347895496612, Lambda: 2.831674295828049e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.5247 - acc: 0.8513\n",
      "Try 14/50: Best_val_acc: [0.5246916782041391, 0.8512667], lr: 0.0018428903690486703, Lambda: 9.07377830256956e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.9756 - acc: 0.8252\n",
      "Try 15/50: Best_val_acc: [0.9756371601422628, 0.82523334], lr: 0.0011697367722052728, Lambda: 0.0007401078663726215\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3383 - acc: 0.9069\n",
      "Try 16/50: Best_val_acc: [0.338298037258784, 0.9069333], lr: 0.012902796262771575, Lambda: 6.40957976271091e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 1.0279 - acc: 0.7069\n",
      "Try 17/50: Best_val_acc: [1.0278939976851145, 0.7068667], lr: 0.00033539194945590185, Lambda: 2.6664377442237186e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.8511 - acc: 0.8219\n",
      "Try 18/50: Best_val_acc: [0.851107648007075, 0.82185], lr: 0.06514921906456529, Lambda: 0.0016066639989937152\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 3.6720 - acc: 0.7006\n",
      "Try 19/50: Best_val_acc: [3.671957897567749, 0.70063335], lr: 0.00035239007690658934, Lambda: 0.00791242093313379\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 1.0366 - acc: 0.7048\n",
      "Try 20/50: Best_val_acc: [1.0365812834103902, 0.7047667], lr: 0.000339148136815745, Lambda: 1.2457166386488624e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 1.6797 - acc: 0.4854\n",
      "Try 21/50: Best_val_acc: [1.6797371521313986, 0.4854], lr: 0.00010399484404422729, Lambda: 5.156186397659796e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3856 - acc: 0.8883\n",
      "Try 22/50: Best_val_acc: [0.38559273134966693, 0.8882667], lr: 0.0799275073576541, Lambda: 1.4421678340179892e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.6839 - acc: 0.8048\n",
      "Try 23/50: Best_val_acc: [0.6839027131716411, 0.80475], lr: 0.0008939041582025948, Lambda: 8.493768784186601e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4047 - acc: 0.8845\n",
      "Try 24/50: Best_val_acc: [0.4046600222001473, 0.88448334], lr: 0.0051301003962691445, Lambda: 3.9360301881133254e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.7279 - acc: 0.7929\n",
      "Try 25/50: Best_val_acc: [0.7279372111479442, 0.79285], lr: 0.0007894188976564456, Lambda: 3.5896006817563007e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.5153 - acc: 0.8558\n",
      "Try 26/50: Best_val_acc: [0.5153476849595706, 0.8558], lr: 0.001896581500763608, Lambda: 4.842917877938902e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4226 - acc: 0.8843\n",
      "Try 27/50: Best_val_acc: [0.4226469209929307, 0.8843333], lr: 0.00381225104344791, Lambda: 1.5771245340909405e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3845 - acc: 0.8909\n",
      "Try 28/50: Best_val_acc: [0.38449919010897476, 0.89085], lr: 0.005450482332314263, Lambda: 1.2070099098014293e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3684 - acc: 0.8964\n",
      "Try 29/50: Best_val_acc: [0.36838499605456987, 0.89635], lr: 0.010230007057870492, Lambda: 1.0070927630054942e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 1.2022 - acc: 0.6610\n",
      "Try 30/50: Best_val_acc: [1.202237831290563, 0.6609667], lr: 0.00022625647014162494, Lambda: 3.654555523360189e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.8253 - acc: 0.7646\n",
      "Try 31/50: Best_val_acc: [0.8252901004473369, 0.76463336], lr: 0.0005933087989328078, Lambda: 4.626097299805178e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.4269 - acc: 0.8952\n",
      "Try 32/50: Best_val_acc: [0.4268717782855034, 0.8951667], lr: 0.009536221761107471, Lambda: 9.206423128215408e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.5266 - acc: 0.8493\n",
      "Try 33/50: Best_val_acc: [0.5265997226377328, 0.84925], lr: 0.0017339810765991462, Lambda: 1.8468367096308277e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.6718 - acc: 0.8597\n",
      "Try 34/50: Best_val_acc: [0.6718107948541642, 0.85975], lr: 0.058466841885823226, Lambda: 0.0006777499696392032\n",
      "\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 1.0325 - acc: 0.7985\n",
      "Try 35/50: Best_val_acc: [1.0324617441972097, 0.7985333], lr: 0.033170171932451184, Lambda: 0.005317391531046035\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3765 - acc: 0.9001\n",
      "Try 36/50: Best_val_acc: [0.37649006032943727, 0.9001], lr: 0.04806045704296777, Lambda: 2.0920432574612262e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3890 - acc: 0.8895\n",
      "Try 37/50: Best_val_acc: [0.3889691035831968, 0.88945], lr: 0.005653987986022514, Lambda: 2.503148311463392e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 1.5274 - acc: 0.8306\n",
      "Try 38/50: Best_val_acc: [1.5274469313939412, 0.83056664], lr: 0.0014424827559423553, Lambda: 0.0031682866936063856\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.5169 - acc: 0.8915\n",
      "Try 39/50: Best_val_acc: [0.5169467686891556, 0.8914833], lr: 0.04043821699725648, Lambda: 0.0002451035417065245\n",
      "\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.8541 - acc: 0.7620\n",
      "Try 40/50: Best_val_acc: [0.8541360755284627, 0.76203334], lr: 0.0005351716334072132, Lambda: 3.604820626456413e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3823 - acc: 0.8918\n",
      "Try 41/50: Best_val_acc: [0.38228827470143634, 0.8918167], lr: 0.008838662745014561, Lambda: 1.909516559606197e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 72us/sample - loss: 1.5758 - acc: 0.5500\n",
      "Try 42/50: Best_val_acc: [1.5757607467651367, 0.55], lr: 0.00012337422768887013, Lambda: 7.056997168388278e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 75us/sample - loss: 0.8029 - acc: 0.8385\n",
      "Try 43/50: Best_val_acc: [0.802875959444046, 0.8385], lr: 0.07425812950704992, Lambda: 0.00133236562844232\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.1416 - acc: 0.7205\n",
      "Try 44/50: Best_val_acc: [2.1415644241333007, 0.7205333], lr: 0.0003838005803083248, Lambda: 0.00266256291369182\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.4262 - acc: 0.8794\n",
      "Try 45/50: Best_val_acc: [0.42615017606317995, 0.8793833], lr: 0.003685020373245454, Lambda: 1.4440037847099888e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3458 - acc: 0.9012\n",
      "Try 46/50: Best_val_acc: [0.3457884439125657, 0.9012], lr: 0.04231330106332872, Lambda: 3.4692027888786875e-06\n",
      "\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 1.6228 - acc: 0.5202\n",
      "Try 47/50: Best_val_acc: [1.6227793269475301, 0.52021664], lr: 0.00012174387966567727, Lambda: 4.4787821821011717e-07\n",
      "\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.4235 - acc: 0.8986\n",
      "Try 48/50: Best_val_acc: [0.4235187349001567, 0.89856666], lr: 0.010227300091283698, Lambda: 0.0001062857777420201\n",
      "\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 1.0979 - acc: 0.7788\n",
      "Try 49/50: Best_val_acc: [1.0979059336662294, 0.77875], lr: 0.06291094277905543, Lambda: 0.005299249092014404\n",
      "\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.3782 - acc: 0.8914\n",
      "Try 50/50: Best_val_acc: [0.37817273418505987, 0.89141667], lr: 0.08023671793328936, Lambda: 2.9423775327847665e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,51):\n",
    "    lr = math.pow(10, np.random.uniform(-4.0, -1.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-8,-2))\n",
    "    modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,False,lr,Lambda)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 50, modeln.evaluate(nfX_val,HY_val), lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RBpHBXAMuSNu"
   },
   "source": [
    "In this trial, none of the accuracy is nan, thus we are nearing the optimum value of parameters\n",
    "\n",
    "The good results with accuracy above 88% appears for following range of values\n",
    "\n",
    "Learning rate: 0.0037 to 0.048\n",
    "\n",
    "Lambda: 1.2e-8 to 2.4e-4\n",
    "\n",
    "Lambda being small shows the good validation results occur without regularization\n",
    "\n",
    "now since the lambda is at the extreme of our limits lets try extending it further\n",
    "\n",
    "\n",
    "perform trial for \n",
    "\n",
    "Learning rate =0.002 to 0.07\n",
    "\n",
    "Lambda- 1e-8 to 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3bYMJuZA6oOl",
    "outputId": "6f0334e2-421f-478c-cc32-0d7ee7552c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.3564 - acc: 0.8978\n",
      "Try 1/50: Best_val_acc: [0.3563851696369549, 0.89775], lr: 0.04448501148744529, Lambda: 8.027830774253326e-08\n",
      "\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.4001 - acc: 0.8995\n",
      "Try 2/50: Best_val_acc: [0.40009393161733947, 0.8994667], lr: 0.015257939807517535, Lambda: 6.590125393019313e-05\n",
      "\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.3481 - acc: 0.9023\n",
      "Try 3/50: Best_val_acc: [0.34814802435437836, 0.9023167], lr: 0.023340518147244096, Lambda: 7.075706764578177e-06\n",
      "\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.3403 - acc: 0.9053\n",
      "Try 4/50: Best_val_acc: [0.34028420534804465, 0.90525], lr: 0.017993295039670988, Lambda: 2.596859177971537e-08\n",
      "\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3565 - acc: 0.9005\n",
      "Try 5/50: Best_val_acc: [0.3565365746145447, 0.9005], lr: 0.028607119359984924, Lambda: 6.336395158149822e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4081 - acc: 0.8990\n",
      "Try 6/50: Best_val_acc: [0.40806150199572244, 0.89898336], lr: 0.03866089443831144, Lambda: 5.7747557938466534e-05\n",
      "\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3493 - acc: 0.9030\n",
      "Try 7/50: Best_val_acc: [0.3492894647469123, 0.9030333], lr: 0.02694584098961323, Lambda: 7.660872351784323e-06\n",
      "\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.3414 - acc: 0.9047\n",
      "Try 8/50: Best_val_acc: [0.34143737191458545, 0.9047167], lr: 0.010291033850951845, Lambda: 5.558293230353899e-07\n",
      "\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.3602 - acc: 0.8987\n",
      "Try 9/50: Best_val_acc: [0.36018563911716145, 0.8987], lr: 0.04642248702957914, Lambda: 6.9293262639528316e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3676 - acc: 0.8959\n",
      "Try 10/50: Best_val_acc: [0.3675689134309689, 0.8958667], lr: 0.007470715001219441, Lambda: 1.8943454990002185e-07\n",
      "\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3625 - acc: 0.8951\n",
      "Try 11/50: Best_val_acc: [0.3625136092076699, 0.8951333], lr: 0.05704597891982703, Lambda: 9.16264487713913e-07\n",
      "\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.4006 - acc: 0.9025\n",
      "Try 12/50: Best_val_acc: [0.4006030668417613, 0.90246665], lr: 0.05818102603572802, Lambda: 4.8505775310446104e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4071 - acc: 0.8842\n",
      "Try 13/50: Best_val_acc: [0.4071172043502331, 0.88421667], lr: 0.004196797855905195, Lambda: 3.421898051672242e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3528 - acc: 0.9002\n",
      "Try 14/50: Best_val_acc: [0.35279376767973103, 0.90015], lr: 0.02674773584932115, Lambda: 5.754589415973677e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3650 - acc: 0.8941\n",
      "Try 15/50: Best_val_acc: [0.3650343383312225, 0.8941333], lr: 0.026780189601119136, Lambda: 5.16432683580821e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.3473 - acc: 0.9040\n",
      "Try 16/50: Best_val_acc: [0.34731759846607846, 0.90405], lr: 0.0355009467083957, Lambda: 6.846955130599591e-06\n",
      "\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3693 - acc: 0.8960\n",
      "Try 17/50: Best_val_acc: [0.3693321255708734, 0.896], lr: 0.007083159409626541, Lambda: 8.138953346553148e-07\n",
      "\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.3796 - acc: 0.8995\n",
      "Try 18/50: Best_val_acc: [0.3796237045228481, 0.8994667], lr: 0.04825635120279068, Lambda: 2.1543665857248804e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3586 - acc: 0.8987\n",
      "Try 19/50: Best_val_acc: [0.358611340106527, 0.8986667], lr: 0.03921627436197928, Lambda: 4.859267210498225e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3525 - acc: 0.8990\n",
      "Try 20/50: Best_val_acc: [0.35249963931192957, 0.8990167], lr: 0.05884799517546448, Lambda: 9.666078306082137e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3342 - acc: 0.9064\n",
      "Try 21/50: Best_val_acc: [0.3341783120224873, 0.90641665], lr: 0.05013530722759909, Lambda: 1.4991270605627184e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.4644 - acc: 0.8908\n",
      "Try 22/50: Best_val_acc: [0.4644449460705121, 0.8908333], lr: 0.048135136318387235, Lambda: 9.892380772424536e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.3575 - acc: 0.9007\n",
      "Try 23/50: Best_val_acc: [0.35751793698370454, 0.90075], lr: 0.025850643502586132, Lambda: 4.962436780196552e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4901 - acc: 0.8626\n",
      "Try 24/50: Best_val_acc: [0.49012716945409773, 0.86263335], lr: 0.0023064795025653285, Lambda: 1.1296881509291356e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3605 - acc: 0.9036\n",
      "Try 25/50: Best_val_acc: [0.3605358115792274, 0.90363336], lr: 0.017215691473484628, Lambda: 2.6029384563630182e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.3555 - acc: 0.8996\n",
      "Try 26/50: Best_val_acc: [0.3555213847974936, 0.89961666], lr: 0.012768813433382384, Lambda: 1.6434011235290171e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.3733 - acc: 0.8950\n",
      "Try 27/50: Best_val_acc: [0.3733357175141573, 0.89496666], lr: 0.007048154362756358, Lambda: 3.7945887106372216e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3702 - acc: 0.8917\n",
      "Try 28/50: Best_val_acc: [0.3701525518561403, 0.89171666], lr: 0.059140251202744736, Lambda: 1.3242341356702467e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3520 - acc: 0.9036\n",
      "Try 29/50: Best_val_acc: [0.3520451459566752, 0.90361667], lr: 0.01097700598089899, Lambda: 1.0175503313321129e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.4275 - acc: 0.8974\n",
      "Try 30/50: Best_val_acc: [0.4274841786424319, 0.89735], lr: 0.05024429549061549, Lambda: 6.431307697396836e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3297 - acc: 0.9086\n",
      "Try 31/50: Best_val_acc: [0.3297086927284797, 0.90863335], lr: 0.019560946340911864, Lambda: 4.715676601623284e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3333 - acc: 0.9066\n",
      "Try 32/50: Best_val_acc: [0.3333251600029568, 0.9066], lr: 0.036349806391140646, Lambda: 2.0666163639372535e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3718 - acc: 0.8937\n",
      "Try 33/50: Best_val_acc: [0.3717741188878814, 0.8936833], lr: 0.05344576991392806, Lambda: 3.9161084761104846e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.3691 - acc: 0.8940\n",
      "Try 34/50: Best_val_acc: [0.36905575380598504, 0.8940167], lr: 0.06491917781254615, Lambda: 6.203134034076461e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3454 - acc: 0.9012\n",
      "Try 35/50: Best_val_acc: [0.34535719923277697, 0.9012167], lr: 0.045216505902848786, Lambda: 1.438964310078901e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3544 - acc: 0.9000\n",
      "Try 36/50: Best_val_acc: [0.35436109225700296, 0.89996666], lr: 0.04276595716607943, Lambda: 3.618227273505658e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4512 - acc: 0.8717\n",
      "Try 37/50: Best_val_acc: [0.4512114234606425, 0.87168336], lr: 0.003037425270146282, Lambda: 1.9116925321102027e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3552 - acc: 0.9030\n",
      "Try 38/50: Best_val_acc: [0.35518599539001783, 0.9030333], lr: 0.02510535860218429, Lambda: 1.3140134448810112e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.3658 - acc: 0.8954\n",
      "Try 39/50: Best_val_acc: [0.3658196974496047, 0.8954333], lr: 0.02565968865490205, Lambda: 2.931365325986567e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.3964 - acc: 0.8846\n",
      "Try 40/50: Best_val_acc: [0.39641211373209956, 0.8846], lr: 0.04463965029520104, Lambda: 1.235892239919377e-06\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3905 - acc: 0.8914\n",
      "Try 41/50: Best_val_acc: [0.3904884127954642, 0.89141667], lr: 0.007755158276521033, Lambda: 1.2146956617918464e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.4109 - acc: 0.8917\n",
      "Try 42/50: Best_val_acc: [0.4108747829318047, 0.8917], lr: 0.04809714496524455, Lambda: 3.0133303386112752e-05\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.3602 - acc: 0.8957\n",
      "Try 43/50: Best_val_acc: [0.36022458728129664, 0.89571667], lr: 0.06491254224194778, Lambda: 1.42421325327955e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3596 - acc: 0.8958\n",
      "Try 44/50: Best_val_acc: [0.35958624222328267, 0.8958333], lr: 0.04892565690292906, Lambda: 1.263955296785538e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3949 - acc: 0.8846\n",
      "Try 45/50: Best_val_acc: [0.39489274839907884, 0.8846], lr: 0.05853526185193507, Lambda: 1.390328252135639e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3536 - acc: 0.9008\n",
      "Try 46/50: Best_val_acc: [0.3535805776466926, 0.9008333], lr: 0.014939897843279815, Lambda: 2.193323140835789e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3483 - acc: 0.9002\n",
      "Try 47/50: Best_val_acc: [0.3483376033162077, 0.90021664], lr: 0.034794198419867355, Lambda: 6.690549443766851e-07\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3309 - acc: 0.9074\n",
      "Try 48/50: Best_val_acc: [0.3308740406945348, 0.90743333], lr: 0.019575798977839336, Lambda: 1.2410199202153769e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.3456 - acc: 0.9021\n",
      "Try 49/50: Best_val_acc: [0.34557336103518804, 0.90206665], lr: 0.03828107366881363, Lambda: 1.604712003487642e-08\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.4241 - acc: 0.8930\n",
      "Try 50/50: Best_val_acc: [0.4241262144843737, 0.893], lr: 0.06635387718266593, Lambda: 4.7880624900119723e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "for k in range(1,51):\n",
    "    lr = np.random.uniform(0.002, 0.07)\n",
    "    Lambda = math.pow(10, np.random.uniform(-8,-4))\n",
    "    modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],30,False,lr,Lambda)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 50, modeln.evaluate(nfX_val,HY_val), lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUCmGAoxG3nG"
   },
   "source": [
    "Nearly for all the values of learning rate and Lamda in the above specified range accuracy is higher than 88 % (except one)\n",
    "\n",
    "Now we have identified the range of values. Meaning the loss curve is Flat in this zone\n",
    "\n",
    "High Validation accuracy of 90.8 % occurs for\n",
    "learning rate: 0.019560946340911864\n",
    "Lambda: 4.715676601623284e-07\n",
    "\n",
    "Lamda being very small means. no need of regularization\n",
    "\n",
    "Lets increase the number of epochs and train the model further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-MVkawTM6oOv",
    "outputId": "54bd4964-f6d6-475e-fbd0-53bff92cd514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 10s 244us/sample - loss: 1.4340 - acc: 0.5289 - val_loss: 0.9727 - val_acc: 0.7003\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.9828 - acc: 0.6925 - val_loss: 0.7572 - val_acc: 0.7749\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.8496 - acc: 0.7364 - val_loss: 0.6624 - val_acc: 0.8046\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.7716 - acc: 0.7622 - val_loss: 0.6158 - val_acc: 0.8195\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.7236 - acc: 0.7757 - val_loss: 0.5882 - val_acc: 0.8283\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.6794 - acc: 0.7890 - val_loss: 0.5494 - val_acc: 0.8387\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.6439 - acc: 0.8003 - val_loss: 0.5235 - val_acc: 0.8449\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.6249 - acc: 0.8055 - val_loss: 0.5211 - val_acc: 0.8459\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.5973 - acc: 0.8129 - val_loss: 0.4866 - val_acc: 0.8572\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.5795 - acc: 0.8205 - val_loss: 0.4564 - val_acc: 0.8679\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.5597 - acc: 0.8256 - val_loss: 0.4544 - val_acc: 0.8675\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.5444 - acc: 0.8290 - val_loss: 0.4755 - val_acc: 0.8589\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5357 - acc: 0.8316 - val_loss: 0.4447 - val_acc: 0.8681\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.5162 - acc: 0.8369 - val_loss: 0.4414 - val_acc: 0.8696\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5062 - acc: 0.8403 - val_loss: 0.4280 - val_acc: 0.8749\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.4967 - acc: 0.8434 - val_loss: 0.4172 - val_acc: 0.8798\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4870 - acc: 0.8470 - val_loss: 0.4206 - val_acc: 0.8774\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4718 - acc: 0.8505 - val_loss: 0.3836 - val_acc: 0.8893\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4700 - acc: 0.8504 - val_loss: 0.3858 - val_acc: 0.8887\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.4608 - acc: 0.8527 - val_loss: 0.3836 - val_acc: 0.8893\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.4463 - acc: 0.8586 - val_loss: 0.3732 - val_acc: 0.8949\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4414 - acc: 0.8590 - val_loss: 0.3837 - val_acc: 0.8889\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.4349 - acc: 0.8592 - val_loss: 0.3593 - val_acc: 0.8956\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4321 - acc: 0.8615 - val_loss: 0.3654 - val_acc: 0.8963\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.4221 - acc: 0.8655 - val_loss: 0.3587 - val_acc: 0.8978\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.4191 - acc: 0.8667 - val_loss: 0.3512 - val_acc: 0.9003\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4059 - acc: 0.8709 - val_loss: 0.3430 - val_acc: 0.9025\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.4054 - acc: 0.8692 - val_loss: 0.3373 - val_acc: 0.9048\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.3975 - acc: 0.8721 - val_loss: 0.3513 - val_acc: 0.9004\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.3942 - acc: 0.8722 - val_loss: 0.3608 - val_acc: 0.8967\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.3946 - acc: 0.8738 - val_loss: 0.3352 - val_acc: 0.9053\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.3836 - acc: 0.8760 - val_loss: 0.3300 - val_acc: 0.9071\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.3827 - acc: 0.8773 - val_loss: 0.3220 - val_acc: 0.9087\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.3718 - acc: 0.8810 - val_loss: 0.3389 - val_acc: 0.9047\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.3715 - acc: 0.8795 - val_loss: 0.3204 - val_acc: 0.9108\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.3640 - acc: 0.8808 - val_loss: 0.3147 - val_acc: 0.9122\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.3669 - acc: 0.8822 - val_loss: 0.3196 - val_acc: 0.9113\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.3582 - acc: 0.8841 - val_loss: 0.3228 - val_acc: 0.9089\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.3560 - acc: 0.8851 - val_loss: 0.3079 - val_acc: 0.9156\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.3561 - acc: 0.8841 - val_loss: 0.3207 - val_acc: 0.9104\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.3519 - acc: 0.8860 - val_loss: 0.3145 - val_acc: 0.9137\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.3465 - acc: 0.8884 - val_loss: 0.3237 - val_acc: 0.9108\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.3400 - acc: 0.8892 - val_loss: 0.3063 - val_acc: 0.9155\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.3423 - acc: 0.8888 - val_loss: 0.3064 - val_acc: 0.9163\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.3323 - acc: 0.8907 - val_loss: 0.3046 - val_acc: 0.9169\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.3331 - acc: 0.8909 - val_loss: 0.2914 - val_acc: 0.9209\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.3246 - acc: 0.8928 - val_loss: 0.3284 - val_acc: 0.9096\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.3277 - acc: 0.8933 - val_loss: 0.2919 - val_acc: 0.9230\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.3288 - acc: 0.8926 - val_loss: 0.2799 - val_acc: 0.9262\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.3214 - acc: 0.8948 - val_loss: 0.3077 - val_acc: 0.9182\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.3214 - acc: 0.8950 - val_loss: 0.2956 - val_acc: 0.9216\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.3138 - acc: 0.8975 - val_loss: 0.3159 - val_acc: 0.9140\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.3123 - acc: 0.8982 - val_loss: 0.2918 - val_acc: 0.9220\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.3152 - acc: 0.8970 - val_loss: 0.2891 - val_acc: 0.9235\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.3087 - acc: 0.8978 - val_loss: 0.2992 - val_acc: 0.9191\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.3118 - acc: 0.8962 - val_loss: 0.2854 - val_acc: 0.9253\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.3030 - acc: 0.9009 - val_loss: 0.2757 - val_acc: 0.9279\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.3009 - acc: 0.9004 - val_loss: 0.2929 - val_acc: 0.9239\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.3060 - acc: 0.8989 - val_loss: 0.2861 - val_acc: 0.9247\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2997 - acc: 0.9006 - val_loss: 0.2883 - val_acc: 0.9249\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2964 - acc: 0.9024 - val_loss: 0.2832 - val_acc: 0.9260\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.2965 - acc: 0.9021 - val_loss: 0.2812 - val_acc: 0.9259\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2988 - acc: 0.9021 - val_loss: 0.2795 - val_acc: 0.9279\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2875 - acc: 0.9067 - val_loss: 0.2804 - val_acc: 0.9265\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2844 - acc: 0.9071 - val_loss: 0.2839 - val_acc: 0.9267\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.2848 - acc: 0.9061 - val_loss: 0.2815 - val_acc: 0.9275\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.2906 - acc: 0.9037 - val_loss: 0.2975 - val_acc: 0.9213\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.2801 - acc: 0.9084 - val_loss: 0.2749 - val_acc: 0.9301\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.2834 - acc: 0.9062 - val_loss: 0.2691 - val_acc: 0.9317\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.2775 - acc: 0.9097 - val_loss: 0.2701 - val_acc: 0.9330\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2849 - acc: 0.9046 - val_loss: 0.3033 - val_acc: 0.9217\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.2757 - acc: 0.9082 - val_loss: 0.2690 - val_acc: 0.9330\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.2767 - acc: 0.9077 - val_loss: 0.2752 - val_acc: 0.9302\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.2687 - acc: 0.9115 - val_loss: 0.2709 - val_acc: 0.9321\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.2725 - acc: 0.9098 - val_loss: 0.2750 - val_acc: 0.9301\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2633 - acc: 0.9135 - val_loss: 0.2760 - val_acc: 0.9313\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2716 - acc: 0.9089 - val_loss: 0.2763 - val_acc: 0.9306\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2681 - acc: 0.9120 - val_loss: 0.2619 - val_acc: 0.9363\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2661 - acc: 0.9133 - val_loss: 0.2671 - val_acc: 0.9341\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2651 - acc: 0.9115 - val_loss: 0.2748 - val_acc: 0.9315\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.2652 - acc: 0.9124 - val_loss: 0.2635 - val_acc: 0.9357\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2579 - acc: 0.9144 - val_loss: 0.2713 - val_acc: 0.9318\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2677 - acc: 0.9110 - val_loss: 0.2672 - val_acc: 0.9346\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.2584 - acc: 0.9147 - val_loss: 0.2673 - val_acc: 0.9342\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2586 - acc: 0.9147 - val_loss: 0.2661 - val_acc: 0.9357\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2618 - acc: 0.9133 - val_loss: 0.2728 - val_acc: 0.9329\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2567 - acc: 0.9153 - val_loss: 0.2744 - val_acc: 0.9333\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2522 - acc: 0.9160 - val_loss: 0.2613 - val_acc: 0.9376\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2547 - acc: 0.9160 - val_loss: 0.2797 - val_acc: 0.9327\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2497 - acc: 0.9170 - val_loss: 0.2652 - val_acc: 0.9356\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2533 - acc: 0.9167 - val_loss: 0.2716 - val_acc: 0.9331\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2506 - acc: 0.9162 - val_loss: 0.2616 - val_acc: 0.9381\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2467 - acc: 0.9191 - val_loss: 0.2559 - val_acc: 0.9394\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2488 - acc: 0.9180 - val_loss: 0.2603 - val_acc: 0.9379\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2444 - acc: 0.9193 - val_loss: 0.2629 - val_acc: 0.9366\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2504 - acc: 0.9178 - val_loss: 0.2563 - val_acc: 0.9396\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2438 - acc: 0.9203 - val_loss: 0.2804 - val_acc: 0.9329\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2392 - acc: 0.9211 - val_loss: 0.2591 - val_acc: 0.9386\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2471 - acc: 0.9175 - val_loss: 0.2823 - val_acc: 0.9305\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.2452 - acc: 0.9193 - val_loss: 0.2597 - val_acc: 0.9390\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2490 - acc: 0.9183 - val_loss: 0.2637 - val_acc: 0.9384\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2406 - acc: 0.9198 - val_loss: 0.2642 - val_acc: 0.9361\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2364 - acc: 0.9225 - val_loss: 0.2587 - val_acc: 0.9400\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2393 - acc: 0.9206 - val_loss: 0.2607 - val_acc: 0.9382\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2387 - acc: 0.9208 - val_loss: 0.2566 - val_acc: 0.9406\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2397 - acc: 0.9205 - val_loss: 0.2695 - val_acc: 0.9348\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.2324 - acc: 0.9226 - val_loss: 0.2720 - val_acc: 0.9357\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2326 - acc: 0.9221 - val_loss: 0.2560 - val_acc: 0.9408\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2330 - acc: 0.9224 - val_loss: 0.2572 - val_acc: 0.9394\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2338 - acc: 0.9236 - val_loss: 0.2635 - val_acc: 0.9383\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2334 - acc: 0.9225 - val_loss: 0.2735 - val_acc: 0.9357\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2331 - acc: 0.9214 - val_loss: 0.2548 - val_acc: 0.9421\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2299 - acc: 0.9251 - val_loss: 0.2652 - val_acc: 0.9389\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2304 - acc: 0.9225 - val_loss: 0.2554 - val_acc: 0.9415\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2317 - acc: 0.9241 - val_loss: 0.2664 - val_acc: 0.9378\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2237 - acc: 0.9256 - val_loss: 0.2716 - val_acc: 0.9363\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.2316 - acc: 0.9247 - val_loss: 0.2545 - val_acc: 0.9416\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2229 - acc: 0.9256 - val_loss: 0.2591 - val_acc: 0.9400\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2257 - acc: 0.9252 - val_loss: 0.2557 - val_acc: 0.9420\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2265 - acc: 0.9237 - val_loss: 0.2537 - val_acc: 0.9424\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.2183 - acc: 0.9280 - val_loss: 0.2576 - val_acc: 0.9407\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2150 - acc: 0.9282 - val_loss: 0.2708 - val_acc: 0.9369\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2237 - acc: 0.9257 - val_loss: 0.2637 - val_acc: 0.9412\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2169 - acc: 0.9294 - val_loss: 0.2729 - val_acc: 0.9385\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2218 - acc: 0.9253 - val_loss: 0.2613 - val_acc: 0.9409\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.2213 - acc: 0.9263 - val_loss: 0.2735 - val_acc: 0.9364\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2147 - acc: 0.9292 - val_loss: 0.2631 - val_acc: 0.9390\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2207 - acc: 0.9260 - val_loss: 0.2576 - val_acc: 0.9414\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2191 - acc: 0.9277 - val_loss: 0.2541 - val_acc: 0.9434\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2184 - acc: 0.9273 - val_loss: 0.2691 - val_acc: 0.9387\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.2188 - acc: 0.9280 - val_loss: 0.2657 - val_acc: 0.9401\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.2137 - acc: 0.9282 - val_loss: 0.2585 - val_acc: 0.9402\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.2138 - acc: 0.9289 - val_loss: 0.2633 - val_acc: 0.9404\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.2153 - acc: 0.9282 - val_loss: 0.2523 - val_acc: 0.9440\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2172 - acc: 0.9267 - val_loss: 0.2827 - val_acc: 0.9342\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.2113 - acc: 0.9285 - val_loss: 0.2678 - val_acc: 0.9395\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.2178 - acc: 0.9288 - val_loss: 0.2509 - val_acc: 0.9437\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2122 - acc: 0.9295 - val_loss: 0.2583 - val_acc: 0.9421\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.2159 - acc: 0.9299 - val_loss: 0.2604 - val_acc: 0.9408\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2059 - acc: 0.9317 - val_loss: 0.2512 - val_acc: 0.9452\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2122 - acc: 0.9312 - val_loss: 0.2600 - val_acc: 0.9421\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.2061 - acc: 0.9315 - val_loss: 0.2554 - val_acc: 0.9436\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2094 - acc: 0.9314 - val_loss: 0.2545 - val_acc: 0.9448\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2102 - acc: 0.9297 - val_loss: 0.2620 - val_acc: 0.9423\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2077 - acc: 0.9299 - val_loss: 0.2535 - val_acc: 0.9438\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.2049 - acc: 0.9318 - val_loss: 0.2534 - val_acc: 0.9437\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2071 - acc: 0.9311 - val_loss: 0.2561 - val_acc: 0.9437\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.2103 - acc: 0.9292 - val_loss: 0.2487 - val_acc: 0.9463\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.2011 - acc: 0.9332 - val_loss: 0.2575 - val_acc: 0.9450\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.2002 - acc: 0.9333 - val_loss: 0.2569 - val_acc: 0.9445\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.2042 - acc: 0.9329 - val_loss: 0.2591 - val_acc: 0.9441\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.2083 - acc: 0.9303 - val_loss: 0.2615 - val_acc: 0.9419\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2098 - acc: 0.9297 - val_loss: 0.2615 - val_acc: 0.9432\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2027 - acc: 0.9325 - val_loss: 0.2627 - val_acc: 0.9431\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.2044 - acc: 0.9323 - val_loss: 0.2564 - val_acc: 0.9428\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.2025 - acc: 0.9322 - val_loss: 0.2563 - val_acc: 0.9452\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.2000 - acc: 0.9331 - val_loss: 0.2580 - val_acc: 0.9452\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1964 - acc: 0.9366 - val_loss: 0.2585 - val_acc: 0.9440\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1978 - acc: 0.9333 - val_loss: 0.2570 - val_acc: 0.9430\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.2027 - acc: 0.9328 - val_loss: 0.2497 - val_acc: 0.9467\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1938 - acc: 0.9354 - val_loss: 0.2676 - val_acc: 0.9419\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.2022 - acc: 0.9337 - val_loss: 0.2575 - val_acc: 0.9444\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2000 - acc: 0.9331 - val_loss: 0.2574 - val_acc: 0.9452\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1980 - acc: 0.9351 - val_loss: 0.2504 - val_acc: 0.9469\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.2007 - acc: 0.9338 - val_loss: 0.2587 - val_acc: 0.9449\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1999 - acc: 0.9331 - val_loss: 0.2528 - val_acc: 0.9459\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.2024 - acc: 0.9333 - val_loss: 0.2574 - val_acc: 0.9442\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1963 - acc: 0.9335 - val_loss: 0.2555 - val_acc: 0.9467\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.2027 - acc: 0.9328 - val_loss: 0.2589 - val_acc: 0.9443\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1952 - acc: 0.9364 - val_loss: 0.2588 - val_acc: 0.9447\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1953 - acc: 0.9355 - val_loss: 0.2520 - val_acc: 0.9474\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1915 - acc: 0.9367 - val_loss: 0.2661 - val_acc: 0.9432\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1986 - acc: 0.9341 - val_loss: 0.2590 - val_acc: 0.9456\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1924 - acc: 0.9353 - val_loss: 0.2563 - val_acc: 0.9462\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1919 - acc: 0.9361 - val_loss: 0.2508 - val_acc: 0.9465\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1938 - acc: 0.9355 - val_loss: 0.2589 - val_acc: 0.9445\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1910 - acc: 0.9371 - val_loss: 0.2509 - val_acc: 0.9470\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1945 - acc: 0.9342 - val_loss: 0.2599 - val_acc: 0.9448\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1890 - acc: 0.9376 - val_loss: 0.2541 - val_acc: 0.9474\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1930 - acc: 0.9369 - val_loss: 0.2537 - val_acc: 0.9469\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1943 - acc: 0.9345 - val_loss: 0.2471 - val_acc: 0.9492\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1902 - acc: 0.9362 - val_loss: 0.2464 - val_acc: 0.9481\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1829 - acc: 0.9389 - val_loss: 0.2547 - val_acc: 0.9483\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1925 - acc: 0.9368 - val_loss: 0.2590 - val_acc: 0.9458\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1931 - acc: 0.9357 - val_loss: 0.2554 - val_acc: 0.9478\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1823 - acc: 0.9391 - val_loss: 0.2556 - val_acc: 0.9472\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1901 - acc: 0.9376 - val_loss: 0.2574 - val_acc: 0.9461\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1884 - acc: 0.9374 - val_loss: 0.2516 - val_acc: 0.9491\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1828 - acc: 0.9393 - val_loss: 0.2566 - val_acc: 0.9475\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1871 - acc: 0.9379 - val_loss: 0.2555 - val_acc: 0.9463\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1847 - acc: 0.9376 - val_loss: 0.2632 - val_acc: 0.9454\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1809 - acc: 0.9400 - val_loss: 0.2596 - val_acc: 0.9470\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1818 - acc: 0.9397 - val_loss: 0.2582 - val_acc: 0.9482\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1858 - acc: 0.9377 - val_loss: 0.2523 - val_acc: 0.9496\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1894 - acc: 0.9367 - val_loss: 0.2558 - val_acc: 0.9488\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1897 - acc: 0.9364 - val_loss: 0.2632 - val_acc: 0.9453\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1856 - acc: 0.9393 - val_loss: 0.2551 - val_acc: 0.9485\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1874 - acc: 0.9381 - val_loss: 0.2562 - val_acc: 0.9473\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1864 - acc: 0.9377 - val_loss: 0.2551 - val_acc: 0.9487\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1821 - acc: 0.9398 - val_loss: 0.2571 - val_acc: 0.9487\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1784 - acc: 0.9406 - val_loss: 0.2554 - val_acc: 0.9492\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1836 - acc: 0.9399 - val_loss: 0.2631 - val_acc: 0.9463\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1815 - acc: 0.9400 - val_loss: 0.2593 - val_acc: 0.9481\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1809 - acc: 0.9393 - val_loss: 0.2622 - val_acc: 0.9470\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1815 - acc: 0.9388 - val_loss: 0.2602 - val_acc: 0.9465\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1775 - acc: 0.9410 - val_loss: 0.2559 - val_acc: 0.9476\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1769 - acc: 0.9410 - val_loss: 0.2569 - val_acc: 0.9483\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1837 - acc: 0.9390 - val_loss: 0.2532 - val_acc: 0.9487\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1759 - acc: 0.9419 - val_loss: 0.2586 - val_acc: 0.9478\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1809 - acc: 0.9397 - val_loss: 0.2592 - val_acc: 0.9483\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1827 - acc: 0.9395 - val_loss: 0.2548 - val_acc: 0.9482\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1802 - acc: 0.9403 - val_loss: 0.2557 - val_acc: 0.9488\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1789 - acc: 0.9407 - val_loss: 0.2543 - val_acc: 0.9493\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1750 - acc: 0.9412 - val_loss: 0.2707 - val_acc: 0.9445\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1843 - acc: 0.9393 - val_loss: 0.2680 - val_acc: 0.9452\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1788 - acc: 0.9405 - val_loss: 0.2579 - val_acc: 0.9469\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1773 - acc: 0.9415 - val_loss: 0.2580 - val_acc: 0.9475\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1742 - acc: 0.9424 - val_loss: 0.2625 - val_acc: 0.9478\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1779 - acc: 0.9403 - val_loss: 0.2564 - val_acc: 0.9483\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1784 - acc: 0.9403 - val_loss: 0.2532 - val_acc: 0.9497\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1782 - acc: 0.9398 - val_loss: 0.2585 - val_acc: 0.9476\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1746 - acc: 0.9424 - val_loss: 0.2612 - val_acc: 0.9461\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1771 - acc: 0.9405 - val_loss: 0.2643 - val_acc: 0.9470\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1713 - acc: 0.9417 - val_loss: 0.2613 - val_acc: 0.9483\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1784 - acc: 0.9405 - val_loss: 0.2631 - val_acc: 0.9472\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1764 - acc: 0.9410 - val_loss: 0.2592 - val_acc: 0.9475\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1699 - acc: 0.9425 - val_loss: 0.2581 - val_acc: 0.9481\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1770 - acc: 0.9408 - val_loss: 0.2491 - val_acc: 0.9506\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1781 - acc: 0.9412 - val_loss: 0.2580 - val_acc: 0.9485\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1778 - acc: 0.9406 - val_loss: 0.2657 - val_acc: 0.9464\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1646 - acc: 0.9457 - val_loss: 0.2563 - val_acc: 0.9502\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1781 - acc: 0.9406 - val_loss: 0.2580 - val_acc: 0.9484\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1722 - acc: 0.9418 - val_loss: 0.2598 - val_acc: 0.9485\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1763 - acc: 0.9410 - val_loss: 0.2610 - val_acc: 0.9488\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1789 - acc: 0.9407 - val_loss: 0.2647 - val_acc: 0.9484\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1771 - acc: 0.9406 - val_loss: 0.2530 - val_acc: 0.9499\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1749 - acc: 0.9425 - val_loss: 0.2639 - val_acc: 0.9469\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1728 - acc: 0.9422 - val_loss: 0.2600 - val_acc: 0.9489\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1726 - acc: 0.9424 - val_loss: 0.2519 - val_acc: 0.9510\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1740 - acc: 0.9421 - val_loss: 0.2556 - val_acc: 0.9503\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1697 - acc: 0.9436 - val_loss: 0.2568 - val_acc: 0.9504\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1640 - acc: 0.9450 - val_loss: 0.2633 - val_acc: 0.9470\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1702 - acc: 0.9444 - val_loss: 0.2586 - val_acc: 0.9493\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1686 - acc: 0.9442 - val_loss: 0.2588 - val_acc: 0.9492\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1779 - acc: 0.9408 - val_loss: 0.2569 - val_acc: 0.9502\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.1692 - acc: 0.9433 - val_loss: 0.2603 - val_acc: 0.9489\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1690 - acc: 0.9438 - val_loss: 0.2622 - val_acc: 0.9472\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1714 - acc: 0.9436 - val_loss: 0.2626 - val_acc: 0.9482\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1704 - acc: 0.9435 - val_loss: 0.2547 - val_acc: 0.9504\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1672 - acc: 0.9449 - val_loss: 0.2554 - val_acc: 0.9508\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1680 - acc: 0.9440 - val_loss: 0.2584 - val_acc: 0.9508\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1692 - acc: 0.9436 - val_loss: 0.2521 - val_acc: 0.9521\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1688 - acc: 0.9447 - val_loss: 0.2590 - val_acc: 0.9488\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1705 - acc: 0.9435 - val_loss: 0.2544 - val_acc: 0.9511\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1659 - acc: 0.9461 - val_loss: 0.2610 - val_acc: 0.9498\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1652 - acc: 0.9452 - val_loss: 0.2551 - val_acc: 0.9507\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1622 - acc: 0.9470 - val_loss: 0.2521 - val_acc: 0.9533\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.1657 - acc: 0.9461 - val_loss: 0.2587 - val_acc: 0.9501\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1667 - acc: 0.9445 - val_loss: 0.2544 - val_acc: 0.9507\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1667 - acc: 0.9450 - val_loss: 0.2540 - val_acc: 0.9509\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1681 - acc: 0.9451 - val_loss: 0.2567 - val_acc: 0.9494\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1620 - acc: 0.9458 - val_loss: 0.2593 - val_acc: 0.9493\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1661 - acc: 0.9458 - val_loss: 0.2565 - val_acc: 0.9513\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.1652 - acc: 0.9452 - val_loss: 0.2547 - val_acc: 0.9517\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1610 - acc: 0.9469 - val_loss: 0.2586 - val_acc: 0.9513\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.1625 - acc: 0.9465 - val_loss: 0.2626 - val_acc: 0.9496\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1679 - acc: 0.9445 - val_loss: 0.2613 - val_acc: 0.9494\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.1654 - acc: 0.9447 - val_loss: 0.2542 - val_acc: 0.9504\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.1617 - acc: 0.9464 - val_loss: 0.2609 - val_acc: 0.9497\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.1661 - acc: 0.9451 - val_loss: 0.2529 - val_acc: 0.9498\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1572 - acc: 0.9477 - val_loss: 0.2560 - val_acc: 0.9510\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.1676 - acc: 0.9448 - val_loss: 0.2560 - val_acc: 0.9515\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1632 - acc: 0.9470 - val_loss: 0.2657 - val_acc: 0.9488\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.1608 - acc: 0.9463 - val_loss: 0.2554 - val_acc: 0.9513\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1639 - acc: 0.9455 - val_loss: 0.2599 - val_acc: 0.9506\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1548 - acc: 0.9480 - val_loss: 0.2557 - val_acc: 0.9519\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1599 - acc: 0.9468 - val_loss: 0.2539 - val_acc: 0.9524\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1608 - acc: 0.9466 - val_loss: 0.2573 - val_acc: 0.9519\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1680 - acc: 0.9450 - val_loss: 0.2636 - val_acc: 0.9487\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.1644 - acc: 0.9454 - val_loss: 0.2615 - val_acc: 0.9501\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1626 - acc: 0.9459 - val_loss: 0.2589 - val_acc: 0.9516\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1617 - acc: 0.9458 - val_loss: 0.2684 - val_acc: 0.9477\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1614 - acc: 0.9462 - val_loss: 0.2693 - val_acc: 0.9493\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1628 - acc: 0.9457 - val_loss: 0.2656 - val_acc: 0.9498\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1589 - acc: 0.9467 - val_loss: 0.2550 - val_acc: 0.9524\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1627 - acc: 0.9456 - val_loss: 0.2556 - val_acc: 0.9512\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1563 - acc: 0.9483 - val_loss: 0.2674 - val_acc: 0.9494\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1620 - acc: 0.9469 - val_loss: 0.2634 - val_acc: 0.9505\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1587 - acc: 0.9470 - val_loss: 0.2662 - val_acc: 0.9495\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1621 - acc: 0.9469 - val_loss: 0.2695 - val_acc: 0.9493\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1590 - acc: 0.9480 - val_loss: 0.2620 - val_acc: 0.9503\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.1542 - acc: 0.9496 - val_loss: 0.2537 - val_acc: 0.9526\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1625 - acc: 0.9462 - val_loss: 0.2809 - val_acc: 0.9443\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.1668 - acc: 0.9450 - val_loss: 0.2665 - val_acc: 0.9486\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1560 - acc: 0.9475 - val_loss: 0.2669 - val_acc: 0.9504\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1576 - acc: 0.9468 - val_loss: 0.2564 - val_acc: 0.9511\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1657 - acc: 0.9443 - val_loss: 0.2656 - val_acc: 0.9508\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1564 - acc: 0.9490 - val_loss: 0.2587 - val_acc: 0.9514\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1542 - acc: 0.9496 - val_loss: 0.2620 - val_acc: 0.9512\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1565 - acc: 0.9484 - val_loss: 0.2645 - val_acc: 0.9492\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1551 - acc: 0.9477 - val_loss: 0.2584 - val_acc: 0.9517\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1633 - acc: 0.9462 - val_loss: 0.2626 - val_acc: 0.9512\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1559 - acc: 0.9484 - val_loss: 0.2553 - val_acc: 0.9525\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1569 - acc: 0.9480 - val_loss: 0.2602 - val_acc: 0.9506\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1527 - acc: 0.9485 - val_loss: 0.2587 - val_acc: 0.9520\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1541 - acc: 0.9477 - val_loss: 0.2686 - val_acc: 0.9485\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1575 - acc: 0.9474 - val_loss: 0.2589 - val_acc: 0.9515\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1636 - acc: 0.9456 - val_loss: 0.2614 - val_acc: 0.9496\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1582 - acc: 0.9481 - val_loss: 0.2592 - val_acc: 0.9517\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1548 - acc: 0.9492 - val_loss: 0.2595 - val_acc: 0.9511\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1606 - acc: 0.9455 - val_loss: 0.2601 - val_acc: 0.9506\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1567 - acc: 0.9487 - val_loss: 0.2671 - val_acc: 0.9496\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1527 - acc: 0.9496 - val_loss: 0.2587 - val_acc: 0.9518\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1579 - acc: 0.9480 - val_loss: 0.2697 - val_acc: 0.9504\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1513 - acc: 0.9495 - val_loss: 0.2598 - val_acc: 0.9513\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1554 - acc: 0.9488 - val_loss: 0.2565 - val_acc: 0.9516\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1537 - acc: 0.9497 - val_loss: 0.2528 - val_acc: 0.9531\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1535 - acc: 0.9487 - val_loss: 0.2670 - val_acc: 0.9499\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1519 - acc: 0.9495 - val_loss: 0.2598 - val_acc: 0.9514\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1507 - acc: 0.9509 - val_loss: 0.2549 - val_acc: 0.9534\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1526 - acc: 0.9486 - val_loss: 0.2606 - val_acc: 0.9520\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1482 - acc: 0.9502 - val_loss: 0.2686 - val_acc: 0.9507\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1518 - acc: 0.9499 - val_loss: 0.2667 - val_acc: 0.9504\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1528 - acc: 0.9506 - val_loss: 0.2604 - val_acc: 0.9517\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1558 - acc: 0.9487 - val_loss: 0.2624 - val_acc: 0.9501\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1552 - acc: 0.9485 - val_loss: 0.2580 - val_acc: 0.9509\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.1535 - acc: 0.9484 - val_loss: 0.2628 - val_acc: 0.9521\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1580 - acc: 0.9474 - val_loss: 0.2583 - val_acc: 0.9513\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1496 - acc: 0.9508 - val_loss: 0.2664 - val_acc: 0.9504\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1527 - acc: 0.9500 - val_loss: 0.2614 - val_acc: 0.9509\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.1484 - acc: 0.9512 - val_loss: 0.2626 - val_acc: 0.9513\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1520 - acc: 0.9497 - val_loss: 0.2683 - val_acc: 0.9495\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1489 - acc: 0.9504 - val_loss: 0.2607 - val_acc: 0.9514\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1507 - acc: 0.9503 - val_loss: 0.2565 - val_acc: 0.9528\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1495 - acc: 0.9515 - val_loss: 0.2622 - val_acc: 0.9510\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1501 - acc: 0.9497 - val_loss: 0.2714 - val_acc: 0.9493\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1491 - acc: 0.9509 - val_loss: 0.2683 - val_acc: 0.9495\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1487 - acc: 0.9513 - val_loss: 0.2609 - val_acc: 0.9516\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1488 - acc: 0.9497 - val_loss: 0.2627 - val_acc: 0.9519\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1454 - acc: 0.9510 - val_loss: 0.2640 - val_acc: 0.9509\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1470 - acc: 0.9514 - val_loss: 0.2668 - val_acc: 0.9515\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1529 - acc: 0.9501 - val_loss: 0.2603 - val_acc: 0.9515\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1475 - acc: 0.9521 - val_loss: 0.2684 - val_acc: 0.9520\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1473 - acc: 0.9502 - val_loss: 0.2608 - val_acc: 0.9527\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1480 - acc: 0.9508 - val_loss: 0.2687 - val_acc: 0.9507\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1498 - acc: 0.9495 - val_loss: 0.2718 - val_acc: 0.9505\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1479 - acc: 0.9516 - val_loss: 0.2662 - val_acc: 0.9512\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1484 - acc: 0.9499 - val_loss: 0.2664 - val_acc: 0.9508\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1484 - acc: 0.9504 - val_loss: 0.2614 - val_acc: 0.9521\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1433 - acc: 0.9536 - val_loss: 0.2645 - val_acc: 0.9519\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1549 - acc: 0.9488 - val_loss: 0.2634 - val_acc: 0.9522\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1502 - acc: 0.9511 - val_loss: 0.2673 - val_acc: 0.9506\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1477 - acc: 0.9518 - val_loss: 0.2661 - val_acc: 0.9528\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1504 - acc: 0.9499 - val_loss: 0.2590 - val_acc: 0.9529\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1481 - acc: 0.9506 - val_loss: 0.2656 - val_acc: 0.9503\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1491 - acc: 0.9496 - val_loss: 0.2632 - val_acc: 0.9523\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1512 - acc: 0.9495 - val_loss: 0.2659 - val_acc: 0.9505\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1484 - acc: 0.9504 - val_loss: 0.2562 - val_acc: 0.9539\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1486 - acc: 0.9500 - val_loss: 0.2591 - val_acc: 0.9530\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1429 - acc: 0.9523 - val_loss: 0.2619 - val_acc: 0.9521\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1455 - acc: 0.9506 - val_loss: 0.2642 - val_acc: 0.9516\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1496 - acc: 0.9513 - val_loss: 0.2631 - val_acc: 0.9510\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1524 - acc: 0.9495 - val_loss: 0.2582 - val_acc: 0.9520\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1445 - acc: 0.9527 - val_loss: 0.2631 - val_acc: 0.9529\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1478 - acc: 0.9509 - val_loss: 0.2629 - val_acc: 0.9510\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1473 - acc: 0.9509 - val_loss: 0.2649 - val_acc: 0.9526\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1427 - acc: 0.9531 - val_loss: 0.2633 - val_acc: 0.9524\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.1538 - acc: 0.9486 - val_loss: 0.2600 - val_acc: 0.9535\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.1431 - acc: 0.9529 - val_loss: 0.2605 - val_acc: 0.9526\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1499 - acc: 0.9509 - val_loss: 0.2683 - val_acc: 0.9492\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1425 - acc: 0.9520 - val_loss: 0.2580 - val_acc: 0.9531\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1427 - acc: 0.9524 - val_loss: 0.2625 - val_acc: 0.9515\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1493 - acc: 0.9511 - val_loss: 0.2627 - val_acc: 0.9518\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1416 - acc: 0.9535 - val_loss: 0.2621 - val_acc: 0.9527\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1448 - acc: 0.9523 - val_loss: 0.2630 - val_acc: 0.9520\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1420 - acc: 0.9526 - val_loss: 0.2590 - val_acc: 0.9521\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1469 - acc: 0.9517 - val_loss: 0.2577 - val_acc: 0.9508\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1402 - acc: 0.9529 - val_loss: 0.2642 - val_acc: 0.9504\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1407 - acc: 0.9545 - val_loss: 0.2604 - val_acc: 0.9523\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1493 - acc: 0.9510 - val_loss: 0.2585 - val_acc: 0.9523\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1475 - acc: 0.9509 - val_loss: 0.2599 - val_acc: 0.9523\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1440 - acc: 0.9517 - val_loss: 0.2637 - val_acc: 0.9521\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1420 - acc: 0.9536 - val_loss: 0.2625 - val_acc: 0.9535\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1439 - acc: 0.9513 - val_loss: 0.2587 - val_acc: 0.9531\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1450 - acc: 0.9525 - val_loss: 0.2611 - val_acc: 0.9525\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1440 - acc: 0.9524 - val_loss: 0.2578 - val_acc: 0.9536\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1431 - acc: 0.9522 - val_loss: 0.2634 - val_acc: 0.9529\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1412 - acc: 0.9536 - val_loss: 0.2619 - val_acc: 0.9526\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1483 - acc: 0.9513 - val_loss: 0.2681 - val_acc: 0.9515\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1408 - acc: 0.9537 - val_loss: 0.2632 - val_acc: 0.9520\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1445 - acc: 0.9527 - val_loss: 0.2613 - val_acc: 0.9519\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1421 - acc: 0.9536 - val_loss: 0.2632 - val_acc: 0.9533\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1420 - acc: 0.9529 - val_loss: 0.2625 - val_acc: 0.9536\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1407 - acc: 0.9532 - val_loss: 0.2655 - val_acc: 0.9521\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1383 - acc: 0.9545 - val_loss: 0.2604 - val_acc: 0.9537\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1403 - acc: 0.9528 - val_loss: 0.2694 - val_acc: 0.9525\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1459 - acc: 0.9514 - val_loss: 0.2598 - val_acc: 0.9535\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1480 - acc: 0.9499 - val_loss: 0.2705 - val_acc: 0.9511\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1461 - acc: 0.9516 - val_loss: 0.2622 - val_acc: 0.9527\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1412 - acc: 0.9532 - val_loss: 0.2651 - val_acc: 0.9519\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1453 - acc: 0.9523 - val_loss: 0.2624 - val_acc: 0.9528\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1422 - acc: 0.9533 - val_loss: 0.2703 - val_acc: 0.9522\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1392 - acc: 0.9542 - val_loss: 0.2640 - val_acc: 0.9528\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1441 - acc: 0.9520 - val_loss: 0.2696 - val_acc: 0.9523\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.1418 - acc: 0.9526 - val_loss: 0.2714 - val_acc: 0.9513\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1402 - acc: 0.9540 - val_loss: 0.2612 - val_acc: 0.9526\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1401 - acc: 0.9535 - val_loss: 0.2677 - val_acc: 0.9511\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1449 - acc: 0.9525 - val_loss: 0.2684 - val_acc: 0.9524\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1435 - acc: 0.9518 - val_loss: 0.2624 - val_acc: 0.9540\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1423 - acc: 0.9514 - val_loss: 0.2660 - val_acc: 0.9523\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1416 - acc: 0.9521 - val_loss: 0.2623 - val_acc: 0.9529\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1374 - acc: 0.9537 - val_loss: 0.2653 - val_acc: 0.9528\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1443 - acc: 0.9531 - val_loss: 0.2594 - val_acc: 0.9543\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1408 - acc: 0.9551 - val_loss: 0.2739 - val_acc: 0.9510\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1395 - acc: 0.9538 - val_loss: 0.2706 - val_acc: 0.9511\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1396 - acc: 0.9542 - val_loss: 0.2703 - val_acc: 0.9524\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1446 - acc: 0.9534 - val_loss: 0.2608 - val_acc: 0.9533\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1397 - acc: 0.9544 - val_loss: 0.2679 - val_acc: 0.9521\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1334 - acc: 0.9560 - val_loss: 0.2680 - val_acc: 0.9522\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1392 - acc: 0.9535 - val_loss: 0.2731 - val_acc: 0.9508\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1383 - acc: 0.9540 - val_loss: 0.2648 - val_acc: 0.9529\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1420 - acc: 0.9539 - val_loss: 0.2730 - val_acc: 0.9515\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1363 - acc: 0.9555 - val_loss: 0.2636 - val_acc: 0.9534\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1405 - acc: 0.9540 - val_loss: 0.2663 - val_acc: 0.9532\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1344 - acc: 0.9556 - val_loss: 0.2663 - val_acc: 0.9535\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1413 - acc: 0.9533 - val_loss: 0.2690 - val_acc: 0.9524\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1392 - acc: 0.9533 - val_loss: 0.2627 - val_acc: 0.9530\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1386 - acc: 0.9553 - val_loss: 0.2653 - val_acc: 0.9532\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1351 - acc: 0.9538 - val_loss: 0.2700 - val_acc: 0.9524\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1435 - acc: 0.9521 - val_loss: 0.2749 - val_acc: 0.9511\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1377 - acc: 0.9542 - val_loss: 0.2700 - val_acc: 0.9538\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1434 - acc: 0.9525 - val_loss: 0.2665 - val_acc: 0.9530\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.1417 - acc: 0.9536 - val_loss: 0.2646 - val_acc: 0.9551\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1353 - acc: 0.9558 - val_loss: 0.2667 - val_acc: 0.9533\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1368 - acc: 0.9555 - val_loss: 0.2655 - val_acc: 0.9523\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1392 - acc: 0.9544 - val_loss: 0.2626 - val_acc: 0.9542\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1339 - acc: 0.9551 - val_loss: 0.2705 - val_acc: 0.9529\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1351 - acc: 0.9550 - val_loss: 0.2619 - val_acc: 0.9536\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1352 - acc: 0.9544 - val_loss: 0.2694 - val_acc: 0.9527\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1391 - acc: 0.9540 - val_loss: 0.2761 - val_acc: 0.9518\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1385 - acc: 0.9539 - val_loss: 0.2651 - val_acc: 0.9531\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1409 - acc: 0.9538 - val_loss: 0.2651 - val_acc: 0.9539\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1386 - acc: 0.9552 - val_loss: 0.2621 - val_acc: 0.9543\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1376 - acc: 0.9555 - val_loss: 0.2611 - val_acc: 0.9537\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.1338 - acc: 0.9556 - val_loss: 0.2682 - val_acc: 0.9526\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1396 - acc: 0.9533 - val_loss: 0.2716 - val_acc: 0.9537\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.1337 - acc: 0.9569 - val_loss: 0.2709 - val_acc: 0.9531\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.1366 - acc: 0.9559 - val_loss: 0.2711 - val_acc: 0.9532\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1343 - acc: 0.9551 - val_loss: 0.2689 - val_acc: 0.9536\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1357 - acc: 0.9551 - val_loss: 0.2711 - val_acc: 0.9534\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1338 - acc: 0.9565 - val_loss: 0.2689 - val_acc: 0.9535\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1406 - acc: 0.9537 - val_loss: 0.2757 - val_acc: 0.9522\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1404 - acc: 0.9532 - val_loss: 0.2735 - val_acc: 0.9530\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1316 - acc: 0.9557 - val_loss: 0.2681 - val_acc: 0.9539\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1317 - acc: 0.9564 - val_loss: 0.2722 - val_acc: 0.9530\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.1329 - acc: 0.9557 - val_loss: 0.2732 - val_acc: 0.9522\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1316 - acc: 0.9560 - val_loss: 0.2735 - val_acc: 0.9520\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1317 - acc: 0.9562 - val_loss: 0.2721 - val_acc: 0.9532\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.1326 - acc: 0.9568 - val_loss: 0.2657 - val_acc: 0.9542\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1341 - acc: 0.9560 - val_loss: 0.2721 - val_acc: 0.9520\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1364 - acc: 0.9555 - val_loss: 0.2664 - val_acc: 0.9531\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1340 - acc: 0.9558 - val_loss: 0.2732 - val_acc: 0.9531\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1349 - acc: 0.9545 - val_loss: 0.2715 - val_acc: 0.9526\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1372 - acc: 0.9543 - val_loss: 0.2675 - val_acc: 0.9535\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1366 - acc: 0.9551 - val_loss: 0.2699 - val_acc: 0.9526\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1343 - acc: 0.9563 - val_loss: 0.2683 - val_acc: 0.9547\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.1300 - acc: 0.9564 - val_loss: 0.2673 - val_acc: 0.9548\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1325 - acc: 0.9567 - val_loss: 0.2681 - val_acc: 0.9533\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.1352 - acc: 0.9550 - val_loss: 0.2622 - val_acc: 0.9552\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1384 - acc: 0.9538 - val_loss: 0.2669 - val_acc: 0.9536\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.1314 - acc: 0.9564 - val_loss: 0.2720 - val_acc: 0.9531\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1362 - acc: 0.9561 - val_loss: 0.2769 - val_acc: 0.9520\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1375 - acc: 0.9550 - val_loss: 0.2723 - val_acc: 0.9530\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1314 - acc: 0.9563 - val_loss: 0.2732 - val_acc: 0.9530\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 8s 181us/sample - loss: 0.1357 - acc: 0.9555 - val_loss: 0.2729 - val_acc: 0.9529\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1381 - acc: 0.9550 - val_loss: 0.2772 - val_acc: 0.9509\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.1397 - acc: 0.9536 - val_loss: 0.2707 - val_acc: 0.9516\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.1294 - acc: 0.9571 - val_loss: 0.2669 - val_acc: 0.9548\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1337 - acc: 0.9556 - val_loss: 0.2667 - val_acc: 0.9543\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1320 - acc: 0.9566 - val_loss: 0.2679 - val_acc: 0.9541\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1310 - acc: 0.9560 - val_loss: 0.2736 - val_acc: 0.9527\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.1340 - acc: 0.9560 - val_loss: 0.2740 - val_acc: 0.9533\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.1345 - acc: 0.9548 - val_loss: 0.2635 - val_acc: 0.9550\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.1308 - acc: 0.9571 - val_loss: 0.2727 - val_acc: 0.9525\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1337 - acc: 0.9564 - val_loss: 0.2684 - val_acc: 0.9539\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.1287 - acc: 0.9566 - val_loss: 0.2741 - val_acc: 0.9537\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.1366 - acc: 0.9539 - val_loss: 0.2717 - val_acc: 0.9539\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.1307 - acc: 0.9561 - val_loss: 0.2704 - val_acc: 0.9544\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1380 - acc: 0.9547 - val_loss: 0.2672 - val_acc: 0.9532\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1367 - acc: 0.9547 - val_loss: 0.2686 - val_acc: 0.9535\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1306 - acc: 0.9564 - val_loss: 0.2660 - val_acc: 0.9550\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.1326 - acc: 0.9556 - val_loss: 0.2727 - val_acc: 0.9529\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.1307 - acc: 0.9577 - val_loss: 0.2753 - val_acc: 0.9524\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.1280 - acc: 0.9577 - val_loss: 0.2723 - val_acc: 0.9538\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1309 - acc: 0.9570 - val_loss: 0.2660 - val_acc: 0.9543\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1334 - acc: 0.9567 - val_loss: 0.2684 - val_acc: 0.9541\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1299 - acc: 0.9569 - val_loss: 0.2731 - val_acc: 0.9518\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.1340 - acc: 0.9560 - val_loss: 0.2708 - val_acc: 0.9541\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1327 - acc: 0.9567 - val_loss: 0.2699 - val_acc: 0.9548\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1321 - acc: 0.9568 - val_loss: 0.2742 - val_acc: 0.9520\n"
     ]
    }
   ],
   "source": [
    "modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[200,100,10],['relu','relu','softmax'],500,True,0.019560946340911864,4.715676601623284e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gmrpYEV4KIVb"
   },
   "source": [
    "Accuracy increased form 90 to 95 %\n",
    "\n",
    "The above Results shows that the accuracy increases with epoch and after a certain number of epoch say 230 epochs, the accuracy is not improving further\n",
    "\n",
    "\n",
    "Lets try changing the architecture for the same parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I9RSAducopJC",
    "outputId": "a9eb19e2-08f0-40cd-b7fa-7acb288a39a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 1.4654 - acc: 0.5182 - val_loss: 0.9590 - val_acc: 0.6999\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.9806 - acc: 0.6907 - val_loss: 0.7747 - val_acc: 0.7681\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.8448 - acc: 0.7368 - val_loss: 0.6417 - val_acc: 0.8083\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.7635 - acc: 0.7609 - val_loss: 0.6034 - val_acc: 0.8181\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.6990 - acc: 0.7819 - val_loss: 0.5869 - val_acc: 0.8243\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.6600 - acc: 0.7928 - val_loss: 0.5395 - val_acc: 0.8385\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.6229 - acc: 0.8040 - val_loss: 0.5100 - val_acc: 0.8462\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.5943 - acc: 0.8124 - val_loss: 0.4734 - val_acc: 0.8587\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.5696 - acc: 0.8193 - val_loss: 0.4478 - val_acc: 0.8668\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.5465 - acc: 0.8278 - val_loss: 0.4501 - val_acc: 0.8660\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.5280 - acc: 0.8298 - val_loss: 0.4192 - val_acc: 0.8758\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.5103 - acc: 0.8361 - val_loss: 0.4341 - val_acc: 0.8703\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.4976 - acc: 0.8421 - val_loss: 0.4355 - val_acc: 0.8685\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.4829 - acc: 0.8456 - val_loss: 0.3921 - val_acc: 0.8831\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.4710 - acc: 0.8479 - val_loss: 0.3862 - val_acc: 0.8863\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.4507 - acc: 0.8557 - val_loss: 0.3860 - val_acc: 0.8858\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.4440 - acc: 0.8579 - val_loss: 0.3649 - val_acc: 0.8923\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.4329 - acc: 0.8596 - val_loss: 0.3562 - val_acc: 0.8953\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.4218 - acc: 0.8628 - val_loss: 0.3457 - val_acc: 0.8994\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.4105 - acc: 0.8693 - val_loss: 0.3506 - val_acc: 0.8973\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.4029 - acc: 0.8690 - val_loss: 0.3336 - val_acc: 0.9029\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.3984 - acc: 0.8696 - val_loss: 0.3306 - val_acc: 0.9039\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3919 - acc: 0.8724 - val_loss: 0.3250 - val_acc: 0.9057\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3808 - acc: 0.8762 - val_loss: 0.3138 - val_acc: 0.9107\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3687 - acc: 0.8781 - val_loss: 0.3127 - val_acc: 0.9111\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.3634 - acc: 0.8821 - val_loss: 0.3074 - val_acc: 0.9132\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.3594 - acc: 0.8825 - val_loss: 0.3059 - val_acc: 0.9141\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3513 - acc: 0.8848 - val_loss: 0.3112 - val_acc: 0.9123\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3510 - acc: 0.8834 - val_loss: 0.3183 - val_acc: 0.9111\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.3447 - acc: 0.8858 - val_loss: 0.3009 - val_acc: 0.9164\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.3386 - acc: 0.8895 - val_loss: 0.3031 - val_acc: 0.9163\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.3287 - acc: 0.8924 - val_loss: 0.2893 - val_acc: 0.9195\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.3258 - acc: 0.8931 - val_loss: 0.2802 - val_acc: 0.9247\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.3187 - acc: 0.8953 - val_loss: 0.2966 - val_acc: 0.9183\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.3132 - acc: 0.8965 - val_loss: 0.2806 - val_acc: 0.9236\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.3087 - acc: 0.8991 - val_loss: 0.2764 - val_acc: 0.9256\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.3046 - acc: 0.8991 - val_loss: 0.2699 - val_acc: 0.9277\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 10s 240us/sample - loss: 0.3043 - acc: 0.8996 - val_loss: 0.2741 - val_acc: 0.9256\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.2967 - acc: 0.9025 - val_loss: 0.2775 - val_acc: 0.9259\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.2898 - acc: 0.9039 - val_loss: 0.2738 - val_acc: 0.9269\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.2886 - acc: 0.9047 - val_loss: 0.2634 - val_acc: 0.9317\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2876 - acc: 0.9050 - val_loss: 0.2628 - val_acc: 0.9313\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.2834 - acc: 0.9045 - val_loss: 0.2716 - val_acc: 0.9283\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.2855 - acc: 0.9057 - val_loss: 0.2649 - val_acc: 0.9309\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.2769 - acc: 0.9081 - val_loss: 0.2634 - val_acc: 0.9312\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.2735 - acc: 0.9107 - val_loss: 0.2621 - val_acc: 0.9322\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.2678 - acc: 0.9107 - val_loss: 0.2523 - val_acc: 0.9363\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.2675 - acc: 0.9096 - val_loss: 0.2598 - val_acc: 0.9330\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.2695 - acc: 0.9107 - val_loss: 0.2562 - val_acc: 0.9363\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.2624 - acc: 0.9126 - val_loss: 0.2558 - val_acc: 0.9349\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.2585 - acc: 0.9147 - val_loss: 0.2584 - val_acc: 0.9343\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.2561 - acc: 0.9146 - val_loss: 0.2555 - val_acc: 0.9346\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.2553 - acc: 0.9162 - val_loss: 0.2632 - val_acc: 0.9341\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.2538 - acc: 0.9162 - val_loss: 0.2579 - val_acc: 0.9348\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.2464 - acc: 0.9188 - val_loss: 0.2496 - val_acc: 0.9378\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2412 - acc: 0.9213 - val_loss: 0.2515 - val_acc: 0.9369\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.2405 - acc: 0.9201 - val_loss: 0.2461 - val_acc: 0.9400\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2414 - acc: 0.9201 - val_loss: 0.2552 - val_acc: 0.9382\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.2405 - acc: 0.9203 - val_loss: 0.2616 - val_acc: 0.9357\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.2388 - acc: 0.9196 - val_loss: 0.2443 - val_acc: 0.9410\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 10s 231us/sample - loss: 0.2312 - acc: 0.9238 - val_loss: 0.2434 - val_acc: 0.9409\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.2330 - acc: 0.9220 - val_loss: 0.2521 - val_acc: 0.9386\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.2324 - acc: 0.9218 - val_loss: 0.2470 - val_acc: 0.9414\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2241 - acc: 0.9263 - val_loss: 0.2587 - val_acc: 0.9361\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.2282 - acc: 0.9243 - val_loss: 0.2438 - val_acc: 0.9419\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.2259 - acc: 0.9251 - val_loss: 0.2573 - val_acc: 0.9377\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.2221 - acc: 0.9267 - val_loss: 0.2428 - val_acc: 0.9428\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.2233 - acc: 0.9258 - val_loss: 0.2505 - val_acc: 0.9426\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.2188 - acc: 0.9275 - val_loss: 0.2432 - val_acc: 0.9434\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.2178 - acc: 0.9266 - val_loss: 0.2530 - val_acc: 0.9399\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.2122 - acc: 0.9294 - val_loss: 0.2452 - val_acc: 0.9439\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.2158 - acc: 0.9285 - val_loss: 0.2462 - val_acc: 0.9436\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 10s 238us/sample - loss: 0.2119 - acc: 0.9279 - val_loss: 0.2485 - val_acc: 0.9435\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 10s 236us/sample - loss: 0.2138 - acc: 0.9288 - val_loss: 0.2492 - val_acc: 0.9441\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 10s 237us/sample - loss: 0.2121 - acc: 0.9297 - val_loss: 0.2557 - val_acc: 0.9412\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 10s 240us/sample - loss: 0.2087 - acc: 0.9312 - val_loss: 0.2452 - val_acc: 0.9444\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.2139 - acc: 0.9292 - val_loss: 0.2471 - val_acc: 0.9431\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.2078 - acc: 0.9301 - val_loss: 0.2631 - val_acc: 0.9381\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.2029 - acc: 0.9333 - val_loss: 0.2531 - val_acc: 0.9412\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 10s 233us/sample - loss: 0.1898 - acc: 0.9374 - val_loss: 0.2474 - val_acc: 0.9436\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1989 - acc: 0.9335 - val_loss: 0.2479 - val_acc: 0.9452\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 10s 233us/sample - loss: 0.2020 - acc: 0.9326 - val_loss: 0.2548 - val_acc: 0.9433\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1989 - acc: 0.9337 - val_loss: 0.2545 - val_acc: 0.9427\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1896 - acc: 0.9370 - val_loss: 0.2395 - val_acc: 0.9474\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.2011 - acc: 0.9331 - val_loss: 0.2541 - val_acc: 0.9443\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1973 - acc: 0.9340 - val_loss: 0.2526 - val_acc: 0.9456\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1934 - acc: 0.9359 - val_loss: 0.2421 - val_acc: 0.9478\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 10s 236us/sample - loss: 0.1957 - acc: 0.9342 - val_loss: 0.2514 - val_acc: 0.9450\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1905 - acc: 0.9364 - val_loss: 0.2533 - val_acc: 0.9448\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1909 - acc: 0.9356 - val_loss: 0.2446 - val_acc: 0.9466\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1870 - acc: 0.9375 - val_loss: 0.2470 - val_acc: 0.9473\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.1854 - acc: 0.9387 - val_loss: 0.2520 - val_acc: 0.9455\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1832 - acc: 0.9390 - val_loss: 0.2418 - val_acc: 0.9469\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1836 - acc: 0.9375 - val_loss: 0.2437 - val_acc: 0.9486\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1819 - acc: 0.9392 - val_loss: 0.2416 - val_acc: 0.9504\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1806 - acc: 0.9394 - val_loss: 0.2532 - val_acc: 0.9465\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1833 - acc: 0.9388 - val_loss: 0.2545 - val_acc: 0.9449\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1798 - acc: 0.9405 - val_loss: 0.2495 - val_acc: 0.9466\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1866 - acc: 0.9384 - val_loss: 0.2476 - val_acc: 0.9479\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1732 - acc: 0.9418 - val_loss: 0.2509 - val_acc: 0.9481\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1803 - acc: 0.9402 - val_loss: 0.2478 - val_acc: 0.9490\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.1742 - acc: 0.9418 - val_loss: 0.2596 - val_acc: 0.9452\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.1786 - acc: 0.9406 - val_loss: 0.2505 - val_acc: 0.9480\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1825 - acc: 0.9395 - val_loss: 0.2505 - val_acc: 0.9495\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1793 - acc: 0.9413 - val_loss: 0.2546 - val_acc: 0.9471\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1767 - acc: 0.9401 - val_loss: 0.2530 - val_acc: 0.9480\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1717 - acc: 0.9429 - val_loss: 0.2493 - val_acc: 0.9489\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1701 - acc: 0.9434 - val_loss: 0.2457 - val_acc: 0.9499\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.1746 - acc: 0.9425 - val_loss: 0.2489 - val_acc: 0.9493\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.1641 - acc: 0.9458 - val_loss: 0.2448 - val_acc: 0.9512\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1701 - acc: 0.9433 - val_loss: 0.2557 - val_acc: 0.9474\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1654 - acc: 0.9444 - val_loss: 0.2582 - val_acc: 0.9478\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1688 - acc: 0.9438 - val_loss: 0.2501 - val_acc: 0.9494\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1689 - acc: 0.9431 - val_loss: 0.2447 - val_acc: 0.9515\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.1651 - acc: 0.9451 - val_loss: 0.2535 - val_acc: 0.9475\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1664 - acc: 0.9445 - val_loss: 0.2490 - val_acc: 0.9509\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1637 - acc: 0.9453 - val_loss: 0.2515 - val_acc: 0.9505\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1591 - acc: 0.9459 - val_loss: 0.2597 - val_acc: 0.9480\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1574 - acc: 0.9467 - val_loss: 0.2486 - val_acc: 0.9500\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 10s 238us/sample - loss: 0.1594 - acc: 0.9475 - val_loss: 0.2531 - val_acc: 0.9496\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1666 - acc: 0.9454 - val_loss: 0.2449 - val_acc: 0.9511\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1593 - acc: 0.9465 - val_loss: 0.2552 - val_acc: 0.9503\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1583 - acc: 0.9484 - val_loss: 0.2528 - val_acc: 0.9493\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 10s 238us/sample - loss: 0.1546 - acc: 0.9481 - val_loss: 0.2513 - val_acc: 0.9505\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 0.1588 - acc: 0.9463 - val_loss: 0.2515 - val_acc: 0.9507\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 10s 231us/sample - loss: 0.1556 - acc: 0.9480 - val_loss: 0.2569 - val_acc: 0.9499\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1587 - acc: 0.9485 - val_loss: 0.2464 - val_acc: 0.9513\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1544 - acc: 0.9487 - val_loss: 0.2507 - val_acc: 0.9515\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.1530 - acc: 0.9488 - val_loss: 0.2631 - val_acc: 0.9477\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1598 - acc: 0.9476 - val_loss: 0.2459 - val_acc: 0.9531\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1521 - acc: 0.9503 - val_loss: 0.2427 - val_acc: 0.9537\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.1566 - acc: 0.9483 - val_loss: 0.2552 - val_acc: 0.9506\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1537 - acc: 0.9488 - val_loss: 0.2507 - val_acc: 0.9513\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.1579 - acc: 0.9485 - val_loss: 0.2488 - val_acc: 0.9524\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.1479 - acc: 0.9511 - val_loss: 0.2569 - val_acc: 0.9515\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.1457 - acc: 0.9509 - val_loss: 0.2489 - val_acc: 0.9538\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1518 - acc: 0.9505 - val_loss: 0.2529 - val_acc: 0.9518\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1526 - acc: 0.9493 - val_loss: 0.2563 - val_acc: 0.9512\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1471 - acc: 0.9504 - val_loss: 0.2523 - val_acc: 0.9526\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1462 - acc: 0.9515 - val_loss: 0.2562 - val_acc: 0.9507\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.1443 - acc: 0.9528 - val_loss: 0.2546 - val_acc: 0.9527\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.1502 - acc: 0.9491 - val_loss: 0.2549 - val_acc: 0.9517\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1530 - acc: 0.9503 - val_loss: 0.2514 - val_acc: 0.9525\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1480 - acc: 0.9513 - val_loss: 0.2544 - val_acc: 0.9511\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1493 - acc: 0.9508 - val_loss: 0.2596 - val_acc: 0.9528\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1474 - acc: 0.9522 - val_loss: 0.2488 - val_acc: 0.9531\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1496 - acc: 0.9508 - val_loss: 0.2536 - val_acc: 0.9510\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1412 - acc: 0.9534 - val_loss: 0.2558 - val_acc: 0.9517\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1371 - acc: 0.9540 - val_loss: 0.2547 - val_acc: 0.9520\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1416 - acc: 0.9531 - val_loss: 0.2524 - val_acc: 0.9530\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1377 - acc: 0.9542 - val_loss: 0.2500 - val_acc: 0.9536\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1438 - acc: 0.9513 - val_loss: 0.2552 - val_acc: 0.9521\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 10s 234us/sample - loss: 0.1436 - acc: 0.9516 - val_loss: 0.2542 - val_acc: 0.9523\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1421 - acc: 0.9525 - val_loss: 0.2637 - val_acc: 0.9519\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1374 - acc: 0.9543 - val_loss: 0.2561 - val_acc: 0.9545\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1363 - acc: 0.9557 - val_loss: 0.2573 - val_acc: 0.9535\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 10s 231us/sample - loss: 0.1476 - acc: 0.9512 - val_loss: 0.2517 - val_acc: 0.9534\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1388 - acc: 0.9532 - val_loss: 0.2591 - val_acc: 0.9524\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1450 - acc: 0.9528 - val_loss: 0.2669 - val_acc: 0.9492\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1409 - acc: 0.9532 - val_loss: 0.2656 - val_acc: 0.9514\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 10s 233us/sample - loss: 0.1386 - acc: 0.9537 - val_loss: 0.2579 - val_acc: 0.9518\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 10s 233us/sample - loss: 0.1451 - acc: 0.9530 - val_loss: 0.2487 - val_acc: 0.9540\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 10s 234us/sample - loss: 0.1394 - acc: 0.9539 - val_loss: 0.2545 - val_acc: 0.9532\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1351 - acc: 0.9552 - val_loss: 0.2565 - val_acc: 0.9534\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 10s 236us/sample - loss: 0.1404 - acc: 0.9534 - val_loss: 0.2583 - val_acc: 0.9530\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1349 - acc: 0.9553 - val_loss: 0.2561 - val_acc: 0.9542\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1311 - acc: 0.9566 - val_loss: 0.2631 - val_acc: 0.9523\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1288 - acc: 0.9561 - val_loss: 0.2599 - val_acc: 0.9532\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.1339 - acc: 0.9553 - val_loss: 0.2556 - val_acc: 0.9548\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1341 - acc: 0.9554 - val_loss: 0.2638 - val_acc: 0.9528\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1333 - acc: 0.9553 - val_loss: 0.2594 - val_acc: 0.9547\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1306 - acc: 0.9563 - val_loss: 0.2627 - val_acc: 0.9543\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1318 - acc: 0.9566 - val_loss: 0.2645 - val_acc: 0.9535\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1343 - acc: 0.9563 - val_loss: 0.2564 - val_acc: 0.9546\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1260 - acc: 0.9587 - val_loss: 0.2600 - val_acc: 0.9542\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 0.1286 - acc: 0.9584 - val_loss: 0.2621 - val_acc: 0.9538\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1300 - acc: 0.9569 - val_loss: 0.2573 - val_acc: 0.9544\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1340 - acc: 0.9571 - val_loss: 0.2576 - val_acc: 0.9545\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1286 - acc: 0.9570 - val_loss: 0.2628 - val_acc: 0.9528\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1279 - acc: 0.9574 - val_loss: 0.2584 - val_acc: 0.9530\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1293 - acc: 0.9567 - val_loss: 0.2632 - val_acc: 0.9528\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1303 - acc: 0.9569 - val_loss: 0.2661 - val_acc: 0.9538\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1310 - acc: 0.9568 - val_loss: 0.2594 - val_acc: 0.9552\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.1309 - acc: 0.9577 - val_loss: 0.2603 - val_acc: 0.9548\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1250 - acc: 0.9585 - val_loss: 0.2555 - val_acc: 0.9546\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 10s 243us/sample - loss: 0.1319 - acc: 0.9566 - val_loss: 0.2563 - val_acc: 0.9541\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.1273 - acc: 0.9574 - val_loss: 0.2597 - val_acc: 0.9546\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 10s 240us/sample - loss: 0.1256 - acc: 0.9594 - val_loss: 0.2642 - val_acc: 0.9535\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1286 - acc: 0.9578 - val_loss: 0.2604 - val_acc: 0.9542\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1282 - acc: 0.9575 - val_loss: 0.2580 - val_acc: 0.9549\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1290 - acc: 0.9579 - val_loss: 0.2621 - val_acc: 0.9524\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.1239 - acc: 0.9592 - val_loss: 0.2647 - val_acc: 0.9547\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 10s 241us/sample - loss: 0.1230 - acc: 0.9597 - val_loss: 0.2619 - val_acc: 0.9553\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 10s 236us/sample - loss: 0.1211 - acc: 0.9602 - val_loss: 0.2604 - val_acc: 0.9556\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1283 - acc: 0.9575 - val_loss: 0.2570 - val_acc: 0.9550\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1227 - acc: 0.9591 - val_loss: 0.2581 - val_acc: 0.9553\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1211 - acc: 0.9605 - val_loss: 0.2634 - val_acc: 0.9538\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 10s 236us/sample - loss: 0.1218 - acc: 0.9602 - val_loss: 0.2573 - val_acc: 0.9554\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1212 - acc: 0.9607 - val_loss: 0.2633 - val_acc: 0.9538\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1183 - acc: 0.9616 - val_loss: 0.2665 - val_acc: 0.9544\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1236 - acc: 0.9593 - val_loss: 0.2642 - val_acc: 0.9550\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1258 - acc: 0.9591 - val_loss: 0.2696 - val_acc: 0.9555\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1256 - acc: 0.9590 - val_loss: 0.2673 - val_acc: 0.9542\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1234 - acc: 0.9588 - val_loss: 0.2660 - val_acc: 0.9536\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1211 - acc: 0.9610 - val_loss: 0.2587 - val_acc: 0.9552\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1183 - acc: 0.9615 - val_loss: 0.2792 - val_acc: 0.9515\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1194 - acc: 0.9607 - val_loss: 0.2738 - val_acc: 0.9542\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.1204 - acc: 0.9606 - val_loss: 0.2591 - val_acc: 0.9550\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1218 - acc: 0.9600 - val_loss: 0.2620 - val_acc: 0.9550\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1151 - acc: 0.9619 - val_loss: 0.2672 - val_acc: 0.9545\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1210 - acc: 0.9605 - val_loss: 0.2610 - val_acc: 0.9564\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1167 - acc: 0.9615 - val_loss: 0.2607 - val_acc: 0.9564\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 10s 238us/sample - loss: 0.1174 - acc: 0.9616 - val_loss: 0.2715 - val_acc: 0.9538\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1181 - acc: 0.9615 - val_loss: 0.2654 - val_acc: 0.9551\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1164 - acc: 0.9600 - val_loss: 0.2661 - val_acc: 0.9560\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1157 - acc: 0.9619 - val_loss: 0.2644 - val_acc: 0.9548\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1154 - acc: 0.9617 - val_loss: 0.2740 - val_acc: 0.9545\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1174 - acc: 0.9613 - val_loss: 0.2657 - val_acc: 0.9543\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 10s 237us/sample - loss: 0.1176 - acc: 0.9603 - val_loss: 0.2720 - val_acc: 0.9549\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1170 - acc: 0.9609 - val_loss: 0.2679 - val_acc: 0.9547\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1131 - acc: 0.9624 - val_loss: 0.2663 - val_acc: 0.9550\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1158 - acc: 0.9616 - val_loss: 0.2670 - val_acc: 0.9545\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1158 - acc: 0.9612 - val_loss: 0.2676 - val_acc: 0.9552\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1190 - acc: 0.9618 - val_loss: 0.2646 - val_acc: 0.9549\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1156 - acc: 0.9622 - val_loss: 0.2665 - val_acc: 0.9552\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1150 - acc: 0.9625 - val_loss: 0.2670 - val_acc: 0.9553\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 10s 235us/sample - loss: 0.1126 - acc: 0.9629 - val_loss: 0.2606 - val_acc: 0.9559\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1173 - acc: 0.9609 - val_loss: 0.2698 - val_acc: 0.9548\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1078 - acc: 0.9647 - val_loss: 0.2712 - val_acc: 0.9551\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1147 - acc: 0.9623 - val_loss: 0.2633 - val_acc: 0.9557\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 10s 231us/sample - loss: 0.1170 - acc: 0.9620 - val_loss: 0.2727 - val_acc: 0.9544\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1077 - acc: 0.9642 - val_loss: 0.2711 - val_acc: 0.9553\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 0.1168 - acc: 0.9626 - val_loss: 0.2742 - val_acc: 0.9538\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1089 - acc: 0.9644 - val_loss: 0.2771 - val_acc: 0.9555\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1178 - acc: 0.9625 - val_loss: 0.2713 - val_acc: 0.9547\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1100 - acc: 0.9640 - val_loss: 0.2713 - val_acc: 0.9551\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1099 - acc: 0.9638 - val_loss: 0.2655 - val_acc: 0.9562\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 10s 236us/sample - loss: 0.1139 - acc: 0.9621 - val_loss: 0.2748 - val_acc: 0.9550\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1113 - acc: 0.9637 - val_loss: 0.2722 - val_acc: 0.9562\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 10s 231us/sample - loss: 0.1179 - acc: 0.9623 - val_loss: 0.2725 - val_acc: 0.9557\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 0.1113 - acc: 0.9634 - val_loss: 0.2703 - val_acc: 0.9565\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1101 - acc: 0.9646 - val_loss: 0.2696 - val_acc: 0.9551\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1159 - acc: 0.9631 - val_loss: 0.2671 - val_acc: 0.9560\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1103 - acc: 0.9636 - val_loss: 0.2734 - val_acc: 0.9549\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1113 - acc: 0.9638 - val_loss: 0.2686 - val_acc: 0.9556\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1065 - acc: 0.9650 - val_loss: 0.2730 - val_acc: 0.9558\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1094 - acc: 0.9630 - val_loss: 0.2675 - val_acc: 0.9566\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1083 - acc: 0.9644 - val_loss: 0.2671 - val_acc: 0.9564\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1067 - acc: 0.9644 - val_loss: 0.2782 - val_acc: 0.9551\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1101 - acc: 0.9639 - val_loss: 0.2775 - val_acc: 0.9550\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1089 - acc: 0.9648 - val_loss: 0.2699 - val_acc: 0.9555\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1093 - acc: 0.9636 - val_loss: 0.2777 - val_acc: 0.9549\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.1121 - acc: 0.9631 - val_loss: 0.2708 - val_acc: 0.9555\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1137 - acc: 0.9630 - val_loss: 0.2705 - val_acc: 0.9565\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1132 - acc: 0.9635 - val_loss: 0.2775 - val_acc: 0.9541\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1078 - acc: 0.9643 - val_loss: 0.2718 - val_acc: 0.9550\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1046 - acc: 0.9660 - val_loss: 0.2676 - val_acc: 0.9559\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1107 - acc: 0.9647 - val_loss: 0.2676 - val_acc: 0.9557\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1100 - acc: 0.9635 - val_loss: 0.2725 - val_acc: 0.9564\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1077 - acc: 0.9641 - val_loss: 0.2798 - val_acc: 0.9537\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.1073 - acc: 0.9651 - val_loss: 0.2782 - val_acc: 0.9540\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1026 - acc: 0.9674 - val_loss: 0.2709 - val_acc: 0.9561\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1053 - acc: 0.9648 - val_loss: 0.2699 - val_acc: 0.9564\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.1068 - acc: 0.9646 - val_loss: 0.2697 - val_acc: 0.9558\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1079 - acc: 0.9649 - val_loss: 0.2811 - val_acc: 0.9543\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1060 - acc: 0.9661 - val_loss: 0.2709 - val_acc: 0.9568\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1003 - acc: 0.9665 - val_loss: 0.2776 - val_acc: 0.9563\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.1065 - acc: 0.9648 - val_loss: 0.2703 - val_acc: 0.9561\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.1035 - acc: 0.9649 - val_loss: 0.2758 - val_acc: 0.9556\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 0.1016 - acc: 0.9661 - val_loss: 0.2726 - val_acc: 0.9564\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 0.1068 - acc: 0.9650 - val_loss: 0.2691 - val_acc: 0.9566\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 10s 226us/sample - loss: 0.1070 - acc: 0.9652 - val_loss: 0.2730 - val_acc: 0.9561\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1074 - acc: 0.9650 - val_loss: 0.2700 - val_acc: 0.9563\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1054 - acc: 0.9665 - val_loss: 0.2722 - val_acc: 0.9574\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1079 - acc: 0.9652 - val_loss: 0.2778 - val_acc: 0.9545\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.0996 - acc: 0.9665 - val_loss: 0.2795 - val_acc: 0.9554\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1032 - acc: 0.9662 - val_loss: 0.2797 - val_acc: 0.9550\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1019 - acc: 0.9664 - val_loss: 0.2796 - val_acc: 0.9554\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.1021 - acc: 0.9662 - val_loss: 0.2785 - val_acc: 0.9559\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1010 - acc: 0.9673 - val_loss: 0.2792 - val_acc: 0.9550\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.0970 - acc: 0.9683 - val_loss: 0.2785 - val_acc: 0.9549\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 10s 234us/sample - loss: 0.1030 - acc: 0.9665 - val_loss: 0.2760 - val_acc: 0.9546\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 9s 226us/sample - loss: 0.1036 - acc: 0.9660 - val_loss: 0.2792 - val_acc: 0.9549\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1007 - acc: 0.9672 - val_loss: 0.2754 - val_acc: 0.9560\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.1023 - acc: 0.9660 - val_loss: 0.2765 - val_acc: 0.9555\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.1012 - acc: 0.9664 - val_loss: 0.2735 - val_acc: 0.9569\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1018 - acc: 0.9662 - val_loss: 0.2850 - val_acc: 0.9518\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 0.1038 - acc: 0.9657 - val_loss: 0.2774 - val_acc: 0.9553\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.1030 - acc: 0.9662 - val_loss: 0.2795 - val_acc: 0.9564\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1018 - acc: 0.9662 - val_loss: 0.2772 - val_acc: 0.9559\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.1014 - acc: 0.9668 - val_loss: 0.2815 - val_acc: 0.9541\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1002 - acc: 0.9675 - val_loss: 0.2748 - val_acc: 0.9563\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.0926 - acc: 0.9698 - val_loss: 0.2857 - val_acc: 0.9564\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.0979 - acc: 0.9682 - val_loss: 0.2790 - val_acc: 0.9560\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 10s 227us/sample - loss: 0.0998 - acc: 0.9673 - val_loss: 0.2817 - val_acc: 0.9555\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.0990 - acc: 0.9678 - val_loss: 0.2779 - val_acc: 0.9563\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.0986 - acc: 0.9681 - val_loss: 0.2783 - val_acc: 0.9564\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1032 - acc: 0.9670 - val_loss: 0.2751 - val_acc: 0.9573\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.0988 - acc: 0.9677 - val_loss: 0.2736 - val_acc: 0.9568\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.1020 - acc: 0.9675 - val_loss: 0.2803 - val_acc: 0.9561\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.0954 - acc: 0.9690 - val_loss: 0.2800 - val_acc: 0.9568\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.1025 - acc: 0.9662 - val_loss: 0.2773 - val_acc: 0.9564\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.0994 - acc: 0.9671 - val_loss: 0.2772 - val_acc: 0.9564\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.0998 - acc: 0.9680 - val_loss: 0.2723 - val_acc: 0.9571\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1003 - acc: 0.9678 - val_loss: 0.2703 - val_acc: 0.9579\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.0989 - acc: 0.9673 - val_loss: 0.2831 - val_acc: 0.9566\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 9s 223us/sample - loss: 0.0992 - acc: 0.9682 - val_loss: 0.2765 - val_acc: 0.9563\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.0989 - acc: 0.9676 - val_loss: 0.2739 - val_acc: 0.9584\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 0.0984 - acc: 0.9682 - val_loss: 0.2833 - val_acc: 0.9567\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 10s 228us/sample - loss: 0.0959 - acc: 0.9682 - val_loss: 0.2882 - val_acc: 0.9556\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0930 - acc: 0.9696 - val_loss: 0.2856 - val_acc: 0.9559\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 9s 221us/sample - loss: 0.0998 - acc: 0.9681 - val_loss: 0.2889 - val_acc: 0.9553\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.1006 - acc: 0.9665 - val_loss: 0.2851 - val_acc: 0.9558\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.0969 - acc: 0.9684 - val_loss: 0.2873 - val_acc: 0.9559\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 9s 219us/sample - loss: 0.1023 - acc: 0.9670 - val_loss: 0.2785 - val_acc: 0.9567\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0951 - acc: 0.9692 - val_loss: 0.2775 - val_acc: 0.9565\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0991 - acc: 0.9679 - val_loss: 0.2825 - val_acc: 0.9548\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0950 - acc: 0.9696 - val_loss: 0.2783 - val_acc: 0.9559\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0990 - acc: 0.9683 - val_loss: 0.2773 - val_acc: 0.9566\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0936 - acc: 0.9696 - val_loss: 0.2866 - val_acc: 0.9535\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0939 - acc: 0.9695 - val_loss: 0.2891 - val_acc: 0.9548\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0962 - acc: 0.9690 - val_loss: 0.2866 - val_acc: 0.9569\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0939 - acc: 0.9688 - val_loss: 0.2819 - val_acc: 0.9561\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0937 - acc: 0.9694 - val_loss: 0.2827 - val_acc: 0.9574\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0868 - acc: 0.9720 - val_loss: 0.2828 - val_acc: 0.9570\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0984 - acc: 0.9678 - val_loss: 0.2927 - val_acc: 0.9558\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0947 - acc: 0.9688 - val_loss: 0.2843 - val_acc: 0.9566\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0904 - acc: 0.9708 - val_loss: 0.2868 - val_acc: 0.9556\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0995 - acc: 0.9662 - val_loss: 0.2865 - val_acc: 0.9563\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0966 - acc: 0.9685 - val_loss: 0.2851 - val_acc: 0.9558\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0973 - acc: 0.9685 - val_loss: 0.2813 - val_acc: 0.9562\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0900 - acc: 0.9703 - val_loss: 0.2902 - val_acc: 0.9558\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0922 - acc: 0.9700 - val_loss: 0.2987 - val_acc: 0.9549\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0914 - acc: 0.9706 - val_loss: 0.2786 - val_acc: 0.9578\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0983 - acc: 0.9685 - val_loss: 0.2895 - val_acc: 0.9557\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0972 - acc: 0.9683 - val_loss: 0.2905 - val_acc: 0.9565\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0983 - acc: 0.9683 - val_loss: 0.2902 - val_acc: 0.9565\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0904 - acc: 0.9698 - val_loss: 0.2840 - val_acc: 0.9576\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.0941 - acc: 0.9704 - val_loss: 0.2808 - val_acc: 0.9560\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 9s 224us/sample - loss: 0.0926 - acc: 0.9702 - val_loss: 0.2834 - val_acc: 0.9553\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.0988 - acc: 0.9685 - val_loss: 0.2825 - val_acc: 0.9565\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0894 - acc: 0.9712 - val_loss: 0.2863 - val_acc: 0.9558\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0917 - acc: 0.9703 - val_loss: 0.2838 - val_acc: 0.9562\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0948 - acc: 0.9675 - val_loss: 0.2865 - val_acc: 0.9564\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0914 - acc: 0.9698 - val_loss: 0.2829 - val_acc: 0.9567\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0978 - acc: 0.9682 - val_loss: 0.2853 - val_acc: 0.9567\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0923 - acc: 0.9696 - val_loss: 0.2887 - val_acc: 0.9545\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0882 - acc: 0.9705 - val_loss: 0.2861 - val_acc: 0.9574\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0927 - acc: 0.9701 - val_loss: 0.2904 - val_acc: 0.9567\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0904 - acc: 0.9700 - val_loss: 0.2818 - val_acc: 0.9580\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0878 - acc: 0.9713 - val_loss: 0.2849 - val_acc: 0.9571\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0930 - acc: 0.9705 - val_loss: 0.2878 - val_acc: 0.9569\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0955 - acc: 0.9696 - val_loss: 0.2821 - val_acc: 0.9571\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.0896 - acc: 0.9710 - val_loss: 0.2766 - val_acc: 0.9582\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0873 - acc: 0.9710 - val_loss: 0.2841 - val_acc: 0.9578\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0903 - acc: 0.9702 - val_loss: 0.2791 - val_acc: 0.9578\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0950 - acc: 0.9698 - val_loss: 0.2926 - val_acc: 0.9557\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0943 - acc: 0.9693 - val_loss: 0.2867 - val_acc: 0.9567\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0930 - acc: 0.9701 - val_loss: 0.2821 - val_acc: 0.9565\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0852 - acc: 0.9725 - val_loss: 0.2835 - val_acc: 0.9574\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0912 - acc: 0.9713 - val_loss: 0.2875 - val_acc: 0.9560\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0895 - acc: 0.9712 - val_loss: 0.2890 - val_acc: 0.9562\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 9s 220us/sample - loss: 0.0907 - acc: 0.9718 - val_loss: 0.2865 - val_acc: 0.9567\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.0837 - acc: 0.9728 - val_loss: 0.2833 - val_acc: 0.9582\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0881 - acc: 0.9721 - val_loss: 0.2845 - val_acc: 0.9574\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0883 - acc: 0.9721 - val_loss: 0.2838 - val_acc: 0.9575\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0924 - acc: 0.9707 - val_loss: 0.2859 - val_acc: 0.9568\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0963 - acc: 0.9693 - val_loss: 0.2863 - val_acc: 0.9566\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0863 - acc: 0.9719 - val_loss: 0.2871 - val_acc: 0.9576\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0891 - acc: 0.9707 - val_loss: 0.2907 - val_acc: 0.9567\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0898 - acc: 0.9710 - val_loss: 0.2915 - val_acc: 0.9569\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0876 - acc: 0.9712 - val_loss: 0.2967 - val_acc: 0.9569\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0908 - acc: 0.9709 - val_loss: 0.2847 - val_acc: 0.9571\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0853 - acc: 0.9733 - val_loss: 0.2883 - val_acc: 0.9572\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0921 - acc: 0.9703 - val_loss: 0.2838 - val_acc: 0.9572\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0872 - acc: 0.9721 - val_loss: 0.2907 - val_acc: 0.9567\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0914 - acc: 0.9696 - val_loss: 0.2947 - val_acc: 0.9569\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0888 - acc: 0.9716 - val_loss: 0.2896 - val_acc: 0.9560\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0894 - acc: 0.9708 - val_loss: 0.2825 - val_acc: 0.9564\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 10s 233us/sample - loss: 0.0910 - acc: 0.9708 - val_loss: 0.2833 - val_acc: 0.9573\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0883 - acc: 0.9716 - val_loss: 0.2839 - val_acc: 0.9565\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0937 - acc: 0.9696 - val_loss: 0.2789 - val_acc: 0.9573\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0860 - acc: 0.9726 - val_loss: 0.2887 - val_acc: 0.9574\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0898 - acc: 0.9705 - val_loss: 0.2856 - val_acc: 0.9570\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0887 - acc: 0.9717 - val_loss: 0.2855 - val_acc: 0.9576\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0840 - acc: 0.9724 - val_loss: 0.2849 - val_acc: 0.9585\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0860 - acc: 0.9718 - val_loss: 0.2880 - val_acc: 0.9561\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0911 - acc: 0.9708 - val_loss: 0.2931 - val_acc: 0.9553\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0891 - acc: 0.9700 - val_loss: 0.2888 - val_acc: 0.9569\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0873 - acc: 0.9720 - val_loss: 0.2923 - val_acc: 0.9572\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0904 - acc: 0.9712 - val_loss: 0.2954 - val_acc: 0.9568\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0896 - acc: 0.9713 - val_loss: 0.2897 - val_acc: 0.9574\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0878 - acc: 0.9715 - val_loss: 0.2900 - val_acc: 0.9563\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0854 - acc: 0.9725 - val_loss: 0.2865 - val_acc: 0.9577\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0831 - acc: 0.9730 - val_loss: 0.2906 - val_acc: 0.9568\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0898 - acc: 0.9710 - val_loss: 0.2904 - val_acc: 0.9561\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0836 - acc: 0.9729 - val_loss: 0.2926 - val_acc: 0.9577\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0839 - acc: 0.9740 - val_loss: 0.2919 - val_acc: 0.9573\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.0865 - acc: 0.9717 - val_loss: 0.2940 - val_acc: 0.9567\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.0904 - acc: 0.9709 - val_loss: 0.2855 - val_acc: 0.9564\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0888 - acc: 0.9719 - val_loss: 0.2892 - val_acc: 0.9567\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0903 - acc: 0.9714 - val_loss: 0.2831 - val_acc: 0.9578\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0857 - acc: 0.9722 - val_loss: 0.2921 - val_acc: 0.9572\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0903 - acc: 0.9715 - val_loss: 0.2919 - val_acc: 0.9560\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0906 - acc: 0.9709 - val_loss: 0.2807 - val_acc: 0.9568\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.0879 - acc: 0.9715 - val_loss: 0.2837 - val_acc: 0.9578\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0880 - acc: 0.9707 - val_loss: 0.2945 - val_acc: 0.9569\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.0865 - acc: 0.9714 - val_loss: 0.2879 - val_acc: 0.9569\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0856 - acc: 0.9726 - val_loss: 0.2876 - val_acc: 0.9575\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.0882 - acc: 0.9717 - val_loss: 0.2883 - val_acc: 0.9570\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.0837 - acc: 0.9727 - val_loss: 0.2882 - val_acc: 0.9575\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0877 - acc: 0.9726 - val_loss: 0.2839 - val_acc: 0.9575\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0823 - acc: 0.9733 - val_loss: 0.2983 - val_acc: 0.9577\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0827 - acc: 0.9734 - val_loss: 0.2891 - val_acc: 0.9571\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0845 - acc: 0.9726 - val_loss: 0.2930 - val_acc: 0.9572\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0858 - acc: 0.9717 - val_loss: 0.2850 - val_acc: 0.9575\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0830 - acc: 0.9731 - val_loss: 0.2957 - val_acc: 0.9572\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0841 - acc: 0.9730 - val_loss: 0.2941 - val_acc: 0.9559\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0890 - acc: 0.9713 - val_loss: 0.2915 - val_acc: 0.9559\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0837 - acc: 0.9728 - val_loss: 0.2943 - val_acc: 0.9575\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0846 - acc: 0.9732 - val_loss: 0.2929 - val_acc: 0.9568\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0904 - acc: 0.9713 - val_loss: 0.2894 - val_acc: 0.9564\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0825 - acc: 0.9728 - val_loss: 0.2911 - val_acc: 0.9570\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0856 - acc: 0.9716 - val_loss: 0.2944 - val_acc: 0.9575\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0834 - acc: 0.9733 - val_loss: 0.2940 - val_acc: 0.9568\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0802 - acc: 0.9739 - val_loss: 0.2956 - val_acc: 0.9568\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0851 - acc: 0.9725 - val_loss: 0.2927 - val_acc: 0.9576\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0835 - acc: 0.9736 - val_loss: 0.2995 - val_acc: 0.9558\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0825 - acc: 0.9736 - val_loss: 0.2915 - val_acc: 0.9580\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0797 - acc: 0.9738 - val_loss: 0.2979 - val_acc: 0.9566\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0842 - acc: 0.9728 - val_loss: 0.2908 - val_acc: 0.9564\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0846 - acc: 0.9725 - val_loss: 0.2982 - val_acc: 0.9559\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0832 - acc: 0.9737 - val_loss: 0.2920 - val_acc: 0.9577\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0874 - acc: 0.9718 - val_loss: 0.3008 - val_acc: 0.9569\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0858 - acc: 0.9720 - val_loss: 0.2957 - val_acc: 0.9581\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0818 - acc: 0.9730 - val_loss: 0.2964 - val_acc: 0.9560\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.0837 - acc: 0.9737 - val_loss: 0.3004 - val_acc: 0.9553\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0856 - acc: 0.9729 - val_loss: 0.2991 - val_acc: 0.9546\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.0819 - acc: 0.9735 - val_loss: 0.2933 - val_acc: 0.9573\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.0858 - acc: 0.9717 - val_loss: 0.2965 - val_acc: 0.9560\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0810 - acc: 0.9740 - val_loss: 0.3038 - val_acc: 0.9545\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0832 - acc: 0.9742 - val_loss: 0.2873 - val_acc: 0.9567\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0809 - acc: 0.9738 - val_loss: 0.2927 - val_acc: 0.9577\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0783 - acc: 0.9743 - val_loss: 0.2920 - val_acc: 0.9582\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0823 - acc: 0.9738 - val_loss: 0.2960 - val_acc: 0.9566\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 9s 214us/sample - loss: 0.0830 - acc: 0.9736 - val_loss: 0.2893 - val_acc: 0.9578\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0826 - acc: 0.9740 - val_loss: 0.2959 - val_acc: 0.9564\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0861 - acc: 0.9721 - val_loss: 0.2974 - val_acc: 0.9563\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0856 - acc: 0.9722 - val_loss: 0.2907 - val_acc: 0.9570\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0825 - acc: 0.9733 - val_loss: 0.2885 - val_acc: 0.9568\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0790 - acc: 0.9745 - val_loss: 0.2992 - val_acc: 0.9572\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0792 - acc: 0.9747 - val_loss: 0.2963 - val_acc: 0.9575\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.0792 - acc: 0.9743 - val_loss: 0.2959 - val_acc: 0.9570\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0853 - acc: 0.9722 - val_loss: 0.2950 - val_acc: 0.9555\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0846 - acc: 0.9733 - val_loss: 0.2915 - val_acc: 0.9569\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0835 - acc: 0.9730 - val_loss: 0.2910 - val_acc: 0.9573\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0830 - acc: 0.9731 - val_loss: 0.2938 - val_acc: 0.9576\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 9s 218us/sample - loss: 0.0828 - acc: 0.9739 - val_loss: 0.2899 - val_acc: 0.9581\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0731 - acc: 0.9766 - val_loss: 0.2950 - val_acc: 0.9580\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0809 - acc: 0.9745 - val_loss: 0.2966 - val_acc: 0.9572\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0783 - acc: 0.9751 - val_loss: 0.3009 - val_acc: 0.9567\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0860 - acc: 0.9717 - val_loss: 0.2994 - val_acc: 0.9557\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.0882 - acc: 0.9717 - val_loss: 0.2874 - val_acc: 0.9574\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.0811 - acc: 0.9732 - val_loss: 0.2840 - val_acc: 0.9580\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0718 - acc: 0.9773 - val_loss: 0.2934 - val_acc: 0.9577\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0768 - acc: 0.9756 - val_loss: 0.2915 - val_acc: 0.9573\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0799 - acc: 0.9734 - val_loss: 0.3003 - val_acc: 0.9567\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0772 - acc: 0.9748 - val_loss: 0.2948 - val_acc: 0.9575\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0796 - acc: 0.9743 - val_loss: 0.2979 - val_acc: 0.9573\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0797 - acc: 0.9748 - val_loss: 0.3003 - val_acc: 0.9567\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.0765 - acc: 0.9752 - val_loss: 0.2966 - val_acc: 0.9570\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0787 - acc: 0.9749 - val_loss: 0.2945 - val_acc: 0.9575\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0797 - acc: 0.9745 - val_loss: 0.3009 - val_acc: 0.9560\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 0.0789 - acc: 0.9742 - val_loss: 0.3024 - val_acc: 0.9577\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0795 - acc: 0.9749 - val_loss: 0.3002 - val_acc: 0.9568\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 9s 215us/sample - loss: 0.0824 - acc: 0.9736 - val_loss: 0.2995 - val_acc: 0.9568\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0826 - acc: 0.9729 - val_loss: 0.2999 - val_acc: 0.9559\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 9s 217us/sample - loss: 0.0778 - acc: 0.9746 - val_loss: 0.3013 - val_acc: 0.9568\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 9s 222us/sample - loss: 0.0822 - acc: 0.9735 - val_loss: 0.3038 - val_acc: 0.9569\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 10s 229us/sample - loss: 0.0789 - acc: 0.9750 - val_loss: 0.2950 - val_acc: 0.9581\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0782 - acc: 0.9748 - val_loss: 0.2945 - val_acc: 0.9574\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 9s 216us/sample - loss: 0.0817 - acc: 0.9738 - val_loss: 0.2919 - val_acc: 0.9581\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0823 - acc: 0.9738 - val_loss: 0.2913 - val_acc: 0.9567\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0849 - acc: 0.9725 - val_loss: 0.2935 - val_acc: 0.9573\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0817 - acc: 0.9731 - val_loss: 0.2891 - val_acc: 0.9576\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0737 - acc: 0.9762 - val_loss: 0.2944 - val_acc: 0.9576\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0765 - acc: 0.9758 - val_loss: 0.3072 - val_acc: 0.9562\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0770 - acc: 0.9748 - val_loss: 0.3053 - val_acc: 0.9574\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0815 - acc: 0.9744 - val_loss: 0.2970 - val_acc: 0.9568\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0758 - acc: 0.9764 - val_loss: 0.2951 - val_acc: 0.9574\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.0796 - acc: 0.9752 - val_loss: 0.3023 - val_acc: 0.9567\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0770 - acc: 0.9761 - val_loss: 0.2952 - val_acc: 0.9574\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0837 - acc: 0.9730 - val_loss: 0.3036 - val_acc: 0.9547\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0810 - acc: 0.9742 - val_loss: 0.2926 - val_acc: 0.9577\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0778 - acc: 0.9745 - val_loss: 0.2960 - val_acc: 0.9578\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0824 - acc: 0.9743 - val_loss: 0.2952 - val_acc: 0.9581\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0774 - acc: 0.9751 - val_loss: 0.2997 - val_acc: 0.9569\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0771 - acc: 0.9752 - val_loss: 0.2966 - val_acc: 0.9578\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0764 - acc: 0.9755 - val_loss: 0.2991 - val_acc: 0.9582\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.0824 - acc: 0.9735 - val_loss: 0.3110 - val_acc: 0.9551\n"
     ]
    }
   ],
   "source": [
    "modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[250,200,100,10],['relu','relu','relu','softmax'],500,True,0.019560946340911864,4.715676601623284e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bY7T7xPmLhSH"
   },
   "source": [
    "With increase in number of layers\n",
    "1) The model learns faster. \n",
    "2) The model training accuracy has increased. But Validation accuracy remains the same at 95% . The difference between training and validation accuracy is low to call it overfitting\n",
    "\n",
    "Lets try increasing the number of neurons in the layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8UDccULuoz4K",
    "outputId": "ac62a8c1-f790-44e0-ebfb-236c607f1ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 10s 244us/sample - loss: 1.4500 - acc: 0.5194 - val_loss: 0.9439 - val_acc: 0.7107\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.9473 - acc: 0.7009 - val_loss: 0.7445 - val_acc: 0.7736\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.8130 - acc: 0.7437 - val_loss: 0.6329 - val_acc: 0.8126\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.7287 - acc: 0.7706 - val_loss: 0.5973 - val_acc: 0.8206\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.6742 - acc: 0.7872 - val_loss: 0.5308 - val_acc: 0.8408\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.6275 - acc: 0.8011 - val_loss: 0.5144 - val_acc: 0.8470\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.5981 - acc: 0.8095 - val_loss: 0.5048 - val_acc: 0.8482\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.5671 - acc: 0.8206 - val_loss: 0.4480 - val_acc: 0.8675\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5459 - acc: 0.8263 - val_loss: 0.4601 - val_acc: 0.8616\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.5214 - acc: 0.8347 - val_loss: 0.4311 - val_acc: 0.8729\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.4991 - acc: 0.8408 - val_loss: 0.4552 - val_acc: 0.8628\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.4858 - acc: 0.8439 - val_loss: 0.4038 - val_acc: 0.8794\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4692 - acc: 0.8492 - val_loss: 0.3987 - val_acc: 0.8819\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.4552 - acc: 0.8542 - val_loss: 0.3682 - val_acc: 0.8921\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.4409 - acc: 0.8578 - val_loss: 0.3837 - val_acc: 0.8851\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4216 - acc: 0.8628 - val_loss: 0.3532 - val_acc: 0.8977\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.4105 - acc: 0.8683 - val_loss: 0.3380 - val_acc: 0.9009\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4057 - acc: 0.8690 - val_loss: 0.3238 - val_acc: 0.9071\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.3896 - acc: 0.8748 - val_loss: 0.3409 - val_acc: 0.9009\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.3823 - acc: 0.8754 - val_loss: 0.3176 - val_acc: 0.9085\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.3788 - acc: 0.8772 - val_loss: 0.3255 - val_acc: 0.9049\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.3692 - acc: 0.8800 - val_loss: 0.3072 - val_acc: 0.9135\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.3592 - acc: 0.8844 - val_loss: 0.3048 - val_acc: 0.9140\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.3527 - acc: 0.8867 - val_loss: 0.2910 - val_acc: 0.9179\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.3469 - acc: 0.8863 - val_loss: 0.2843 - val_acc: 0.9227\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.3322 - acc: 0.8925 - val_loss: 0.3051 - val_acc: 0.9151\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.3328 - acc: 0.8916 - val_loss: 0.2994 - val_acc: 0.9160\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.3204 - acc: 0.8935 - val_loss: 0.2977 - val_acc: 0.9195\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.3133 - acc: 0.8968 - val_loss: 0.2960 - val_acc: 0.9182\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.3072 - acc: 0.8985 - val_loss: 0.2710 - val_acc: 0.9272\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.3020 - acc: 0.9005 - val_loss: 0.2733 - val_acc: 0.9276\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2974 - acc: 0.9011 - val_loss: 0.2822 - val_acc: 0.9240\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2986 - acc: 0.9026 - val_loss: 0.2974 - val_acc: 0.9196\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.2886 - acc: 0.9043 - val_loss: 0.2810 - val_acc: 0.9251\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2851 - acc: 0.9061 - val_loss: 0.2668 - val_acc: 0.9304\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2820 - acc: 0.9079 - val_loss: 0.2735 - val_acc: 0.9276\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2759 - acc: 0.9098 - val_loss: 0.2602 - val_acc: 0.9325\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2713 - acc: 0.9101 - val_loss: 0.2765 - val_acc: 0.9280\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2670 - acc: 0.9124 - val_loss: 0.2669 - val_acc: 0.9310\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2600 - acc: 0.9135 - val_loss: 0.2664 - val_acc: 0.9316\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2563 - acc: 0.9159 - val_loss: 0.2694 - val_acc: 0.9299\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2560 - acc: 0.9155 - val_loss: 0.2576 - val_acc: 0.9343\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2503 - acc: 0.9174 - val_loss: 0.2597 - val_acc: 0.9338\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2477 - acc: 0.9188 - val_loss: 0.2504 - val_acc: 0.9369\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.2384 - acc: 0.9197 - val_loss: 0.2481 - val_acc: 0.9408\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2415 - acc: 0.9206 - val_loss: 0.2517 - val_acc: 0.9382\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.2394 - acc: 0.9205 - val_loss: 0.2482 - val_acc: 0.9394\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2319 - acc: 0.9238 - val_loss: 0.2566 - val_acc: 0.9373\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2267 - acc: 0.9253 - val_loss: 0.2473 - val_acc: 0.9406\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2254 - acc: 0.9253 - val_loss: 0.2475 - val_acc: 0.9407\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.2225 - acc: 0.9260 - val_loss: 0.2510 - val_acc: 0.9405\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2199 - acc: 0.9277 - val_loss: 0.2555 - val_acc: 0.9392\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2217 - acc: 0.9243 - val_loss: 0.2511 - val_acc: 0.9404\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2119 - acc: 0.9284 - val_loss: 0.2527 - val_acc: 0.9393\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2194 - acc: 0.9271 - val_loss: 0.2492 - val_acc: 0.9415\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2104 - acc: 0.9285 - val_loss: 0.2614 - val_acc: 0.9373\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.2091 - acc: 0.9300 - val_loss: 0.2511 - val_acc: 0.9426\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.2113 - acc: 0.9301 - val_loss: 0.2459 - val_acc: 0.9436\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.2063 - acc: 0.9318 - val_loss: 0.2537 - val_acc: 0.9404\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1980 - acc: 0.9345 - val_loss: 0.2524 - val_acc: 0.9424\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.2001 - acc: 0.9340 - val_loss: 0.2358 - val_acc: 0.9466\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1987 - acc: 0.9343 - val_loss: 0.2464 - val_acc: 0.9446\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1963 - acc: 0.9346 - val_loss: 0.2538 - val_acc: 0.9420\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1969 - acc: 0.9347 - val_loss: 0.2497 - val_acc: 0.9444\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1963 - acc: 0.9341 - val_loss: 0.2431 - val_acc: 0.9467\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1902 - acc: 0.9365 - val_loss: 0.2539 - val_acc: 0.9435\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.1897 - acc: 0.9374 - val_loss: 0.2543 - val_acc: 0.9421\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1889 - acc: 0.9365 - val_loss: 0.2501 - val_acc: 0.9446\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1889 - acc: 0.9374 - val_loss: 0.2405 - val_acc: 0.9471\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1774 - acc: 0.9402 - val_loss: 0.2518 - val_acc: 0.9452\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1830 - acc: 0.9386 - val_loss: 0.2583 - val_acc: 0.9434\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1791 - acc: 0.9408 - val_loss: 0.2435 - val_acc: 0.9473\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1798 - acc: 0.9406 - val_loss: 0.2428 - val_acc: 0.9482\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1778 - acc: 0.9408 - val_loss: 0.2458 - val_acc: 0.9477\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1724 - acc: 0.9430 - val_loss: 0.2444 - val_acc: 0.9466\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1703 - acc: 0.9439 - val_loss: 0.2503 - val_acc: 0.9470\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1768 - acc: 0.9413 - val_loss: 0.2400 - val_acc: 0.9493\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1708 - acc: 0.9432 - val_loss: 0.2413 - val_acc: 0.9490\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1707 - acc: 0.9439 - val_loss: 0.2509 - val_acc: 0.9480\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1635 - acc: 0.9447 - val_loss: 0.2446 - val_acc: 0.9490\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1672 - acc: 0.9441 - val_loss: 0.2457 - val_acc: 0.9487\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1668 - acc: 0.9436 - val_loss: 0.2474 - val_acc: 0.9484\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.1632 - acc: 0.9460 - val_loss: 0.2515 - val_acc: 0.9487\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.1637 - acc: 0.9458 - val_loss: 0.2444 - val_acc: 0.9492\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1568 - acc: 0.9477 - val_loss: 0.2600 - val_acc: 0.9448\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1634 - acc: 0.9450 - val_loss: 0.2477 - val_acc: 0.9502\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1607 - acc: 0.9459 - val_loss: 0.2439 - val_acc: 0.9505\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1637 - acc: 0.9465 - val_loss: 0.2513 - val_acc: 0.9485\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1612 - acc: 0.9471 - val_loss: 0.2531 - val_acc: 0.9483\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1532 - acc: 0.9491 - val_loss: 0.2480 - val_acc: 0.9507\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1578 - acc: 0.9471 - val_loss: 0.2545 - val_acc: 0.9491\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1502 - acc: 0.9513 - val_loss: 0.2478 - val_acc: 0.9495\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1524 - acc: 0.9497 - val_loss: 0.2450 - val_acc: 0.9522\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1478 - acc: 0.9505 - val_loss: 0.2427 - val_acc: 0.9541\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1504 - acc: 0.9500 - val_loss: 0.2563 - val_acc: 0.9503\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1483 - acc: 0.9513 - val_loss: 0.2531 - val_acc: 0.9491\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1510 - acc: 0.9505 - val_loss: 0.2506 - val_acc: 0.9518\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1450 - acc: 0.9515 - val_loss: 0.2541 - val_acc: 0.9503\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1447 - acc: 0.9533 - val_loss: 0.2530 - val_acc: 0.9512\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1384 - acc: 0.9542 - val_loss: 0.2540 - val_acc: 0.9505\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1460 - acc: 0.9524 - val_loss: 0.2619 - val_acc: 0.9496\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1452 - acc: 0.9518 - val_loss: 0.2585 - val_acc: 0.9495\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1457 - acc: 0.9519 - val_loss: 0.2519 - val_acc: 0.9529\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1455 - acc: 0.9529 - val_loss: 0.2537 - val_acc: 0.9520\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1347 - acc: 0.9541 - val_loss: 0.2597 - val_acc: 0.9508\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1389 - acc: 0.9544 - val_loss: 0.2502 - val_acc: 0.9534\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1393 - acc: 0.9536 - val_loss: 0.2606 - val_acc: 0.9502\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1330 - acc: 0.9564 - val_loss: 0.2539 - val_acc: 0.9530\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1348 - acc: 0.9551 - val_loss: 0.2600 - val_acc: 0.9510\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1374 - acc: 0.9550 - val_loss: 0.2568 - val_acc: 0.9523\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1332 - acc: 0.9563 - val_loss: 0.2629 - val_acc: 0.9518\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1385 - acc: 0.9546 - val_loss: 0.2591 - val_acc: 0.9521\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1395 - acc: 0.9537 - val_loss: 0.2531 - val_acc: 0.9536\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1372 - acc: 0.9532 - val_loss: 0.2673 - val_acc: 0.9516\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1314 - acc: 0.9560 - val_loss: 0.2546 - val_acc: 0.9523\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1297 - acc: 0.9586 - val_loss: 0.2545 - val_acc: 0.9538\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1257 - acc: 0.9593 - val_loss: 0.2589 - val_acc: 0.9529\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1299 - acc: 0.9567 - val_loss: 0.2581 - val_acc: 0.9527\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1323 - acc: 0.9561 - val_loss: 0.2592 - val_acc: 0.9529\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.1313 - acc: 0.9568 - val_loss: 0.2622 - val_acc: 0.9523\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.1298 - acc: 0.9566 - val_loss: 0.2646 - val_acc: 0.9517\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1309 - acc: 0.9570 - val_loss: 0.2563 - val_acc: 0.9535\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1192 - acc: 0.9611 - val_loss: 0.2619 - val_acc: 0.9530\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1281 - acc: 0.9581 - val_loss: 0.2681 - val_acc: 0.9522\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1273 - acc: 0.9583 - val_loss: 0.2650 - val_acc: 0.9527\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1248 - acc: 0.9587 - val_loss: 0.2600 - val_acc: 0.9531\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1225 - acc: 0.9594 - val_loss: 0.2682 - val_acc: 0.9531\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1293 - acc: 0.9582 - val_loss: 0.2651 - val_acc: 0.9506\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1322 - acc: 0.9565 - val_loss: 0.2568 - val_acc: 0.9535\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1200 - acc: 0.9606 - val_loss: 0.2692 - val_acc: 0.9525\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1206 - acc: 0.9607 - val_loss: 0.2632 - val_acc: 0.9543\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1264 - acc: 0.9583 - val_loss: 0.2599 - val_acc: 0.9534\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1219 - acc: 0.9605 - val_loss: 0.2616 - val_acc: 0.9543\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1231 - acc: 0.9593 - val_loss: 0.2686 - val_acc: 0.9532\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1177 - acc: 0.9604 - val_loss: 0.2606 - val_acc: 0.9538\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1183 - acc: 0.9609 - val_loss: 0.2546 - val_acc: 0.9563\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1147 - acc: 0.9622 - val_loss: 0.2617 - val_acc: 0.9556\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.1194 - acc: 0.9609 - val_loss: 0.2699 - val_acc: 0.9542\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1180 - acc: 0.9611 - val_loss: 0.2704 - val_acc: 0.9536\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1166 - acc: 0.9607 - val_loss: 0.2634 - val_acc: 0.9559\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.1175 - acc: 0.9610 - val_loss: 0.2749 - val_acc: 0.9520\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1180 - acc: 0.9609 - val_loss: 0.2624 - val_acc: 0.9543\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1211 - acc: 0.9599 - val_loss: 0.2729 - val_acc: 0.9527\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1137 - acc: 0.9631 - val_loss: 0.2625 - val_acc: 0.9549\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.1170 - acc: 0.9618 - val_loss: 0.2637 - val_acc: 0.9548\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1180 - acc: 0.9609 - val_loss: 0.2724 - val_acc: 0.9519\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1197 - acc: 0.9600 - val_loss: 0.2608 - val_acc: 0.9552\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1134 - acc: 0.9633 - val_loss: 0.2709 - val_acc: 0.9539\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1120 - acc: 0.9630 - val_loss: 0.2627 - val_acc: 0.9554\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1136 - acc: 0.9620 - val_loss: 0.2636 - val_acc: 0.9532\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1117 - acc: 0.9637 - val_loss: 0.2688 - val_acc: 0.9544\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1161 - acc: 0.9621 - val_loss: 0.2650 - val_acc: 0.9549\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.1099 - acc: 0.9644 - val_loss: 0.2651 - val_acc: 0.9552\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.1131 - acc: 0.9631 - val_loss: 0.2735 - val_acc: 0.9550\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1161 - acc: 0.9626 - val_loss: 0.2742 - val_acc: 0.9519\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1068 - acc: 0.9649 - val_loss: 0.2660 - val_acc: 0.9547\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1083 - acc: 0.9649 - val_loss: 0.2758 - val_acc: 0.9547\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.1071 - acc: 0.9643 - val_loss: 0.2693 - val_acc: 0.9550\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1066 - acc: 0.9651 - val_loss: 0.2713 - val_acc: 0.9562\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1104 - acc: 0.9634 - val_loss: 0.2672 - val_acc: 0.9553\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1044 - acc: 0.9662 - val_loss: 0.2782 - val_acc: 0.9545\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1078 - acc: 0.9646 - val_loss: 0.2666 - val_acc: 0.9551\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1132 - acc: 0.9632 - val_loss: 0.2723 - val_acc: 0.9543\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1096 - acc: 0.9641 - val_loss: 0.2700 - val_acc: 0.9540\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1094 - acc: 0.9645 - val_loss: 0.2634 - val_acc: 0.9565\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1055 - acc: 0.9651 - val_loss: 0.2692 - val_acc: 0.9554\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1046 - acc: 0.9659 - val_loss: 0.2715 - val_acc: 0.9555\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1079 - acc: 0.9643 - val_loss: 0.2706 - val_acc: 0.9544\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.1117 - acc: 0.9641 - val_loss: 0.2711 - val_acc: 0.9558\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1069 - acc: 0.9656 - val_loss: 0.2774 - val_acc: 0.9538\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1016 - acc: 0.9667 - val_loss: 0.2694 - val_acc: 0.9558\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.1025 - acc: 0.9662 - val_loss: 0.2716 - val_acc: 0.9552\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.1055 - acc: 0.9653 - val_loss: 0.2752 - val_acc: 0.9545\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.1070 - acc: 0.9650 - val_loss: 0.2694 - val_acc: 0.9546\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1077 - acc: 0.9646 - val_loss: 0.2693 - val_acc: 0.9560\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1076 - acc: 0.9649 - val_loss: 0.2807 - val_acc: 0.9567\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1004 - acc: 0.9668 - val_loss: 0.2787 - val_acc: 0.9541\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0987 - acc: 0.9682 - val_loss: 0.2720 - val_acc: 0.9556\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1039 - acc: 0.9671 - val_loss: 0.2821 - val_acc: 0.9544\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0966 - acc: 0.9680 - val_loss: 0.2811 - val_acc: 0.9534\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0982 - acc: 0.9676 - val_loss: 0.2823 - val_acc: 0.9542\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1002 - acc: 0.9675 - val_loss: 0.2739 - val_acc: 0.9561\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0937 - acc: 0.9691 - val_loss: 0.2834 - val_acc: 0.9552\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0969 - acc: 0.9686 - val_loss: 0.2822 - val_acc: 0.9546\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.1012 - acc: 0.9661 - val_loss: 0.2749 - val_acc: 0.9553\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0984 - acc: 0.9678 - val_loss: 0.2767 - val_acc: 0.9559\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0975 - acc: 0.9685 - val_loss: 0.2831 - val_acc: 0.9550\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.1014 - acc: 0.9667 - val_loss: 0.2729 - val_acc: 0.9561\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0924 - acc: 0.9695 - val_loss: 0.2837 - val_acc: 0.9557\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0999 - acc: 0.9666 - val_loss: 0.2832 - val_acc: 0.9548\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0926 - acc: 0.9705 - val_loss: 0.2859 - val_acc: 0.9559\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0996 - acc: 0.9666 - val_loss: 0.2858 - val_acc: 0.9542\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.1000 - acc: 0.9672 - val_loss: 0.2833 - val_acc: 0.9542\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0969 - acc: 0.9683 - val_loss: 0.2854 - val_acc: 0.9544\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0974 - acc: 0.9682 - val_loss: 0.2793 - val_acc: 0.9553\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0939 - acc: 0.9692 - val_loss: 0.2767 - val_acc: 0.9563\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0937 - acc: 0.9693 - val_loss: 0.2794 - val_acc: 0.9545\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.0926 - acc: 0.9705 - val_loss: 0.2794 - val_acc: 0.9567\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0951 - acc: 0.9689 - val_loss: 0.2840 - val_acc: 0.9553\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0968 - acc: 0.9680 - val_loss: 0.2801 - val_acc: 0.9560\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0860 - acc: 0.9719 - val_loss: 0.2865 - val_acc: 0.9549\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0944 - acc: 0.9692 - val_loss: 0.2929 - val_acc: 0.9538\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0914 - acc: 0.9693 - val_loss: 0.2850 - val_acc: 0.9565\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0935 - acc: 0.9704 - val_loss: 0.2807 - val_acc: 0.9553\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0959 - acc: 0.9687 - val_loss: 0.2826 - val_acc: 0.9552\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0953 - acc: 0.9684 - val_loss: 0.2850 - val_acc: 0.9551\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0934 - acc: 0.9695 - val_loss: 0.2893 - val_acc: 0.9543\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0919 - acc: 0.9694 - val_loss: 0.2760 - val_acc: 0.9570\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0947 - acc: 0.9697 - val_loss: 0.2808 - val_acc: 0.9560\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0891 - acc: 0.9711 - val_loss: 0.2811 - val_acc: 0.9570\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0881 - acc: 0.9712 - val_loss: 0.2833 - val_acc: 0.9569\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0886 - acc: 0.9707 - val_loss: 0.2783 - val_acc: 0.9566\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0904 - acc: 0.9703 - val_loss: 0.2818 - val_acc: 0.9544\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0889 - acc: 0.9713 - val_loss: 0.2819 - val_acc: 0.9564\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0875 - acc: 0.9710 - val_loss: 0.2815 - val_acc: 0.9559\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0902 - acc: 0.9710 - val_loss: 0.2826 - val_acc: 0.9555\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0888 - acc: 0.9714 - val_loss: 0.2841 - val_acc: 0.9549\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0854 - acc: 0.9720 - val_loss: 0.2772 - val_acc: 0.9573\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0897 - acc: 0.9713 - val_loss: 0.2850 - val_acc: 0.9561\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0880 - acc: 0.9709 - val_loss: 0.2856 - val_acc: 0.9550\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0909 - acc: 0.9697 - val_loss: 0.2920 - val_acc: 0.9536\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0876 - acc: 0.9719 - val_loss: 0.2798 - val_acc: 0.9557\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0862 - acc: 0.9718 - val_loss: 0.2839 - val_acc: 0.9548\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0937 - acc: 0.9702 - val_loss: 0.2835 - val_acc: 0.9553\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0923 - acc: 0.9696 - val_loss: 0.2853 - val_acc: 0.9556\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0882 - acc: 0.9708 - val_loss: 0.2785 - val_acc: 0.9571\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0932 - acc: 0.9703 - val_loss: 0.2861 - val_acc: 0.9553\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0864 - acc: 0.9722 - val_loss: 0.2922 - val_acc: 0.9544\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0838 - acc: 0.9727 - val_loss: 0.2976 - val_acc: 0.9538\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0855 - acc: 0.9722 - val_loss: 0.2873 - val_acc: 0.9553\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0826 - acc: 0.9731 - val_loss: 0.2873 - val_acc: 0.9562\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0899 - acc: 0.9706 - val_loss: 0.2872 - val_acc: 0.9566\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0798 - acc: 0.9739 - val_loss: 0.2902 - val_acc: 0.9549\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0828 - acc: 0.9728 - val_loss: 0.2887 - val_acc: 0.9564\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0831 - acc: 0.9728 - val_loss: 0.2977 - val_acc: 0.9549\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0856 - acc: 0.9718 - val_loss: 0.2876 - val_acc: 0.9558\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0832 - acc: 0.9730 - val_loss: 0.2830 - val_acc: 0.9572\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0864 - acc: 0.9723 - val_loss: 0.2943 - val_acc: 0.9556\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0838 - acc: 0.9726 - val_loss: 0.2859 - val_acc: 0.9566\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0827 - acc: 0.9730 - val_loss: 0.2968 - val_acc: 0.9559\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0844 - acc: 0.9723 - val_loss: 0.3008 - val_acc: 0.9552\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0852 - acc: 0.9722 - val_loss: 0.2940 - val_acc: 0.9563\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0883 - acc: 0.9717 - val_loss: 0.2929 - val_acc: 0.9556\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0856 - acc: 0.9724 - val_loss: 0.2915 - val_acc: 0.9563\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0843 - acc: 0.9731 - val_loss: 0.2878 - val_acc: 0.9568\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0765 - acc: 0.9751 - val_loss: 0.2868 - val_acc: 0.9567\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0835 - acc: 0.9721 - val_loss: 0.2936 - val_acc: 0.9549\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0843 - acc: 0.9724 - val_loss: 0.3040 - val_acc: 0.9549\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0848 - acc: 0.9723 - val_loss: 0.3021 - val_acc: 0.9550\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0852 - acc: 0.9715 - val_loss: 0.2998 - val_acc: 0.9555\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0788 - acc: 0.9749 - val_loss: 0.2955 - val_acc: 0.9569\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0852 - acc: 0.9726 - val_loss: 0.2858 - val_acc: 0.9575\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0829 - acc: 0.9735 - val_loss: 0.2981 - val_acc: 0.9546\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0790 - acc: 0.9753 - val_loss: 0.2924 - val_acc: 0.9562\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0845 - acc: 0.9726 - val_loss: 0.2921 - val_acc: 0.9560\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0810 - acc: 0.9739 - val_loss: 0.2969 - val_acc: 0.9553\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0784 - acc: 0.9744 - val_loss: 0.2978 - val_acc: 0.9558\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0793 - acc: 0.9741 - val_loss: 0.3011 - val_acc: 0.9556\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0823 - acc: 0.9733 - val_loss: 0.2908 - val_acc: 0.9548\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0802 - acc: 0.9747 - val_loss: 0.2983 - val_acc: 0.9561\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0786 - acc: 0.9748 - val_loss: 0.2920 - val_acc: 0.9563\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0751 - acc: 0.9760 - val_loss: 0.2985 - val_acc: 0.9559\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0761 - acc: 0.9748 - val_loss: 0.3048 - val_acc: 0.9549\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0781 - acc: 0.9747 - val_loss: 0.2934 - val_acc: 0.9553\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0821 - acc: 0.9732 - val_loss: 0.2982 - val_acc: 0.9567\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0763 - acc: 0.9750 - val_loss: 0.2978 - val_acc: 0.9571\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0762 - acc: 0.9751 - val_loss: 0.2969 - val_acc: 0.9566\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0821 - acc: 0.9738 - val_loss: 0.3126 - val_acc: 0.9549\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0830 - acc: 0.9729 - val_loss: 0.2940 - val_acc: 0.9562\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0763 - acc: 0.9756 - val_loss: 0.3117 - val_acc: 0.9559\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0800 - acc: 0.9741 - val_loss: 0.3075 - val_acc: 0.9552\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0778 - acc: 0.9754 - val_loss: 0.2977 - val_acc: 0.9571\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0772 - acc: 0.9745 - val_loss: 0.3024 - val_acc: 0.9569\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0775 - acc: 0.9749 - val_loss: 0.3024 - val_acc: 0.9560\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0771 - acc: 0.9750 - val_loss: 0.2955 - val_acc: 0.9568\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0758 - acc: 0.9751 - val_loss: 0.2900 - val_acc: 0.9571\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0736 - acc: 0.9771 - val_loss: 0.2996 - val_acc: 0.9542\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0746 - acc: 0.9765 - val_loss: 0.3038 - val_acc: 0.9565\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0795 - acc: 0.9745 - val_loss: 0.3008 - val_acc: 0.9565\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0764 - acc: 0.9758 - val_loss: 0.2960 - val_acc: 0.9574\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0773 - acc: 0.9751 - val_loss: 0.3093 - val_acc: 0.9549\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0765 - acc: 0.9745 - val_loss: 0.3019 - val_acc: 0.9564\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0776 - acc: 0.9751 - val_loss: 0.3030 - val_acc: 0.9559\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0763 - acc: 0.9754 - val_loss: 0.2980 - val_acc: 0.9555\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0730 - acc: 0.9767 - val_loss: 0.2994 - val_acc: 0.9565\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0750 - acc: 0.9755 - val_loss: 0.3138 - val_acc: 0.9558\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0769 - acc: 0.9756 - val_loss: 0.3070 - val_acc: 0.9556\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0736 - acc: 0.9764 - val_loss: 0.3040 - val_acc: 0.9566\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0722 - acc: 0.9770 - val_loss: 0.3131 - val_acc: 0.9554\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0772 - acc: 0.9750 - val_loss: 0.3075 - val_acc: 0.9563\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0738 - acc: 0.9767 - val_loss: 0.3045 - val_acc: 0.9560\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0786 - acc: 0.9755 - val_loss: 0.3009 - val_acc: 0.9564\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0792 - acc: 0.9744 - val_loss: 0.3055 - val_acc: 0.9550\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0738 - acc: 0.9765 - val_loss: 0.3031 - val_acc: 0.9561\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0748 - acc: 0.9760 - val_loss: 0.3062 - val_acc: 0.9571\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0745 - acc: 0.9759 - val_loss: 0.3078 - val_acc: 0.9560\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0748 - acc: 0.9758 - val_loss: 0.3033 - val_acc: 0.9565\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0743 - acc: 0.9758 - val_loss: 0.2986 - val_acc: 0.9559\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0771 - acc: 0.9746 - val_loss: 0.3004 - val_acc: 0.9571\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0744 - acc: 0.9762 - val_loss: 0.3003 - val_acc: 0.9569\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0732 - acc: 0.9767 - val_loss: 0.2996 - val_acc: 0.9566\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0756 - acc: 0.9772 - val_loss: 0.3038 - val_acc: 0.9564\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0751 - acc: 0.9760 - val_loss: 0.3015 - val_acc: 0.9573\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0703 - acc: 0.9772 - val_loss: 0.3117 - val_acc: 0.9561\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0717 - acc: 0.9765 - val_loss: 0.3079 - val_acc: 0.9562\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0713 - acc: 0.9774 - val_loss: 0.3043 - val_acc: 0.9559\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0719 - acc: 0.9770 - val_loss: 0.3110 - val_acc: 0.9559\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0729 - acc: 0.9764 - val_loss: 0.3124 - val_acc: 0.9559\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0742 - acc: 0.9767 - val_loss: 0.3096 - val_acc: 0.9563\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0704 - acc: 0.9773 - val_loss: 0.3069 - val_acc: 0.9566\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0732 - acc: 0.9767 - val_loss: 0.3080 - val_acc: 0.9560\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0699 - acc: 0.9778 - val_loss: 0.3144 - val_acc: 0.9561\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0764 - acc: 0.9752 - val_loss: 0.3088 - val_acc: 0.9556\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0705 - acc: 0.9774 - val_loss: 0.3093 - val_acc: 0.9561\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0692 - acc: 0.9774 - val_loss: 0.3112 - val_acc: 0.9562\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0703 - acc: 0.9778 - val_loss: 0.3159 - val_acc: 0.9551\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0717 - acc: 0.9772 - val_loss: 0.3165 - val_acc: 0.9564\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0748 - acc: 0.9767 - val_loss: 0.3084 - val_acc: 0.9555\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0668 - acc: 0.9786 - val_loss: 0.3020 - val_acc: 0.9563\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0705 - acc: 0.9777 - val_loss: 0.3068 - val_acc: 0.9562\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0691 - acc: 0.9780 - val_loss: 0.3039 - val_acc: 0.9575\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0663 - acc: 0.9789 - val_loss: 0.3088 - val_acc: 0.9559\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0696 - acc: 0.9781 - val_loss: 0.3073 - val_acc: 0.9565\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0689 - acc: 0.9786 - val_loss: 0.3171 - val_acc: 0.9546\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0698 - acc: 0.9778 - val_loss: 0.3131 - val_acc: 0.9557\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0717 - acc: 0.9773 - val_loss: 0.3092 - val_acc: 0.9568\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0684 - acc: 0.9771 - val_loss: 0.3059 - val_acc: 0.9567\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.0735 - acc: 0.9767 - val_loss: 0.3074 - val_acc: 0.9568\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0692 - acc: 0.9775 - val_loss: 0.3049 - val_acc: 0.9559\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0721 - acc: 0.9775 - val_loss: 0.3079 - val_acc: 0.9570\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0697 - acc: 0.9771 - val_loss: 0.3100 - val_acc: 0.9568\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0725 - acc: 0.9770 - val_loss: 0.3043 - val_acc: 0.9569\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0685 - acc: 0.9788 - val_loss: 0.3002 - val_acc: 0.9577\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0709 - acc: 0.9771 - val_loss: 0.3127 - val_acc: 0.9567\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0655 - acc: 0.9794 - val_loss: 0.3038 - val_acc: 0.9573\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0700 - acc: 0.9776 - val_loss: 0.3088 - val_acc: 0.9568\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0686 - acc: 0.9774 - val_loss: 0.3207 - val_acc: 0.9561\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0727 - acc: 0.9767 - val_loss: 0.3055 - val_acc: 0.9569\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0707 - acc: 0.9775 - val_loss: 0.3105 - val_acc: 0.9564\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0662 - acc: 0.9794 - val_loss: 0.3037 - val_acc: 0.9564\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0687 - acc: 0.9775 - val_loss: 0.3097 - val_acc: 0.9572\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0689 - acc: 0.9784 - val_loss: 0.3083 - val_acc: 0.9570\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0704 - acc: 0.9772 - val_loss: 0.3053 - val_acc: 0.9574\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0692 - acc: 0.9772 - val_loss: 0.3131 - val_acc: 0.9567\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0649 - acc: 0.9793 - val_loss: 0.3162 - val_acc: 0.9556\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0667 - acc: 0.9786 - val_loss: 0.3127 - val_acc: 0.9565\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0718 - acc: 0.9778 - val_loss: 0.3151 - val_acc: 0.9561\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0638 - acc: 0.9798 - val_loss: 0.3171 - val_acc: 0.9576\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0687 - acc: 0.9781 - val_loss: 0.3148 - val_acc: 0.9566\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0673 - acc: 0.9787 - val_loss: 0.3060 - val_acc: 0.9574\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0627 - acc: 0.9801 - val_loss: 0.3130 - val_acc: 0.9570\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0651 - acc: 0.9800 - val_loss: 0.3155 - val_acc: 0.9569\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0647 - acc: 0.9789 - val_loss: 0.3217 - val_acc: 0.9563\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0690 - acc: 0.9779 - val_loss: 0.3153 - val_acc: 0.9566\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0708 - acc: 0.9780 - val_loss: 0.3100 - val_acc: 0.9567\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0630 - acc: 0.9802 - val_loss: 0.3061 - val_acc: 0.9574\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0597 - acc: 0.9810 - val_loss: 0.3152 - val_acc: 0.9568\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0664 - acc: 0.9785 - val_loss: 0.3207 - val_acc: 0.9560\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0632 - acc: 0.9795 - val_loss: 0.3212 - val_acc: 0.9557\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0682 - acc: 0.9784 - val_loss: 0.3108 - val_acc: 0.9566\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0671 - acc: 0.9788 - val_loss: 0.3194 - val_acc: 0.9560\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0609 - acc: 0.9807 - val_loss: 0.3133 - val_acc: 0.9566\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0635 - acc: 0.9793 - val_loss: 0.3165 - val_acc: 0.9570\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0681 - acc: 0.9786 - val_loss: 0.3173 - val_acc: 0.9559\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0652 - acc: 0.9784 - val_loss: 0.3145 - val_acc: 0.9566\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0583 - acc: 0.9810 - val_loss: 0.3307 - val_acc: 0.9553\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0640 - acc: 0.9788 - val_loss: 0.3234 - val_acc: 0.9554\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0642 - acc: 0.9790 - val_loss: 0.3225 - val_acc: 0.9566\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0687 - acc: 0.9779 - val_loss: 0.3235 - val_acc: 0.9560\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0627 - acc: 0.9795 - val_loss: 0.3194 - val_acc: 0.9564\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0675 - acc: 0.9783 - val_loss: 0.3160 - val_acc: 0.9577\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0632 - acc: 0.9805 - val_loss: 0.3125 - val_acc: 0.9580\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0652 - acc: 0.9792 - val_loss: 0.3117 - val_acc: 0.9571\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0613 - acc: 0.9804 - val_loss: 0.3205 - val_acc: 0.9570\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0643 - acc: 0.9800 - val_loss: 0.3260 - val_acc: 0.9565\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 0.0702 - acc: 0.9782 - val_loss: 0.3155 - val_acc: 0.9571\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0627 - acc: 0.9800 - val_loss: 0.3129 - val_acc: 0.9570\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0662 - acc: 0.9783 - val_loss: 0.3194 - val_acc: 0.9570\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0640 - acc: 0.9803 - val_loss: 0.3133 - val_acc: 0.9557\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0649 - acc: 0.9791 - val_loss: 0.3172 - val_acc: 0.9564\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0569 - acc: 0.9820 - val_loss: 0.3116 - val_acc: 0.9571\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0565 - acc: 0.9822 - val_loss: 0.3249 - val_acc: 0.9556\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0647 - acc: 0.9794 - val_loss: 0.3246 - val_acc: 0.9566\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0637 - acc: 0.9801 - val_loss: 0.3183 - val_acc: 0.9572\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0621 - acc: 0.9798 - val_loss: 0.3112 - val_acc: 0.9575\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0629 - acc: 0.9806 - val_loss: 0.3210 - val_acc: 0.9560\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0680 - acc: 0.9784 - val_loss: 0.3166 - val_acc: 0.9567\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0618 - acc: 0.9800 - val_loss: 0.3210 - val_acc: 0.9562\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0634 - acc: 0.9800 - val_loss: 0.3184 - val_acc: 0.9566\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0638 - acc: 0.9797 - val_loss: 0.3214 - val_acc: 0.9565\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0634 - acc: 0.9805 - val_loss: 0.3229 - val_acc: 0.9570\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0574 - acc: 0.9820 - val_loss: 0.3200 - val_acc: 0.9571\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0628 - acc: 0.9798 - val_loss: 0.3218 - val_acc: 0.9556\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0642 - acc: 0.9790 - val_loss: 0.3203 - val_acc: 0.9566\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0602 - acc: 0.9808 - val_loss: 0.3249 - val_acc: 0.9574\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0642 - acc: 0.9802 - val_loss: 0.3191 - val_acc: 0.9580\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0572 - acc: 0.9815 - val_loss: 0.3258 - val_acc: 0.9561\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0648 - acc: 0.9793 - val_loss: 0.3248 - val_acc: 0.9571\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0616 - acc: 0.9802 - val_loss: 0.3259 - val_acc: 0.9558\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0603 - acc: 0.9800 - val_loss: 0.3293 - val_acc: 0.9553\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0634 - acc: 0.9800 - val_loss: 0.3276 - val_acc: 0.9566\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0646 - acc: 0.9800 - val_loss: 0.3194 - val_acc: 0.9559\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.0630 - acc: 0.9802 - val_loss: 0.3271 - val_acc: 0.9556\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0640 - acc: 0.9796 - val_loss: 0.3286 - val_acc: 0.9555\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0653 - acc: 0.9795 - val_loss: 0.3291 - val_acc: 0.9557\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0595 - acc: 0.9809 - val_loss: 0.3219 - val_acc: 0.9569\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0631 - acc: 0.9802 - val_loss: 0.3213 - val_acc: 0.9560\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0597 - acc: 0.9818 - val_loss: 0.3312 - val_acc: 0.9553\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0612 - acc: 0.9808 - val_loss: 0.3232 - val_acc: 0.9560\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0598 - acc: 0.9815 - val_loss: 0.3264 - val_acc: 0.9553\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0638 - acc: 0.9797 - val_loss: 0.3284 - val_acc: 0.9562\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 0.0600 - acc: 0.9812 - val_loss: 0.3214 - val_acc: 0.9566\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0582 - acc: 0.9820 - val_loss: 0.3215 - val_acc: 0.9564\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0630 - acc: 0.9805 - val_loss: 0.3219 - val_acc: 0.9560\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0603 - acc: 0.9815 - val_loss: 0.3207 - val_acc: 0.9564\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0606 - acc: 0.9806 - val_loss: 0.3252 - val_acc: 0.9559\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0599 - acc: 0.9809 - val_loss: 0.3179 - val_acc: 0.9566\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0601 - acc: 0.9816 - val_loss: 0.3219 - val_acc: 0.9562\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0586 - acc: 0.9817 - val_loss: 0.3220 - val_acc: 0.9569\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0604 - acc: 0.9812 - val_loss: 0.3166 - val_acc: 0.9570\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0630 - acc: 0.9802 - val_loss: 0.3263 - val_acc: 0.9558\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0571 - acc: 0.9816 - val_loss: 0.3303 - val_acc: 0.9562\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0599 - acc: 0.9806 - val_loss: 0.3295 - val_acc: 0.9569\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0635 - acc: 0.9801 - val_loss: 0.3152 - val_acc: 0.9567\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0561 - acc: 0.9820 - val_loss: 0.3215 - val_acc: 0.9569\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0650 - acc: 0.9800 - val_loss: 0.3234 - val_acc: 0.9569\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0611 - acc: 0.9809 - val_loss: 0.3292 - val_acc: 0.9560\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0635 - acc: 0.9806 - val_loss: 0.3187 - val_acc: 0.9568\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0577 - acc: 0.9815 - val_loss: 0.3233 - val_acc: 0.9575\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0588 - acc: 0.9813 - val_loss: 0.3241 - val_acc: 0.9572\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0609 - acc: 0.9814 - val_loss: 0.3265 - val_acc: 0.9564\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0602 - acc: 0.9809 - val_loss: 0.3262 - val_acc: 0.9555\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0582 - acc: 0.9818 - val_loss: 0.3163 - val_acc: 0.9571\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0578 - acc: 0.9819 - val_loss: 0.3186 - val_acc: 0.9566\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0583 - acc: 0.9815 - val_loss: 0.3248 - val_acc: 0.9562\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0536 - acc: 0.9830 - val_loss: 0.3236 - val_acc: 0.9566\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0621 - acc: 0.9804 - val_loss: 0.3170 - val_acc: 0.9567\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0572 - acc: 0.9815 - val_loss: 0.3260 - val_acc: 0.9567\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0561 - acc: 0.9825 - val_loss: 0.3243 - val_acc: 0.9560\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0606 - acc: 0.9813 - val_loss: 0.3258 - val_acc: 0.9571\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0548 - acc: 0.9822 - val_loss: 0.3274 - val_acc: 0.9577\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0598 - acc: 0.9809 - val_loss: 0.3324 - val_acc: 0.9564\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0593 - acc: 0.9814 - val_loss: 0.3255 - val_acc: 0.9568\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0562 - acc: 0.9819 - val_loss: 0.3263 - val_acc: 0.9553\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0617 - acc: 0.9811 - val_loss: 0.3258 - val_acc: 0.9564\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0529 - acc: 0.9837 - val_loss: 0.3288 - val_acc: 0.9574\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0566 - acc: 0.9824 - val_loss: 0.3312 - val_acc: 0.9557\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.0601 - acc: 0.9815 - val_loss: 0.3263 - val_acc: 0.9571\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 9s 212us/sample - loss: 0.0555 - acc: 0.9826 - val_loss: 0.3325 - val_acc: 0.9564\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0596 - acc: 0.9816 - val_loss: 0.3402 - val_acc: 0.9568\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.0574 - acc: 0.9819 - val_loss: 0.3311 - val_acc: 0.9560\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0627 - acc: 0.9810 - val_loss: 0.3249 - val_acc: 0.9574\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.0593 - acc: 0.9812 - val_loss: 0.3308 - val_acc: 0.9558\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0589 - acc: 0.9812 - val_loss: 0.3259 - val_acc: 0.9563\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0567 - acc: 0.9826 - val_loss: 0.3224 - val_acc: 0.9576\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0539 - acc: 0.9825 - val_loss: 0.3301 - val_acc: 0.9561\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0549 - acc: 0.9833 - val_loss: 0.3274 - val_acc: 0.9574\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0532 - acc: 0.9827 - val_loss: 0.3340 - val_acc: 0.9563\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0537 - acc: 0.9829 - val_loss: 0.3336 - val_acc: 0.9564\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.0608 - acc: 0.9809 - val_loss: 0.3384 - val_acc: 0.9555\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0575 - acc: 0.9815 - val_loss: 0.3249 - val_acc: 0.9569\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0541 - acc: 0.9830 - val_loss: 0.3376 - val_acc: 0.9562\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0632 - acc: 0.9806 - val_loss: 0.3316 - val_acc: 0.9569\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0573 - acc: 0.9821 - val_loss: 0.3265 - val_acc: 0.9568\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0561 - acc: 0.9819 - val_loss: 0.3340 - val_acc: 0.9564\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0555 - acc: 0.9837 - val_loss: 0.3303 - val_acc: 0.9568\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0579 - acc: 0.9816 - val_loss: 0.3260 - val_acc: 0.9578\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0556 - acc: 0.9825 - val_loss: 0.3214 - val_acc: 0.9573\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0521 - acc: 0.9829 - val_loss: 0.3288 - val_acc: 0.9571\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0555 - acc: 0.9825 - val_loss: 0.3387 - val_acc: 0.9558\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0583 - acc: 0.9820 - val_loss: 0.3288 - val_acc: 0.9577\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0515 - acc: 0.9838 - val_loss: 0.3233 - val_acc: 0.9575\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0603 - acc: 0.9815 - val_loss: 0.3294 - val_acc: 0.9571\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0531 - acc: 0.9832 - val_loss: 0.3354 - val_acc: 0.9567\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0566 - acc: 0.9831 - val_loss: 0.3317 - val_acc: 0.9570\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0521 - acc: 0.9847 - val_loss: 0.3442 - val_acc: 0.9554\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0554 - acc: 0.9820 - val_loss: 0.3304 - val_acc: 0.9568\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0530 - acc: 0.9838 - val_loss: 0.3379 - val_acc: 0.9554\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0525 - acc: 0.9835 - val_loss: 0.3410 - val_acc: 0.9558\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0512 - acc: 0.9831 - val_loss: 0.3377 - val_acc: 0.9575\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0548 - acc: 0.9835 - val_loss: 0.3369 - val_acc: 0.9565\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0561 - acc: 0.9823 - val_loss: 0.3419 - val_acc: 0.9566\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0568 - acc: 0.9829 - val_loss: 0.3298 - val_acc: 0.9570\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0544 - acc: 0.9828 - val_loss: 0.3316 - val_acc: 0.9565\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.0500 - acc: 0.9843 - val_loss: 0.3311 - val_acc: 0.9576\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0551 - acc: 0.9833 - val_loss: 0.3445 - val_acc: 0.9556\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.0543 - acc: 0.9835 - val_loss: 0.3290 - val_acc: 0.9574\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.0553 - acc: 0.9833 - val_loss: 0.3341 - val_acc: 0.9571\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0582 - acc: 0.9819 - val_loss: 0.3390 - val_acc: 0.9569\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0571 - acc: 0.9824 - val_loss: 0.3352 - val_acc: 0.9561\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0577 - acc: 0.9818 - val_loss: 0.3324 - val_acc: 0.9560\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.0536 - acc: 0.9830 - val_loss: 0.3273 - val_acc: 0.9567\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0543 - acc: 0.9827 - val_loss: 0.3384 - val_acc: 0.9566\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0557 - acc: 0.9826 - val_loss: 0.3309 - val_acc: 0.9576\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0516 - acc: 0.9838 - val_loss: 0.3247 - val_acc: 0.9587\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0508 - acc: 0.9842 - val_loss: 0.3350 - val_acc: 0.9576\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0547 - acc: 0.9830 - val_loss: 0.3341 - val_acc: 0.9568\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.0585 - acc: 0.9821 - val_loss: 0.3271 - val_acc: 0.9575\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0544 - acc: 0.9821 - val_loss: 0.3301 - val_acc: 0.9576\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.0567 - acc: 0.9827 - val_loss: 0.3342 - val_acc: 0.9572\n"
     ]
    }
   ],
   "source": [
    "model_1= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[350,250,150,10],['relu','relu','relu','softmax'],500,True,0.019560946340911864,4.715676601623284e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G4gMm8t6MH6f"
   },
   "source": [
    "With increase in number of neurons\n",
    "1) The model learns faster than before \n",
    "2) The model training accuracy has increased to 98.37%. But Validation accuracy remains the same at 95% . The difference between training and validation accuracy is low to call it overfitting\n",
    "\n",
    "Lets try using momentum of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uWOFD2L4nB1F",
    "outputId": "2f8c7217-cd11-4ee5-b010-df6ca74f04fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 10s 230us/sample - loss: 1.4351 - acc: 0.5265 - val_loss: 1.0495 - val_acc: 0.6797\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 1.0840 - acc: 0.6609 - val_loss: 0.8483 - val_acc: 0.7424\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.9776 - acc: 0.7002 - val_loss: 0.8351 - val_acc: 0.7442\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.9071 - acc: 0.7227 - val_loss: 0.7139 - val_acc: 0.7833\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.8471 - acc: 0.7419 - val_loss: 0.6790 - val_acc: 0.7945\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.8050 - acc: 0.7540 - val_loss: 0.6836 - val_acc: 0.7944\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.7739 - acc: 0.7634 - val_loss: 0.6090 - val_acc: 0.8200\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.7395 - acc: 0.7726 - val_loss: 0.5996 - val_acc: 0.8209\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.7243 - acc: 0.7803 - val_loss: 0.5882 - val_acc: 0.8254\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.6880 - acc: 0.7908 - val_loss: 0.5698 - val_acc: 0.8315\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.6718 - acc: 0.7946 - val_loss: 0.5415 - val_acc: 0.8410\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.6547 - acc: 0.7999 - val_loss: 0.5195 - val_acc: 0.8467\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.6331 - acc: 0.8075 - val_loss: 0.5698 - val_acc: 0.8372\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.6197 - acc: 0.8114 - val_loss: 0.4876 - val_acc: 0.8573\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.6071 - acc: 0.8145 - val_loss: 0.5044 - val_acc: 0.8520\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.5823 - acc: 0.8219 - val_loss: 0.4729 - val_acc: 0.8597\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 0.5816 - acc: 0.8229 - val_loss: 0.5089 - val_acc: 0.8515\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.5635 - acc: 0.8271 - val_loss: 0.4864 - val_acc: 0.8548\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.5576 - acc: 0.8292 - val_loss: 0.4484 - val_acc: 0.8678\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5426 - acc: 0.8346 - val_loss: 0.4435 - val_acc: 0.8726\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5349 - acc: 0.8370 - val_loss: 0.4636 - val_acc: 0.8658\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.5242 - acc: 0.8369 - val_loss: 0.4375 - val_acc: 0.8723\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.5231 - acc: 0.8373 - val_loss: 0.4437 - val_acc: 0.8730\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4977 - acc: 0.8449 - val_loss: 0.4467 - val_acc: 0.8685\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.4946 - acc: 0.8462 - val_loss: 0.4149 - val_acc: 0.8827\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4972 - acc: 0.8473 - val_loss: 0.4067 - val_acc: 0.8821\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4916 - acc: 0.8481 - val_loss: 0.3934 - val_acc: 0.8893\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4824 - acc: 0.8500 - val_loss: 0.4382 - val_acc: 0.8752\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4765 - acc: 0.8542 - val_loss: 0.3927 - val_acc: 0.8888\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.4809 - acc: 0.8523 - val_loss: 0.4047 - val_acc: 0.8833\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4725 - acc: 0.8554 - val_loss: 0.4148 - val_acc: 0.8821\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.4769 - acc: 0.8541 - val_loss: 0.4123 - val_acc: 0.8840\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4703 - acc: 0.8560 - val_loss: 0.4136 - val_acc: 0.8821\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4498 - acc: 0.8632 - val_loss: 0.4224 - val_acc: 0.8832\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4656 - acc: 0.8587 - val_loss: 0.4239 - val_acc: 0.8804\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4550 - acc: 0.8618 - val_loss: 0.3975 - val_acc: 0.8875\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.4467 - acc: 0.8631 - val_loss: 0.3758 - val_acc: 0.8960\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.4425 - acc: 0.8639 - val_loss: 0.4100 - val_acc: 0.8862\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4447 - acc: 0.8646 - val_loss: 0.4016 - val_acc: 0.8901\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4430 - acc: 0.8636 - val_loss: 0.3929 - val_acc: 0.8918\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4435 - acc: 0.8626 - val_loss: 0.3887 - val_acc: 0.8932\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4454 - acc: 0.8656 - val_loss: 0.4128 - val_acc: 0.8827\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4481 - acc: 0.8631 - val_loss: 0.3684 - val_acc: 0.8994\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4480 - acc: 0.8636 - val_loss: 0.4386 - val_acc: 0.8775\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4435 - acc: 0.8670 - val_loss: 0.4132 - val_acc: 0.8892\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4484 - acc: 0.8651 - val_loss: 0.3925 - val_acc: 0.8931\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.4441 - acc: 0.8673 - val_loss: 0.3926 - val_acc: 0.8921\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4276 - acc: 0.8716 - val_loss: 0.3862 - val_acc: 0.8984\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.4430 - acc: 0.8668 - val_loss: 0.4196 - val_acc: 0.8877\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4566 - acc: 0.8644 - val_loss: 0.4190 - val_acc: 0.8856\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4487 - acc: 0.8656 - val_loss: 0.4087 - val_acc: 0.8925\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4439 - acc: 0.8666 - val_loss: 0.4068 - val_acc: 0.8914\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4530 - acc: 0.8653 - val_loss: 0.4085 - val_acc: 0.8915\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4545 - acc: 0.8657 - val_loss: 0.4159 - val_acc: 0.8898\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4366 - acc: 0.8697 - val_loss: 0.4354 - val_acc: 0.8871\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.4476 - acc: 0.8683 - val_loss: 0.4312 - val_acc: 0.8859\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4521 - acc: 0.8671 - val_loss: 0.4063 - val_acc: 0.8910\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4488 - acc: 0.8654 - val_loss: 0.4113 - val_acc: 0.8910\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 9s 209us/sample - loss: 0.4431 - acc: 0.8690 - val_loss: 0.4105 - val_acc: 0.8954\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4277 - acc: 0.8738 - val_loss: 0.4206 - val_acc: 0.8842\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4338 - acc: 0.8733 - val_loss: 0.4448 - val_acc: 0.8867\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4501 - acc: 0.8695 - val_loss: 0.4051 - val_acc: 0.8949\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 9s 207us/sample - loss: 0.4368 - acc: 0.8709 - val_loss: 0.4170 - val_acc: 0.8898\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4528 - acc: 0.8705 - val_loss: 0.4212 - val_acc: 0.8902\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4637 - acc: 0.8653 - val_loss: 0.4573 - val_acc: 0.8809\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4356 - acc: 0.8721 - val_loss: 0.4046 - val_acc: 0.8979\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4423 - acc: 0.8722 - val_loss: 0.4221 - val_acc: 0.8864\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4511 - acc: 0.8697 - val_loss: 0.4099 - val_acc: 0.8968\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4406 - acc: 0.8710 - val_loss: 0.4233 - val_acc: 0.8888\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4412 - acc: 0.8725 - val_loss: 0.4032 - val_acc: 0.8964\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4383 - acc: 0.8731 - val_loss: 0.3521 - val_acc: 0.9121\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4265 - acc: 0.8767 - val_loss: 0.4032 - val_acc: 0.8978\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4363 - acc: 0.8722 - val_loss: 0.4387 - val_acc: 0.8916\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4496 - acc: 0.8717 - val_loss: 0.4343 - val_acc: 0.8867\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4481 - acc: 0.8699 - val_loss: 0.4749 - val_acc: 0.8832\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4375 - acc: 0.8738 - val_loss: 0.4182 - val_acc: 0.8974\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4454 - acc: 0.8714 - val_loss: 0.4910 - val_acc: 0.8788\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.4583 - acc: 0.8678 - val_loss: 0.4139 - val_acc: 0.8959\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4385 - acc: 0.8735 - val_loss: 0.4399 - val_acc: 0.8906\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4705 - acc: 0.8678 - val_loss: 0.4251 - val_acc: 0.8944\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4458 - acc: 0.8732 - val_loss: 0.4256 - val_acc: 0.8926\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4582 - acc: 0.8693 - val_loss: 0.4164 - val_acc: 0.8951\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4497 - acc: 0.8714 - val_loss: 0.4012 - val_acc: 0.8975\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4350 - acc: 0.8758 - val_loss: 0.4409 - val_acc: 0.8922\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.4436 - acc: 0.8748 - val_loss: 0.4086 - val_acc: 0.8985\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4561 - acc: 0.8717 - val_loss: 0.4262 - val_acc: 0.9009\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4598 - acc: 0.8704 - val_loss: 0.4237 - val_acc: 0.8990\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4671 - acc: 0.8693 - val_loss: 0.4489 - val_acc: 0.8824\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4740 - acc: 0.8685 - val_loss: 0.4367 - val_acc: 0.8954\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4802 - acc: 0.8673 - val_loss: 0.4389 - val_acc: 0.8945\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4737 - acc: 0.8679 - val_loss: 0.4308 - val_acc: 0.8980\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4793 - acc: 0.8666 - val_loss: 0.4346 - val_acc: 0.8955\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4779 - acc: 0.8681 - val_loss: 0.4795 - val_acc: 0.8786\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.4743 - acc: 0.8683 - val_loss: 0.5531 - val_acc: 0.8665\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4759 - acc: 0.8649 - val_loss: 0.4854 - val_acc: 0.8803\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 0.4799 - acc: 0.8648 - val_loss: 0.5025 - val_acc: 0.8762\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4828 - acc: 0.8665 - val_loss: 0.4613 - val_acc: 0.8883\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4944 - acc: 0.8656 - val_loss: 0.5143 - val_acc: 0.8735\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.5103 - acc: 0.8583 - val_loss: 0.5069 - val_acc: 0.8804\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4975 - acc: 0.8626 - val_loss: 0.4277 - val_acc: 0.8980\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4829 - acc: 0.8672 - val_loss: 0.4203 - val_acc: 0.9002\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4753 - acc: 0.8686 - val_loss: 0.4361 - val_acc: 0.8960\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.4642 - acc: 0.8697 - val_loss: 0.4374 - val_acc: 0.8943\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4773 - acc: 0.8692 - val_loss: 0.4459 - val_acc: 0.8927\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4839 - acc: 0.8665 - val_loss: 0.4391 - val_acc: 0.8917\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 0.4786 - acc: 0.8679 - val_loss: 0.4093 - val_acc: 0.8991\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.4782 - acc: 0.8690 - val_loss: 0.4701 - val_acc: 0.8883\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4860 - acc: 0.8665 - val_loss: 0.4663 - val_acc: 0.8890\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.4904 - acc: 0.8671 - val_loss: 0.5448 - val_acc: 0.8731\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4950 - acc: 0.8658 - val_loss: 0.5012 - val_acc: 0.8810\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4863 - acc: 0.8667 - val_loss: 0.5316 - val_acc: 0.8771\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5174 - acc: 0.8599 - val_loss: 0.4758 - val_acc: 0.8826\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.4554 - acc: 0.8744 - val_loss: 0.4236 - val_acc: 0.8991\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4828 - acc: 0.8696 - val_loss: 0.5037 - val_acc: 0.8867\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.4900 - acc: 0.8663 - val_loss: 0.5017 - val_acc: 0.8817\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5172 - acc: 0.8614 - val_loss: 0.4680 - val_acc: 0.8901\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4905 - acc: 0.8662 - val_loss: 0.4330 - val_acc: 0.8963\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4865 - acc: 0.8665 - val_loss: 0.4679 - val_acc: 0.8919\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.4907 - acc: 0.8699 - val_loss: 0.4634 - val_acc: 0.8923\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 0.5025 - acc: 0.8639 - val_loss: 0.5061 - val_acc: 0.8785\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5091 - acc: 0.8632 - val_loss: 0.4361 - val_acc: 0.8968\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5072 - acc: 0.8618 - val_loss: 0.4561 - val_acc: 0.8892\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.4889 - acc: 0.8660 - val_loss: 0.4949 - val_acc: 0.8856\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5024 - acc: 0.8631 - val_loss: 0.4519 - val_acc: 0.8946\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.5555 - acc: 0.8545 - val_loss: 0.5895 - val_acc: 0.8634\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 0.5297 - acc: 0.8598 - val_loss: 0.5049 - val_acc: 0.8826\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5285 - acc: 0.8564 - val_loss: 0.4921 - val_acc: 0.8830\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 0.5184 - acc: 0.8594 - val_loss: 0.5069 - val_acc: 0.8719\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5595 - acc: 0.8498 - val_loss: 0.5926 - val_acc: 0.8550\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.5587 - acc: 0.8515 - val_loss: 0.4921 - val_acc: 0.8839\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5576 - acc: 0.8519 - val_loss: 0.4911 - val_acc: 0.8874\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5142 - acc: 0.8619 - val_loss: 0.5124 - val_acc: 0.8837\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5284 - acc: 0.8593 - val_loss: 0.5594 - val_acc: 0.8771\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 0.5261 - acc: 0.8597 - val_loss: 0.4948 - val_acc: 0.8740\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.5201 - acc: 0.8584 - val_loss: 0.5041 - val_acc: 0.8850\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5833 - acc: 0.8475 - val_loss: 0.5719 - val_acc: 0.8596\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.5510 - acc: 0.8537 - val_loss: 0.5517 - val_acc: 0.8745\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.5818 - acc: 0.8460 - val_loss: 0.5242 - val_acc: 0.8783\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 0.5867 - acc: 0.8480 - val_loss: 0.5961 - val_acc: 0.8695\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.5894 - acc: 0.8446 - val_loss: 0.5307 - val_acc: 0.8796\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.5932 - acc: 0.8446 - val_loss: 0.5027 - val_acc: 0.8899\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 0.6013 - acc: 0.8430 - val_loss: 0.5670 - val_acc: 0.8682\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.5908 - acc: 0.8470 - val_loss: 0.5573 - val_acc: 0.8731\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.5867 - acc: 0.8466 - val_loss: 0.5165 - val_acc: 0.8792\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.6267 - acc: 0.8383 - val_loss: 0.5882 - val_acc: 0.8551\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.6587 - acc: 0.8282 - val_loss: 0.6094 - val_acc: 0.8577\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 0.6283 - acc: 0.8375 - val_loss: 0.6134 - val_acc: 0.8580\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.6380 - acc: 0.8352 - val_loss: 0.7024 - val_acc: 0.8421\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.6326 - acc: 0.8354 - val_loss: 0.6956 - val_acc: 0.8418\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.6993 - acc: 0.8199 - val_loss: 0.6751 - val_acc: 0.8408\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.6890 - acc: 0.8213 - val_loss: 0.7264 - val_acc: 0.8310\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 0.6981 - acc: 0.8219 - val_loss: 0.7972 - val_acc: 0.7843\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 0.7591 - acc: 0.8054 - val_loss: 0.6364 - val_acc: 0.8457\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.6612 - acc: 0.8288 - val_loss: 0.6203 - val_acc: 0.8605\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 0.7610 - acc: 0.8037 - val_loss: 1.0899 - val_acc: 0.7106\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 0.9179 - acc: 0.7633 - val_loss: 0.8021 - val_acc: 0.8105\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 1.2890 - acc: 0.6370 - val_loss: 1.4513 - val_acc: 0.5809\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.0203 - acc: 0.3173 - val_loss: 2.4843 - val_acc: 0.2974\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.1985 - acc: 0.1948 - val_loss: 2.1807 - val_acc: 0.1730\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.2998 - acc: 0.1333 - val_loss: 2.2905 - val_acc: 0.1206\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3085 - acc: 0.1199 - val_loss: 2.3178 - val_acc: 0.1003\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3199 - acc: 0.1023 - val_loss: 2.3173 - val_acc: 0.1007\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3195 - acc: 0.1015 - val_loss: 2.3181 - val_acc: 0.1010\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3212 - acc: 0.1007 - val_loss: 2.3201 - val_acc: 0.1001\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3208 - acc: 0.1015 - val_loss: 2.3197 - val_acc: 0.1001\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3193 - acc: 0.0997 - val_loss: 2.3195 - val_acc: 0.1001\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3194 - acc: 0.1005 - val_loss: 2.3193 - val_acc: 0.1000\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3192 - acc: 0.0999 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3196 - acc: 0.0991 - val_loss: 2.3196 - val_acc: 0.1000\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 2.3192 - acc: 0.0994 - val_loss: 2.3181 - val_acc: 0.1001\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3192 - acc: 0.1019 - val_loss: 2.3191 - val_acc: 0.1001\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3193 - acc: 0.0997 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3187 - acc: 0.0994 - val_loss: 2.3192 - val_acc: 0.1006\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3183 - acc: 0.1006 - val_loss: 2.3142 - val_acc: 0.1025\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3086 - acc: 0.1053 - val_loss: 2.3086 - val_acc: 0.1046\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3144 - acc: 0.1025 - val_loss: 2.3163 - val_acc: 0.1013\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3266 - acc: 0.1001 - val_loss: 2.3207 - val_acc: 0.1000\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3193 - acc: 0.0993 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 2.3192 - acc: 0.1012 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3195 - acc: 0.0973 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3193 - acc: 0.1011 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 2.3194 - acc: 0.1000 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 9s 211us/sample - loss: 2.3193 - acc: 0.0994 - val_loss: 2.3194 - val_acc: 0.1000\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 9s 210us/sample - loss: 2.3190 - acc: 0.0998 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 2.3192 - acc: 0.0989 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3194 - acc: 0.0992 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3194 - acc: 0.0990 - val_loss: 2.3194 - val_acc: 0.1000\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3194 - acc: 0.1023 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 2.3190 - acc: 0.1001 - val_loss: 2.3196 - val_acc: 0.1000\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3193 - acc: 0.0971 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3194 - acc: 0.0976 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3191 - acc: 0.1001 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3194 - acc: 0.0989 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3188 - acc: 0.1021 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3192 - acc: 0.0991 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3191 - acc: 0.1014 - val_loss: 2.3199 - val_acc: 0.1000\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3192 - acc: 0.1014 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3192 - acc: 0.1011 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3190 - acc: 0.1004 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3194 - acc: 0.1003 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3192 - acc: 0.0986 - val_loss: 2.3190 - val_acc: 0.1000\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3193 - acc: 0.0998 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3191 - acc: 0.0999 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3190 - acc: 0.0987 - val_loss: 2.3204 - val_acc: 0.1000\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3190 - acc: 0.1024 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 9s 213us/sample - loss: 2.3191 - acc: 0.0978 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3190 - acc: 0.0985 - val_loss: 2.3199 - val_acc: 0.1000\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3192 - acc: 0.0990 - val_loss: 2.3199 - val_acc: 0.1000\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3192 - acc: 0.0999 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3191 - acc: 0.0995 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3191 - acc: 0.1000 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3191 - acc: 0.1002 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3191 - acc: 0.0990 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3191 - acc: 0.0993 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3189 - acc: 0.1004 - val_loss: 2.3196 - val_acc: 0.1000\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 9s 202us/sample - loss: 2.3191 - acc: 0.0976 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3192 - acc: 0.1001 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3191 - acc: 0.0994 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3188 - acc: 0.1022 - val_loss: 2.3200 - val_acc: 0.1000\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3192 - acc: 0.0995 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3191 - acc: 0.1003 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3188 - acc: 0.1012 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3190 - acc: 0.0978 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3189 - acc: 0.0996 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3191 - acc: 0.1005 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3186 - acc: 0.1009 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3190 - acc: 0.0985 - val_loss: 2.3194 - val_acc: 0.1000\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3190 - acc: 0.0995 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3189 - acc: 0.0998 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3188 - acc: 0.1021 - val_loss: 2.3194 - val_acc: 0.1000\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3189 - acc: 0.0970 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3189 - acc: 0.1014 - val_loss: 2.3190 - val_acc: 0.1000\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3191 - acc: 0.0996 - val_loss: 2.3194 - val_acc: 0.1000\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3188 - acc: 0.1002 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3189 - acc: 0.0989 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3190 - acc: 0.1012 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3191 - acc: 0.0981 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3186 - acc: 0.1001 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3187 - acc: 0.1001 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3190 - acc: 0.0982 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3189 - acc: 0.0995 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 9s 206us/sample - loss: 2.3190 - acc: 0.0984 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 9s 208us/sample - loss: 2.3187 - acc: 0.1013 - val_loss: 2.3189 - val_acc: 0.1000\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3184 - acc: 0.1010 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3185 - acc: 0.0995 - val_loss: 2.3210 - val_acc: 0.1000\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3189 - acc: 0.1002 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3185 - acc: 0.1006 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3186 - acc: 0.1004 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3189 - acc: 0.0994 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3187 - acc: 0.1005 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3189 - acc: 0.0991 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3188 - acc: 0.0979 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3190 - acc: 0.0992 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3185 - acc: 0.0995 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3189 - acc: 0.0972 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3185 - acc: 0.0998 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3185 - acc: 0.0989 - val_loss: 2.3200 - val_acc: 0.1000\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3189 - acc: 0.1007 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3186 - acc: 0.0996 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3189 - acc: 0.0985 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3187 - acc: 0.0998 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3187 - acc: 0.1005 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3186 - acc: 0.1002 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3187 - acc: 0.1003 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3186 - acc: 0.1009 - val_loss: 2.3194 - val_acc: 0.1000\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3188 - acc: 0.0977 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3188 - acc: 0.0970 - val_loss: 2.3195 - val_acc: 0.1000\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3186 - acc: 0.1005 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3189 - acc: 0.0984 - val_loss: 2.3195 - val_acc: 0.1000\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3185 - acc: 0.0985 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3189 - acc: 0.0977 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3184 - acc: 0.0991 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3185 - acc: 0.0992 - val_loss: 2.3196 - val_acc: 0.1000\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3186 - acc: 0.0991 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3185 - acc: 0.0999 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3186 - acc: 0.0988 - val_loss: 2.3191 - val_acc: 0.1000\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3185 - acc: 0.0994 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3185 - acc: 0.0989 - val_loss: 2.3190 - val_acc: 0.1000\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3184 - acc: 0.1012 - val_loss: 2.3202 - val_acc: 0.1000\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3189 - acc: 0.0983 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3186 - acc: 0.0989 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3183 - acc: 0.1010 - val_loss: 2.3210 - val_acc: 0.1000\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3185 - acc: 0.0990 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3184 - acc: 0.1007 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3184 - acc: 0.0992 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3185 - acc: 0.0983 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3183 - acc: 0.1028 - val_loss: 2.3190 - val_acc: 0.1000\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3185 - acc: 0.1000 - val_loss: 2.3173 - val_acc: 0.1000\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3184 - acc: 0.1012 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3184 - acc: 0.0993 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3186 - acc: 0.0998 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3186 - acc: 0.0975 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3186 - acc: 0.0986 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3186 - acc: 0.0985 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3185 - acc: 0.1007 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3184 - acc: 0.0975 - val_loss: 2.3173 - val_acc: 0.1000\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3186 - acc: 0.0978 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3186 - acc: 0.1000 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3182 - acc: 0.1010 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3187 - acc: 0.0990 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3183 - acc: 0.1007 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3185 - acc: 0.0991 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3184 - acc: 0.1005 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3186 - acc: 0.1007 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3184 - acc: 0.0991 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3184 - acc: 0.0998 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3185 - acc: 0.0966 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3184 - acc: 0.0959 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3182 - acc: 0.1001 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3183 - acc: 0.0999 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3181 - acc: 0.0999 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3184 - acc: 0.0977 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3182 - acc: 0.0999 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3183 - acc: 0.0986 - val_loss: 2.3183 - val_acc: 0.1000\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3183 - acc: 0.0987 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3184 - acc: 0.0971 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3182 - acc: 0.0998 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3184 - acc: 0.1022 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3181 - acc: 0.1011 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3184 - acc: 0.0994 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3180 - acc: 0.1000 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3180 - acc: 0.0990 - val_loss: 2.3195 - val_acc: 0.1000\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3183 - acc: 0.1017 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3181 - acc: 0.0999 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3182 - acc: 0.0985 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3181 - acc: 0.0984 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3182 - acc: 0.0986 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3181 - acc: 0.0977 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3182 - acc: 0.1008 - val_loss: 2.3173 - val_acc: 0.1000\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3180 - acc: 0.0996 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3184 - acc: 0.0981 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3183 - acc: 0.0992 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3180 - acc: 0.0983 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3184 - acc: 0.0988 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3174 - acc: 0.1032 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3180 - acc: 0.1006 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3184 - acc: 0.0973 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3183 - acc: 0.0990 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3182 - acc: 0.0992 - val_loss: 2.3173 - val_acc: 0.1000\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3178 - acc: 0.1004 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3180 - acc: 0.1020 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3180 - acc: 0.1000 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3180 - acc: 0.0998 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3179 - acc: 0.1002 - val_loss: 2.3187 - val_acc: 0.1000\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 2.3182 - acc: 0.0988 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 2.3179 - acc: 0.1021 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 2.3179 - acc: 0.0995 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3180 - acc: 0.0982 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3179 - acc: 0.0981 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3180 - acc: 0.0973 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3180 - acc: 0.1016 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3181 - acc: 0.0995 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3182 - acc: 0.0978 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3177 - acc: 0.1021 - val_loss: 2.3193 - val_acc: 0.1000\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3176 - acc: 0.1003 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3177 - acc: 0.1005 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3182 - acc: 0.0978 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3181 - acc: 0.1023 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3180 - acc: 0.0988 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3177 - acc: 0.0994 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3181 - acc: 0.0979 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3179 - acc: 0.0993 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3179 - acc: 0.0997 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3180 - acc: 0.0973 - val_loss: 2.3173 - val_acc: 0.1000\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3180 - acc: 0.0976 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3179 - acc: 0.1003 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3179 - acc: 0.1004 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3180 - acc: 0.0990 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3180 - acc: 0.0990 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3179 - acc: 0.0991 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3180 - acc: 0.0951 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3177 - acc: 0.1008 - val_loss: 2.3188 - val_acc: 0.1000\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3176 - acc: 0.1002 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3179 - acc: 0.1008 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3176 - acc: 0.1011 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3177 - acc: 0.1017 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3180 - acc: 0.0976 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3176 - acc: 0.1008 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3179 - acc: 0.1009 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3177 - acc: 0.0986 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3175 - acc: 0.0999 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3175 - acc: 0.1010 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3177 - acc: 0.0991 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3177 - acc: 0.0993 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3175 - acc: 0.1004 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3177 - acc: 0.0991 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3175 - acc: 0.1003 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3176 - acc: 0.0984 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3178 - acc: 0.0989 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3178 - acc: 0.1002 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3177 - acc: 0.1004 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 9s 204us/sample - loss: 2.3176 - acc: 0.0981 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3179 - acc: 0.0991 - val_loss: 2.3166 - val_acc: 0.1000\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3178 - acc: 0.1001 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3178 - acc: 0.0998 - val_loss: 2.3180 - val_acc: 0.1000\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3178 - acc: 0.0990 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3179 - acc: 0.0973 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3177 - acc: 0.1012 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3175 - acc: 0.1028 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3176 - acc: 0.1000 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3178 - acc: 0.0982 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3173 - acc: 0.1005 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3175 - acc: 0.1010 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3177 - acc: 0.1018 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3178 - acc: 0.0975 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3177 - acc: 0.1011 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3176 - acc: 0.0978 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3176 - acc: 0.0985 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3176 - acc: 0.1006 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3174 - acc: 0.1019 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3177 - acc: 0.1007 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3176 - acc: 0.1025 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3174 - acc: 0.0994 - val_loss: 2.3166 - val_acc: 0.1000\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3175 - acc: 0.0985 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 2.3176 - acc: 0.0996 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3176 - acc: 0.0982 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3174 - acc: 0.0985 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3174 - acc: 0.1000 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3176 - acc: 0.0990 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3175 - acc: 0.0987 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3173 - acc: 0.0990 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3175 - acc: 0.1034 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3173 - acc: 0.1009 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3176 - acc: 0.0981 - val_loss: 2.3175 - val_acc: 0.1000\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3175 - acc: 0.0999 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3174 - acc: 0.1000 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3175 - acc: 0.0985 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 9s 203us/sample - loss: 2.3171 - acc: 0.1031 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 9s 205us/sample - loss: 2.3172 - acc: 0.1003 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3172 - acc: 0.1020 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3170 - acc: 0.1022 - val_loss: 2.3186 - val_acc: 0.1000\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3176 - acc: 0.0996 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 2.3173 - acc: 0.0999 - val_loss: 2.3181 - val_acc: 0.1000\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3175 - acc: 0.0998 - val_loss: 2.3162 - val_acc: 0.1000\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3172 - acc: 0.0997 - val_loss: 2.3185 - val_acc: 0.1000\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3176 - acc: 0.0986 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3175 - acc: 0.0993 - val_loss: 2.3163 - val_acc: 0.1000\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3173 - acc: 0.0997 - val_loss: 2.3174 - val_acc: 0.1000\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 8s 202us/sample - loss: 2.3174 - acc: 0.0997 - val_loss: 2.3164 - val_acc: 0.1000\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3174 - acc: 0.0987 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3174 - acc: 0.1000 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 2.3177 - acc: 0.0982 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3173 - acc: 0.0989 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 2.3174 - acc: 0.0985 - val_loss: 2.3163 - val_acc: 0.1000\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 2.3173 - acc: 0.0961 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 2.3175 - acc: 0.0986 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3174 - acc: 0.1005 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3170 - acc: 0.1010 - val_loss: 2.3192 - val_acc: 0.1000\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 2.3173 - acc: 0.1002 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 2.3171 - acc: 0.1016 - val_loss: 2.3172 - val_acc: 0.1000\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 2.3174 - acc: 0.0984 - val_loss: 2.3166 - val_acc: 0.1000\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 2.3173 - acc: 0.1009 - val_loss: 2.3164 - val_acc: 0.1000\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 2.3172 - acc: 0.1013 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3173 - acc: 0.1005 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3171 - acc: 0.0997 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3174 - acc: 0.0978 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3174 - acc: 0.0995 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3172 - acc: 0.0985 - val_loss: 2.3184 - val_acc: 0.1000\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3173 - acc: 0.0976 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3173 - acc: 0.0986 - val_loss: 2.3166 - val_acc: 0.1000\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3172 - acc: 0.1007 - val_loss: 2.3173 - val_acc: 0.1000\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3172 - acc: 0.0979 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3171 - acc: 0.0983 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3170 - acc: 0.1009 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3174 - acc: 0.0995 - val_loss: 2.3163 - val_acc: 0.1000\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3172 - acc: 0.0977 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 8s 201us/sample - loss: 2.3173 - acc: 0.1010 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3173 - acc: 0.0981 - val_loss: 2.3179 - val_acc: 0.1000\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3171 - acc: 0.1005 - val_loss: 2.3163 - val_acc: 0.1000\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3171 - acc: 0.1015 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3171 - acc: 0.1000 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3170 - acc: 0.0990 - val_loss: 2.3161 - val_acc: 0.1000\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3171 - acc: 0.0991 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3168 - acc: 0.1000 - val_loss: 2.3182 - val_acc: 0.1000\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3172 - acc: 0.0972 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3170 - acc: 0.1005 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3171 - acc: 0.1022 - val_loss: 2.3170 - val_acc: 0.1000\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3171 - acc: 0.0998 - val_loss: 2.3164 - val_acc: 0.1000\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3170 - acc: 0.0974 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3171 - acc: 0.0998 - val_loss: 2.3162 - val_acc: 0.1000\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 8s 194us/sample - loss: 2.3170 - acc: 0.0997 - val_loss: 2.3163 - val_acc: 0.1000\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3168 - acc: 0.1009 - val_loss: 2.3176 - val_acc: 0.1000\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3171 - acc: 0.0990 - val_loss: 2.3161 - val_acc: 0.1000\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3173 - acc: 0.0983 - val_loss: 2.3168 - val_acc: 0.1000\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 8s 200us/sample - loss: 2.3171 - acc: 0.0998 - val_loss: 2.3171 - val_acc: 0.1000\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3173 - acc: 0.0978 - val_loss: 2.3163 - val_acc: 0.1000\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 8s 199us/sample - loss: 2.3172 - acc: 0.0957 - val_loss: 2.3177 - val_acc: 0.1000\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3170 - acc: 0.0992 - val_loss: 2.3169 - val_acc: 0.1000\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3170 - acc: 0.1002 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3170 - acc: 0.0976 - val_loss: 2.3159 - val_acc: 0.1000\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3171 - acc: 0.0992 - val_loss: 2.3165 - val_acc: 0.1000\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 8s 197us/sample - loss: 2.3168 - acc: 0.0981 - val_loss: 2.3178 - val_acc: 0.1000\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3169 - acc: 0.0990 - val_loss: 2.3158 - val_acc: 0.1000\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3170 - acc: 0.1000 - val_loss: 2.3164 - val_acc: 0.1000\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3171 - acc: 0.0993 - val_loss: 2.3158 - val_acc: 0.1000\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3169 - acc: 0.0986 - val_loss: 2.3167 - val_acc: 0.1000\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3169 - acc: 0.0981 - val_loss: 2.3164 - val_acc: 0.1000\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 8s 198us/sample - loss: 2.3169 - acc: 0.1005 - val_loss: 2.3197 - val_acc: 0.1000\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 2.3170 - acc: 0.1016 - val_loss: 2.3159 - val_acc: 0.1000\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 8s 196us/sample - loss: 2.3168 - acc: 0.1021 - val_loss: 2.3164 - val_acc: 0.1000\n"
     ]
    }
   ],
   "source": [
    "modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[350,250,150,10],['relu','relu','relu','softmax'],500,True,0.019560946340911864,4.715676601623284e-07,'sgd',0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORYzVN3WMjJp"
   },
   "source": [
    "With use of momentum, till say ~150 epochs the model is learning and accuracy is improving. but after that the model accuracy suddenly drops to 10% and it is not increasing further. the model is oscillating back and forth\n",
    "\n",
    "Lets try using adams model with tensorflow default parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "x4pXsTyvzt0p",
    "outputId": "1a702183-3f07-464f-881c-2889c3d5fa13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 10s 232us/sample - loss: 1.3438 - acc: 0.5556 - val_loss: 0.8898 - val_acc: 0.7193\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.9528 - acc: 0.6958 - val_loss: 0.7311 - val_acc: 0.7765\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.8298 - acc: 0.7366 - val_loss: 0.6541 - val_acc: 0.7975\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.7583 - acc: 0.7578 - val_loss: 0.6137 - val_acc: 0.8109\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.6961 - acc: 0.7792 - val_loss: 0.5773 - val_acc: 0.8215\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.6547 - acc: 0.7942 - val_loss: 0.5282 - val_acc: 0.8369\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 8s 195us/sample - loss: 0.6284 - acc: 0.8011 - val_loss: 0.5063 - val_acc: 0.8441\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.6023 - acc: 0.8096 - val_loss: 0.4748 - val_acc: 0.8552\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.5700 - acc: 0.8202 - val_loss: 0.4423 - val_acc: 0.8665\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.5553 - acc: 0.8208 - val_loss: 0.4646 - val_acc: 0.8594\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.5362 - acc: 0.8294 - val_loss: 0.4524 - val_acc: 0.8643\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.5205 - acc: 0.8355 - val_loss: 0.4133 - val_acc: 0.8753\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.5067 - acc: 0.8376 - val_loss: 0.3911 - val_acc: 0.8830\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.4969 - acc: 0.8404 - val_loss: 0.3916 - val_acc: 0.8831\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.4850 - acc: 0.8427 - val_loss: 0.3878 - val_acc: 0.8844\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.4640 - acc: 0.8515 - val_loss: 0.3772 - val_acc: 0.8862\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.4541 - acc: 0.8544 - val_loss: 0.3781 - val_acc: 0.8875\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.4566 - acc: 0.8537 - val_loss: 0.3681 - val_acc: 0.8909\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.4348 - acc: 0.8581 - val_loss: 0.3719 - val_acc: 0.8912\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.4281 - acc: 0.8638 - val_loss: 0.3871 - val_acc: 0.8849\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.4199 - acc: 0.8654 - val_loss: 0.3373 - val_acc: 0.9013\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.4199 - acc: 0.8655 - val_loss: 0.3631 - val_acc: 0.8920\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.4122 - acc: 0.8663 - val_loss: 0.3529 - val_acc: 0.8955\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.4061 - acc: 0.8684 - val_loss: 0.3377 - val_acc: 0.9039\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.3984 - acc: 0.8712 - val_loss: 0.3390 - val_acc: 0.9002\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.3819 - acc: 0.8751 - val_loss: 0.3495 - val_acc: 0.9018\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.3834 - acc: 0.8767 - val_loss: 0.3234 - val_acc: 0.9084\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.3883 - acc: 0.8766 - val_loss: 0.3161 - val_acc: 0.9120\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.3720 - acc: 0.8807 - val_loss: 0.3208 - val_acc: 0.9102\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.3791 - acc: 0.8788 - val_loss: 0.3334 - val_acc: 0.9025\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.3618 - acc: 0.8834 - val_loss: 0.3265 - val_acc: 0.9086\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.3655 - acc: 0.8819 - val_loss: 0.3241 - val_acc: 0.9093\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.3555 - acc: 0.8867 - val_loss: 0.3194 - val_acc: 0.9130\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.3555 - acc: 0.8854 - val_loss: 0.3049 - val_acc: 0.9173\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.3520 - acc: 0.8863 - val_loss: 0.3074 - val_acc: 0.9161\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.3444 - acc: 0.8888 - val_loss: 0.3371 - val_acc: 0.9079\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.3387 - acc: 0.8911 - val_loss: 0.2963 - val_acc: 0.9202\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.3402 - acc: 0.8896 - val_loss: 0.3213 - val_acc: 0.9123\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.3444 - acc: 0.8894 - val_loss: 0.3065 - val_acc: 0.9171\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.3313 - acc: 0.8928 - val_loss: 0.2984 - val_acc: 0.9207\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.3326 - acc: 0.8931 - val_loss: 0.3083 - val_acc: 0.9175\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.3320 - acc: 0.8923 - val_loss: 0.2952 - val_acc: 0.9217\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.3204 - acc: 0.8956 - val_loss: 0.2938 - val_acc: 0.9212\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.3189 - acc: 0.8961 - val_loss: 0.3055 - val_acc: 0.9187\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.3199 - acc: 0.8982 - val_loss: 0.3053 - val_acc: 0.9191\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.3191 - acc: 0.8981 - val_loss: 0.3035 - val_acc: 0.9207\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 8s 193us/sample - loss: 0.3180 - acc: 0.8975 - val_loss: 0.2849 - val_acc: 0.9264\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.3032 - acc: 0.9029 - val_loss: 0.3083 - val_acc: 0.9187\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 8s 188us/sample - loss: 0.3099 - acc: 0.9005 - val_loss: 0.3130 - val_acc: 0.9207\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.3069 - acc: 0.9006 - val_loss: 0.3008 - val_acc: 0.9243\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.3068 - acc: 0.9008 - val_loss: 0.3034 - val_acc: 0.9211\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2965 - acc: 0.9036 - val_loss: 0.3092 - val_acc: 0.9204\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2966 - acc: 0.9049 - val_loss: 0.3017 - val_acc: 0.9248\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.3026 - acc: 0.9037 - val_loss: 0.2983 - val_acc: 0.9241\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2998 - acc: 0.9032 - val_loss: 0.3010 - val_acc: 0.9240\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2980 - acc: 0.9053 - val_loss: 0.2988 - val_acc: 0.9273\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.2932 - acc: 0.9065 - val_loss: 0.3002 - val_acc: 0.9239\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2901 - acc: 0.9073 - val_loss: 0.3029 - val_acc: 0.9254\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.2945 - acc: 0.9057 - val_loss: 0.2938 - val_acc: 0.9273\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2814 - acc: 0.9095 - val_loss: 0.2991 - val_acc: 0.9273\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2839 - acc: 0.9094 - val_loss: 0.2833 - val_acc: 0.9309\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2883 - acc: 0.9089 - val_loss: 0.3011 - val_acc: 0.9270\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2782 - acc: 0.9093 - val_loss: 0.2961 - val_acc: 0.9277\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2827 - acc: 0.9097 - val_loss: 0.2863 - val_acc: 0.9319\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2827 - acc: 0.9105 - val_loss: 0.2879 - val_acc: 0.9288\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2806 - acc: 0.9125 - val_loss: 0.2973 - val_acc: 0.9229\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 8s 187us/sample - loss: 0.2705 - acc: 0.9136 - val_loss: 0.2900 - val_acc: 0.9320\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2800 - acc: 0.9115 - val_loss: 0.2931 - val_acc: 0.9291\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2670 - acc: 0.9154 - val_loss: 0.2939 - val_acc: 0.9293\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2666 - acc: 0.9152 - val_loss: 0.2984 - val_acc: 0.9282\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2678 - acc: 0.9138 - val_loss: 0.2948 - val_acc: 0.9312\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2738 - acc: 0.9140 - val_loss: 0.3028 - val_acc: 0.9290\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2693 - acc: 0.9144 - val_loss: 0.2989 - val_acc: 0.9304\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 8s 190us/sample - loss: 0.2611 - acc: 0.9162 - val_loss: 0.2867 - val_acc: 0.9336\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2542 - acc: 0.9182 - val_loss: 0.2830 - val_acc: 0.9349\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2586 - acc: 0.9177 - val_loss: 0.2836 - val_acc: 0.9342\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2617 - acc: 0.9155 - val_loss: 0.2900 - val_acc: 0.9322\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2646 - acc: 0.9173 - val_loss: 0.2956 - val_acc: 0.9342\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2619 - acc: 0.9173 - val_loss: 0.2962 - val_acc: 0.9331\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2607 - acc: 0.9184 - val_loss: 0.2835 - val_acc: 0.9356\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2569 - acc: 0.9190 - val_loss: 0.2970 - val_acc: 0.9328\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 8s 189us/sample - loss: 0.2587 - acc: 0.9185 - val_loss: 0.2870 - val_acc: 0.9334\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2568 - acc: 0.9196 - val_loss: 0.2915 - val_acc: 0.9340\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2508 - acc: 0.9200 - val_loss: 0.2844 - val_acc: 0.9359\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2546 - acc: 0.9202 - val_loss: 0.2844 - val_acc: 0.9347\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 8s 191us/sample - loss: 0.2519 - acc: 0.9203 - val_loss: 0.2916 - val_acc: 0.9342\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 8s 192us/sample - loss: 0.2549 - acc: 0.9209 - val_loss: 0.3058 - val_acc: 0.9306\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 8s 181us/sample - loss: 0.2461 - acc: 0.9220 - val_loss: 0.3104 - val_acc: 0.9319\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2466 - acc: 0.9214 - val_loss: 0.2879 - val_acc: 0.9354\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 8s 186us/sample - loss: 0.2549 - acc: 0.9184 - val_loss: 0.2871 - val_acc: 0.9352\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.2387 - acc: 0.9252 - val_loss: 0.3042 - val_acc: 0.9324\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.2406 - acc: 0.9236 - val_loss: 0.3001 - val_acc: 0.9347\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 8s 185us/sample - loss: 0.2456 - acc: 0.9222 - val_loss: 0.2867 - val_acc: 0.9371\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.2430 - acc: 0.9230 - val_loss: 0.2938 - val_acc: 0.9367\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2436 - acc: 0.9231 - val_loss: 0.2940 - val_acc: 0.9339\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 8s 180us/sample - loss: 0.2455 - acc: 0.9236 - val_loss: 0.2882 - val_acc: 0.9388\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 8s 183us/sample - loss: 0.2398 - acc: 0.9250 - val_loss: 0.3123 - val_acc: 0.9311\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 8s 182us/sample - loss: 0.2402 - acc: 0.9242 - val_loss: 0.2832 - val_acc: 0.9392\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 8s 181us/sample - loss: 0.2413 - acc: 0.9250 - val_loss: 0.2887 - val_acc: 0.9386\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 8s 184us/sample - loss: 0.2378 - acc: 0.9238 - val_loss: 0.2964 - val_acc: 0.9340\n"
     ]
    }
   ],
   "source": [
    "modeln= train_val_loop(nfX_train,HY_train,nfX_val,HY_val,[350,250,150,10],['relu','relu','relu','softmax'],100,True,.001,1e-8,'adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D4lM37nNbfF"
   },
   "source": [
    "This model is good too, this also show similar performance as SGD. with default parameter. Lets try hyperparameter optimiztion\n",
    "\n",
    "Learning rate: 1e-3 to 1\n",
    "Lamda: 1e-7 to 10\n",
    "Dropout: 0 to 0.5\n",
    "\n",
    "\n",
    "**Lets use adam optimizer with earlystopping and train for 50 iterations. (higher than SGD trial)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D3JBD3BLLg9E",
    "outputId": "f8e8c5ba-5303-4a0d-802e-d14742475e9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 67us/sample - loss: 2.3494 - acc: 0.1000\n",
      "Try 1/50: Best_val_acc: [2.3494174719492595, 0.1], lr: 0.335858276035639, Lambda: 0.0077473502710283405, Dropout: 0.1828850422425532\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.3257 - acc: 0.1000\n",
      "Try 2/50: Best_val_acc: [2.3256923795700075, 0.1], lr: 0.179508625429429, Lambda: 7.431940693105811e-06, Dropout: 0.08511076304206011\n",
      "\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 2.3038 - acc: 0.1000\n",
      "Try 3/50: Best_val_acc: [2.3037890195210773, 0.1], lr: 0.00607718466427572, Lambda: 0.030631179201924912, Dropout: 0.41340639618186853\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 899.8380 - acc: 0.1000\n",
      "Try 4/50: Best_val_acc: [899.8380249023437, 0.1], lr: 0.2805338625265805, Lambda: 5.728929433367494e-05, Dropout: 0.16578072986437137\n",
      "\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 2.3215 - acc: 0.1000\n",
      "Try 5/50: Best_val_acc: [2.3215110062917073, 0.1], lr: 0.007162373598390588, Lambda: 2.0396225616712487, Dropout: 0.34896989919129234\n",
      "\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.5005 - acc: 0.8717\n",
      "Try 6/50: Best_val_acc: [0.5005200248559316, 0.87168336], lr: 0.0005996512194093587, Lambda: 1.0768486049501851e-05, Dropout: 0.4147682843914782\n",
      "\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5420 - acc: 0.8861\n",
      "Try 7/50: Best_val_acc: [0.5420488944133123, 0.8860667], lr: 0.0013487174877761735, Lambda: 0.0001240468989408244, Dropout: 0.039419641907579406\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3084 - acc: 0.9104\n",
      "Try 8/50: Best_val_acc: [0.30837497860491275, 0.91038334], lr: 0.0001684391024133771, Lambda: 1.3352078958789704e-06, Dropout: 0.13683855077022705\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2976 - acc: 0.9281\n",
      "Try 9/50: Best_val_acc: [0.2976131211578846, 0.9280667], lr: 0.00020803272069801047, Lambda: 2.049301313777822e-05, Dropout: 0.10520507876199808\n",
      "\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 2.3029 - acc: 0.1000\n",
      "Try 10/50: Best_val_acc: [2.3028632643381757, 0.1], lr: 0.001043863819532335, Lambda: 3.0228644163872103, Dropout: 0.41122574268218554\n",
      "\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 3.0780 - acc: 0.1000\n",
      "Try 11/50: Best_val_acc: [3.0779602526346843, 0.1], lr: 0.04464170387596275, Lambda: 1.233221707748244, Dropout: 0.49561435187338054\n",
      "\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 1.0669 - acc: 0.7560\n",
      "Try 12/50: Best_val_acc: [1.066930321041743, 0.75603336], lr: 0.002853645229358323, Lambda: 0.0012982449988459468, Dropout: 0.1518968682393782\n",
      "\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 9.3318 - acc: 0.1000\n",
      "Try 13/50: Best_val_acc: [9.331784343973796, 0.1], lr: 0.07912777168914824, Lambda: 3.0779605374430554, Dropout: 0.37825164288598084\n",
      "\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 1.0720 - acc: 0.7937\n",
      "Try 14/50: Best_val_acc: [1.0720387062072754, 0.79375], lr: 0.0005686962431670138, Lambda: 0.002107749685252896, Dropout: 0.4611630884432443\n",
      "\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.6555 - acc: 0.8137\n",
      "Try 15/50: Best_val_acc: [0.6554685611128807, 0.81366664], lr: 0.002632550321755097, Lambda: 5.128065409281944e-07, Dropout: 0.2697200568494108\n",
      "\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 2.3034 - acc: 0.1000\n",
      "Try 16/50: Best_val_acc: [2.3033737420399985, 0.1], lr: 0.007751584214019705, Lambda: 0.0002578195298884316, Dropout: 0.43755380050758164\n",
      "\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 2.3160 - acc: 0.1000\n",
      "Try 17/50: Best_val_acc: [2.315973710886637, 0.1], lr: 0.12871979565568584, Lambda: 0.0293054148186648, Dropout: 0.3069798269950517\n",
      "\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 2.3054 - acc: 0.1000\n",
      "Try 18/50: Best_val_acc: [2.3054224013010662, 0.1], lr: 0.014566043947864748, Lambda: 2.0502218806948738e-08, Dropout: 0.04037944054456838\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.4319 - acc: 0.9121\n",
      "Try 19/50: Best_val_acc: [0.43188254381815594, 0.9121], lr: 0.00012227870351362517, Lambda: 0.0001390428803777569, Dropout: 0.21260268288301343\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.6160 - acc: 0.8668\n",
      "Try 20/50: Best_val_acc: [0.6160261647621791, 0.86675], lr: 0.0006578892300445888, Lambda: 6.570464358735724e-05, Dropout: 0.3979531044000935\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.9189 - acc: 0.8198\n",
      "Try 21/50: Best_val_acc: [0.9189489865461985, 0.81978333], lr: 0.0005503846481415592, Lambda: 0.001171837487043087, Dropout: 0.3926709332361572\n",
      "\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 2.3564 - acc: 0.1000\n",
      "Try 22/50: Best_val_acc: [2.3563604356129964, 0.1], lr: 0.022988196305243794, Lambda: 7.35190235354455e-07, Dropout: 0.1745398773852841\n",
      "\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 2.3027 - acc: 0.1000\n",
      "Try 23/50: Best_val_acc: [2.30271747118632, 0.1], lr: 0.0010112580478357435, Lambda: 1.4591987740117498, Dropout: 0.44355733781451\n",
      "\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.7322 - acc: 0.8620\n",
      "Try 24/50: Best_val_acc: [0.7321971314907074, 0.862], lr: 0.0003339700174694069, Lambda: 0.0015561441652976525, Dropout: 0.11613236494945162\n",
      "\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 2.3039 - acc: 0.1000\n",
      "Try 25/50: Best_val_acc: [2.3039018630981447, 0.1], lr: 0.007722083894635972, Lambda: 0.13524439827091442, Dropout: 0.36268114307215915\n",
      "\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 2.5286 - acc: 0.1000\n",
      "Try 26/50: Best_val_acc: [2.5285753739674885, 0.1], lr: 0.016807799822934393, Lambda: 2.337991023488629, Dropout: 0.02382174533348802\n",
      "\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.3068 - acc: 0.1000\n",
      "Try 27/50: Best_val_acc: [2.306791591771444, 0.1], lr: 0.020049612302274304, Lambda: 0.015018973253643894, Dropout: 0.43727606272096137\n",
      "\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 2.3053 - acc: 0.1000\n",
      "Try 28/50: Best_val_acc: [2.3053344011942545, 0.1], lr: 0.010410333683374393, Lambda: 5.6668032291379193e-08, Dropout: 0.37228392491559564\n",
      "\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.3027 - acc: 0.1000\n",
      "Try 29/50: Best_val_acc: [2.3027483462015788, 0.1], lr: 0.0013106987745279432, Lambda: 0.5483346057409747, Dropout: 0.4871810097821812\n",
      "\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 2.3169 - acc: 0.1000\n",
      "Try 30/50: Best_val_acc: [2.316948485438029, 0.1], lr: 0.005239697377420515, Lambda: 2.4044891146083684, Dropout: 0.02175300302119082\n",
      "\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.3215 - acc: 0.1000\n",
      "Try 31/50: Best_val_acc: [2.3214715243021646, 0.1], lr: 0.09420379643842178, Lambda: 1.0463302356225256e-05, Dropout: 0.16870874857008672\n",
      "\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 6.8734 - acc: 0.1000\n",
      "Try 32/50: Best_val_acc: [6.873356676228841, 0.1], lr: 0.4681657822749142, Lambda: 1.3242392845633004, Dropout: 0.4697089080165641\n",
      "\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.3684 - acc: 0.8942\n",
      "Try 33/50: Best_val_acc: [0.3683916671966513, 0.89423335], lr: 0.0001373786077218739, Lambda: 2.2024237045659366e-08, Dropout: 0.35314793532924454\n",
      "\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 2.3028 - acc: 0.1000\n",
      "Try 34/50: Best_val_acc: [2.3027701377868652, 0.1], lr: 0.0019085847290830284, Lambda: 0.05477097320113774, Dropout: 0.24895161197421584\n",
      "\n",
      "60000/60000 [==============================] - 5s 88us/sample - loss: 14.9454 - acc: 0.1000\n",
      "Try 35/50: Best_val_acc: [14.945399188741048, 0.1], lr: 0.0960373398560404, Lambda: 3.816091223320543, Dropout: 0.2944674199263145\n",
      "\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 9.9199 - acc: 0.1000\n",
      "Try 36/50: Best_val_acc: [9.91989688873291, 0.1], lr: 0.0688358908865397, Lambda: 3.815861722528436, Dropout: 0.16344954957065966\n",
      "\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 2.3260 - acc: 0.1000\n",
      "Try 37/50: Best_val_acc: [2.325962126223246, 0.1], lr: 0.2220508521943963, Lambda: 5.484271867862434e-08, Dropout: 0.15303578973605597\n",
      "\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.3787 - acc: 0.8900\n",
      "Try 38/50: Best_val_acc: [0.3787360233525435, 0.89], lr: 0.00018716594895722545, Lambda: 2.4745782272066035e-08, Dropout: 0.3969940425768441\n",
      "\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 0.2794 - acc: 0.9217\n",
      "Try 39/50: Best_val_acc: [0.279386650060614, 0.9217333], lr: 0.0005590645153625638, Lambda: 2.0722515596993252e-07, Dropout: 0.16042937357085169\n",
      "\n",
      "60000/60000 [==============================] - 6s 92us/sample - loss: 1.2778 - acc: 0.7393\n",
      "Try 40/50: Best_val_acc: [1.2778245789845784, 0.7392833], lr: 0.0010351375550060515, Lambda: 0.00888531259492081, Dropout: 0.1332834503582246\n",
      "\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 2.3147 - acc: 0.1000\n",
      "Try 41/50: Best_val_acc: [2.3146514417012534, 0.1], lr: 0.08104858017065648, Lambda: 0.058857959984541214, Dropout: 0.20277957063848384\n",
      "\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 2.3029 - acc: 0.1000\n",
      "Try 42/50: Best_val_acc: [2.3028575182596844, 0.1], lr: 0.0020900372469661917, Lambda: 0.30084035423919, Dropout: 0.3378574435518931\n",
      "\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.9323 - acc: 0.7572\n",
      "Try 43/50: Best_val_acc: [0.9322890132904053, 0.7571833], lr: 0.0067840924090613345, Lambda: 5.7757158462744104e-06, Dropout: 0.12479754404463517\n",
      "\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 2.4984 - acc: 0.1000\n",
      "Try 44/50: Best_val_acc: [2.4984395979563394, 0.1], lr: 0.025735747948581977, Lambda: 5.228803712902774e-06, Dropout: 0.39794133783635466\n",
      "\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.6160 - acc: 0.8541\n",
      "Try 45/50: Best_val_acc: [0.6160194541692734, 0.8541333], lr: 0.001118898330205379, Lambda: 1.4712330243647164e-05, Dropout: 0.3812831819080304\n",
      "\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 4.9042 - acc: 0.1000\n",
      "Try 46/50: Best_val_acc: [4.904172610982259, 0.1], lr: 0.07316312638696244, Lambda: 6.042511917532745e-06, Dropout: 0.39686798563334896\n",
      "\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 25.8367 - acc: 0.1000\n",
      "Try 47/50: Best_val_acc: [25.83672885945638, 0.1], lr: 0.9518489845518092, Lambda: 1.179006375708598e-07, Dropout: 0.15384539799026276\n",
      "\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 2.3113 - acc: 0.1000\n",
      "Try 48/50: Best_val_acc: [2.311325716908773, 0.1], lr: 0.0805656757481528, Lambda: 5.32105852676934e-07, Dropout: 0.08439982087968606\n",
      "\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 201.5772 - acc: 0.1000\n",
      "Try 49/50: Best_val_acc: [201.57719725748697, 0.1], lr: 0.41540547519008086, Lambda: 9.195242286787136e-06, Dropout: 0.002314557637847381\n",
      "\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.3979 - acc: 0.8852\n",
      "Try 50/50: Best_val_acc: [0.3978811040262381, 0.88523334], lr: 0.0006841210066484436, Lambda: 3.359903365521853e-08, Dropout: 0.3722970518346221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from keras.callbacks import EarlyStopping\n",
    "for k in range(1,51):\n",
    "  Learning_rate=math.pow(10, np.random.uniform(-4,0))\n",
    "  neurons=[350,250,150,10]\n",
    "  Regularize=math.pow(10, np.random.uniform(-8,1))\n",
    "  Activation=['relu','relu','relu','softmax']\n",
    "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=False, patience=5)\n",
    "  do=np.random.uniform(0,0.5)\n",
    "  Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "\n",
    "  modelx=tf.keras.models.Sequential()\n",
    "  modelx.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "  for i in range(0,len(neurons)):\n",
    "    modelx.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "    if i != (len(neurons)-1):\n",
    "      modelx.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "  modelx.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  modelx.fit(nfX_train,HY_train,validation_data=(nfX_val,HY_val),batch_size=30,epochs=50,verbose=False,callbacks=[es])\n",
    "\n",
    "  print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}, Dropout: {5}\\n\".format(k, 50, modelx.evaluate(nfX_val,HY_val),Learning_rate,Regularize,do))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mhg7jQDQzlY"
   },
   "source": [
    "Based on the above results good results above 88% occur for \n",
    "\n",
    "\n",
    "Learning rate : 1e-4 to 1e-3  \n",
    "lamda : 2e-8 to 1e-3\n",
    "Dropout : 0.10 to 0.41\n",
    "\n",
    "So lets try finesearch with following parameters\n",
    "\n",
    "Learning rate : 1e-4 to 1e-2  \n",
    "lamda : 1e-8 to 1e-3\n",
    "\n",
    "Dropout :0 to 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BP4EkgrKQ9rM",
    "outputId": "70969b76-cc72-4d44-838a-6952fa17115d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.507471 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.507471 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.507471 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.5206 - acc: 0.8512\n",
      "Try 1/50: Best_val_acc: [0.5205991354544958, 0.85118335], lr: 0.0008911061828518497, Lambda: 1.702728080774462e-07, Dropout: 0.5074706657149367\n",
      "\n",
      "WARNING:tensorflow:Large dropout rate: 0.670209 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.670209 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "60000/60000 [==============================] - 5s 76us/sample - loss: 0.8167 - acc: 0.7718\n",
      "Try 2/50: Best_val_acc: [0.8167142047564189, 0.77185], lr: 0.0006196679196760468, Lambda: 2.273797228012805e-06, Dropout: 0.6702086860093851\n",
      "\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.6663 - acc: 0.8139\n",
      "Try 3/50: Best_val_acc: [0.6662896778424581, 0.8139167], lr: 7.725338918613566e-05, Lambda: 1.9111119500577555e-06, Dropout: 0.4954869921603956\n",
      "\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 0.6548 - acc: 0.8120\n",
      "Try 4/50: Best_val_acc: [0.6548317306121191, 0.81196666], lr: 1.971833698140096e-05, Lambda: 6.495369368238641e-06, Dropout: 0.1623043923041183\n",
      "\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.4816 - acc: 0.8603\n",
      "Try 5/50: Best_val_acc: [0.4816319747944673, 0.8602667], lr: 0.0003733933334253252, Lambda: 1.2315935446502954e-08, Dropout: 0.4739270348929428\n",
      "\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 2.4255 - acc: 0.1897\n",
      "Try 6/50: Best_val_acc: [2.425452079264323, 0.18973333], lr: 1.213428406060518e-05, Lambda: 0.0001401621019063793, Dropout: 0.6328085704452111\n",
      "\n",
      "60000/60000 [==============================] - 5s 84us/sample - loss: 0.8660 - acc: 0.7501\n",
      "Try 7/50: Best_val_acc: [0.8660384991725286, 0.75005], lr: 1.0439575065832536e-05, Lambda: 3.2586793070771505e-08, Dropout: 0.19793609714925575\n",
      "\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.6936 - acc: 0.8023\n",
      "Try 8/50: Best_val_acc: [0.6936134272178014, 0.8023], lr: 2.779046235211851e-05, Lambda: 9.145787000372092e-06, Dropout: 0.3079282740305926\n",
      "\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 1.7966 - acc: 0.6459\n",
      "Try 9/50: Best_val_acc: [1.7965979303995767, 0.6458833], lr: 5.549318580337609e-05, Lambda: 0.000715921277019012, Dropout: 0.6594001467254541\n",
      "\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 0.5064 - acc: 0.8556\n",
      "Try 10/50: Best_val_acc: [0.5063808766166369, 0.8556333], lr: 0.0006287150644918739, Lambda: 5.082014681314381e-07, Dropout: 0.5099637745581038\n",
      "\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.4978 - acc: 0.8671\n",
      "Try 11/50: Best_val_acc: [0.4977966637372971, 0.8671167], lr: 3.2679399263319106e-05, Lambda: 2.983665971857336e-05, Dropout: 0.04267547490265545\n",
      "\n",
      "60000/60000 [==============================] - 5s 90us/sample - loss: 2.2832 - acc: 0.1645\n",
      "Try 12/50: Best_val_acc: [2.283246378835042, 0.16453333], lr: 1.1261123880886504e-05, Lambda: 1.735722538071299e-08, Dropout: 0.6403536806592245\n",
      "\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 1.1590 - acc: 0.7190\n",
      "Try 13/50: Best_val_acc: [1.1590304747104645, 0.71905], lr: 4.4066630406835465e-05, Lambda: 0.00011285778587666016, Dropout: 0.5789888458955473\n",
      "\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 1.0892 - acc: 0.7184\n",
      "Try 14/50: Best_val_acc: [1.0891712549686432, 0.7184], lr: 8.42122474393031e-05, Lambda: 1.4557056681522198e-07, Dropout: 0.6482601916808702\n",
      "\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.4021 - acc: 0.9160\n",
      "Try 15/50: Best_val_acc: [0.40208750643332797, 0.916], lr: 0.00021547354899397432, Lambda: 9.24601485495635e-05, Dropout: 0.07687714936567429\n",
      "\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5896 - acc: 0.8392\n",
      "Try 16/50: Best_val_acc: [0.5896430824955304, 0.83915], lr: 6.201789576494624e-05, Lambda: 2.4004260856320393e-05, Dropout: 0.3721378864364119\n",
      "\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 0.4814 - acc: 0.8615\n",
      "Try 17/50: Best_val_acc: [0.481405620153745, 0.86153334], lr: 0.0009384094836098869, Lambda: 5.520376918696661e-07, Dropout: 0.47382970507728206\n",
      "\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.8019 - acc: 0.7836\n",
      "Try 18/50: Best_val_acc: [0.8018868937969208, 0.78363335], lr: 0.000525291563898483, Lambda: 4.3698509209794896e-06, Dropout: 0.6628901172328646\n",
      "\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 1.1999 - acc: 0.6837\n",
      "Try 19/50: Best_val_acc: [1.199862862666448, 0.6837], lr: 4.5788606031674364e-05, Lambda: 1.0016759676651387e-08, Dropout: 0.6077406644182523\n",
      "\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.6224 - acc: 0.8736\n",
      "Try 20/50: Best_val_acc: [0.6223901075442632, 0.8736], lr: 0.00029306402223162013, Lambda: 0.00027240592976468487, Dropout: 0.413249564870641\n",
      "\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.3150 - acc: 0.9130\n",
      "Try 21/50: Best_val_acc: [0.3150106683244308, 0.91298336], lr: 0.00017180240883672221, Lambda: 3.3354069454190284e-06, Dropout: 0.11795656402711585\n",
      "\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.3697 - acc: 0.8909\n",
      "Try 22/50: Best_val_acc: [0.36969186913073065, 0.8909], lr: 0.0005629195868333627, Lambda: 8.974055487529077e-08, Dropout: 0.29550405002351793\n",
      "\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 1.3705 - acc: 0.6799\n",
      "Try 23/50: Best_val_acc: [1.3704736657778422, 0.6799], lr: 2.2618984832724654e-05, Lambda: 0.00015190515674919577, Dropout: 0.5210020959980948\n",
      "\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.5869 - acc: 0.8893\n",
      "Try 24/50: Best_val_acc: [0.5868722072045008, 0.8893], lr: 0.0005246706674880988, Lambda: 0.0005488101196586733, Dropout: 0.20553101451829195\n",
      "\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 1.8626 - acc: 0.6586\n",
      "Try 25/50: Best_val_acc: [1.862581201839447, 0.6586], lr: 1.6037115480514707e-05, Lambda: 0.0006066020879331514, Dropout: 0.49505533161540566\n",
      "\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.4075 - acc: 0.9090\n",
      "Try 26/50: Best_val_acc: [0.40749214076598483, 0.90898335], lr: 0.0002602563420059396, Lambda: 7.857394057750596e-05, Dropout: 0.08033476979866838\n",
      "\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.3001 - acc: 0.9131\n",
      "Try 27/50: Best_val_acc: [0.30009660258342824, 0.9130833], lr: 0.00047909377468167893, Lambda: 1.1646240400979536e-07, Dropout: 0.1831389392937475\n",
      "\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.2717 - acc: 0.9287\n",
      "Try 28/50: Best_val_acc: [0.2716522947644194, 0.92865], lr: 0.0003221410197800375, Lambda: 1.3083277528087247e-07, Dropout: 0.03572196350233002\n",
      "\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 1.3734 - acc: 0.6430\n",
      "Try 29/50: Best_val_acc: [1.373447557703654, 0.6429667], lr: 1.0934950265751674e-05, Lambda: 7.831515907533287e-06, Dropout: 0.43430271820872357\n",
      "\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.6720 - acc: 0.8097\n",
      "Try 30/50: Best_val_acc: [0.671967972111702, 0.8097], lr: 7.469641503360753e-05, Lambda: 6.526768491438203e-07, Dropout: 0.4851511178425011\n",
      "\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.7062 - acc: 0.8184\n",
      "Try 31/50: Best_val_acc: [0.7062031371037165, 0.81841666], lr: 0.00022780998278004483, Lambda: 4.91202985765221e-05, Dropout: 0.6003741574631447\n",
      "\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.8744 - acc: 0.8805\n",
      "Try 32/50: Best_val_acc: [0.874368551492691, 0.88046664], lr: 7.911507911725797e-05, Lambda: 0.0006868138761643703, Dropout: 0.19691756458520784\n",
      "\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.5417 - acc: 0.8448\n",
      "Try 33/50: Best_val_acc: [0.5417162330031395, 0.8448], lr: 3.2374135989210986e-05, Lambda: 6.97211240106146e-06, Dropout: 0.16222493162254634\n",
      "\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 1.1130 - acc: 0.7894\n",
      "Try 34/50: Best_val_acc: [1.1130031895955403, 0.7894167], lr: 2.453752308037228e-05, Lambda: 0.000407601140495414, Dropout: 0.3043930887962394\n",
      "\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.2935 - acc: 0.9186\n",
      "Try 35/50: Best_val_acc: [0.29354947985385854, 0.91855], lr: 0.00017683192922520423, Lambda: 3.032715078641204e-08, Dropout: 0.036945460043161314\n",
      "\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 0.4684 - acc: 0.9026\n",
      "Try 36/50: Best_val_acc: [0.4683959664344788, 0.90258336], lr: 0.0001087667581225553, Lambda: 0.00012162038762492421, Dropout: 0.099488975021573\n",
      "\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.5224 - acc: 0.8489\n",
      "Try 37/50: Best_val_acc: [0.5224376432478428, 0.84891665], lr: 0.00017091904564205202, Lambda: 4.131845628294247e-08, Dropout: 0.4690350312317236\n",
      "\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.7239 - acc: 0.7894\n",
      "Try 38/50: Best_val_acc: [0.7239219153086345, 0.78943336], lr: 1.799925650544557e-05, Lambda: 2.9440275479040892e-08, Dropout: 0.21365213226015378\n",
      "\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.3301 - acc: 0.9045\n",
      "Try 39/50: Best_val_acc: [0.33008190119067826, 0.90451664], lr: 0.0005028916162997343, Lambda: 1.427453007116601e-06, Dropout: 0.2285388887981482\n",
      "\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.3715 - acc: 0.8943\n",
      "Try 40/50: Best_val_acc: [0.37150145696997644, 0.89426666], lr: 6.849715628294701e-05, Lambda: 1.7085429467311552e-07, Dropout: 0.05480242497185654\n",
      "\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 2.2815 - acc: 0.2145\n",
      "Try 41/50: Best_val_acc: [2.2814745797475178, 0.21448334], lr: 1.3285741495629416e-05, Lambda: 6.218936890278706e-08, Dropout: 0.6174789880886123\n",
      "\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 0.3425 - acc: 0.9003\n",
      "Try 42/50: Best_val_acc: [0.34246806031515203, 0.90025], lr: 0.00023183249671443872, Lambda: 3.3641251163184875e-08, Dropout: 0.21223204492083028\n",
      "\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.8293 - acc: 0.7811\n",
      "Try 43/50: Best_val_acc: [0.8293179833889007, 0.78108335], lr: 0.0001088222852690998, Lambda: 1.8021518195072238e-06, Dropout: 0.6117856281506929\n",
      "\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.6967 - acc: 0.8357\n",
      "Try 44/50: Best_val_acc: [0.6967423229297002, 0.83565], lr: 3.510501172891555e-05, Lambda: 0.00012631619431821927, Dropout: 0.21972556051312592\n",
      "\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.7101 - acc: 0.8245\n",
      "Try 45/50: Best_val_acc: [0.7101320574124654, 0.82451665], lr: 0.0002622872029289675, Lambda: 8.150309295583149e-05, Dropout: 0.5858936288851764\n",
      "\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 0.9200 - acc: 0.8067\n",
      "Try 46/50: Best_val_acc: [0.920028996181488, 0.80668336], lr: 6.0996994980138235e-05, Lambda: 0.00025643575964921523, Dropout: 0.47976854676598224\n",
      "\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.9334 - acc: 0.7843\n",
      "Try 47/50: Best_val_acc: [0.9333953462918599, 0.78426665], lr: 6.59864953521403e-05, Lambda: 0.00015264084171056328, Dropout: 0.5558922290872739\n",
      "\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.7236 - acc: 0.8707\n",
      "Try 48/50: Best_val_acc: [0.7236076512972514, 0.8706667], lr: 0.00019136831903929162, Lambda: 0.0006073697074013857, Dropout: 0.36477771852376306\n",
      "\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.2853 - acc: 0.9190\n",
      "Try 49/50: Best_val_acc: [0.28534128354688487, 0.91896665], lr: 0.000930469420532006, Lambda: 5.717950379970852e-08, Dropout: 0.09824235587602002\n",
      "\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.5977 - acc: 0.8300\n",
      "Try 50/50: Best_val_acc: [0.5976639101107916, 0.83], lr: 0.00014826504717081515, Lambda: 4.2464089258374724e-07, Dropout: 0.5229546010253362\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from keras.callbacks import EarlyStopping\n",
    "for k in range(1,51):\n",
    "  Learning_rate=math.pow(10, np.random.uniform(-5,-3))\n",
    "  neurons=[350,250,150,10]\n",
    "  Regularize=math.pow(10, np.random.uniform(-8,-3))\n",
    "  Activation=['relu','relu','relu','softmax']\n",
    "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=False, patience=5)\n",
    "  do=np.random.uniform(0,0.7)\n",
    "  Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "\n",
    "  modelx=tf.keras.models.Sequential()\n",
    "  modelx.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "  for i in range(0,len(neurons)):\n",
    "    modelx.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "    if i != (len(neurons)-1):\n",
    "      modelx.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "  modelx.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  modelx.fit(nfX_train,HY_train,validation_data=(nfX_val,HY_val),batch_size=128,epochs=30,verbose=False,callbacks=[es])\n",
    "\n",
    "  print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}, Dropout: {5}\\n\".format(k, 50, modelx.evaluate(nfX_val,HY_val),Learning_rate,Regularize,do))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42pfuDcr3zh0"
   },
   "source": [
    "\n",
    "There are few trials with validation accuracy greater than 90%\n",
    "\n",
    "The validation accuracy is higher for \n",
    "\n",
    "lr: 0.0003221410197800375, \n",
    "Lambda: 1.3083277528087247e-07, \n",
    "Dropout: 0.03572196350233002\n",
    "\n",
    "Lets try running the model for 500 epochs with this parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kZK3MHd-0Ur_",
    "outputId": "1bc68474-c736-46cd-b337-d19c608ea73a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 9s 225us/sample - loss: 1.4584 - acc: 0.5223 - val_loss: 0.9995 - val_acc: 0.7220\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.9117 - acc: 0.7174 - val_loss: 0.7528 - val_acc: 0.7717\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.7756 - acc: 0.7612 - val_loss: 0.6602 - val_acc: 0.8027\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.6863 - acc: 0.7895 - val_loss: 0.5871 - val_acc: 0.8247\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.6267 - acc: 0.8077 - val_loss: 0.5401 - val_acc: 0.8400\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.5835 - acc: 0.8193 - val_loss: 0.5134 - val_acc: 0.8487\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.5432 - acc: 0.8335 - val_loss: 0.4822 - val_acc: 0.8598\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.5184 - acc: 0.8389 - val_loss: 0.4529 - val_acc: 0.8664\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.4909 - acc: 0.8479 - val_loss: 0.4346 - val_acc: 0.8723\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.4674 - acc: 0.8531 - val_loss: 0.4258 - val_acc: 0.8755\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.4459 - acc: 0.8600 - val_loss: 0.4292 - val_acc: 0.8718\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.4270 - acc: 0.8650 - val_loss: 0.3923 - val_acc: 0.8853\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.4091 - acc: 0.8705 - val_loss: 0.4089 - val_acc: 0.8787\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.3929 - acc: 0.8760 - val_loss: 0.3971 - val_acc: 0.8820\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.3751 - acc: 0.8820 - val_loss: 0.3811 - val_acc: 0.8888\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.3610 - acc: 0.8853 - val_loss: 0.3558 - val_acc: 0.8981\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.3551 - acc: 0.8872 - val_loss: 0.3417 - val_acc: 0.9034\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.3385 - acc: 0.8914 - val_loss: 0.3632 - val_acc: 0.8950\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.3384 - acc: 0.8920 - val_loss: 0.3254 - val_acc: 0.9081\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.3177 - acc: 0.8991 - val_loss: 0.3259 - val_acc: 0.9076\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.3194 - acc: 0.8968 - val_loss: 0.3343 - val_acc: 0.9045\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.3016 - acc: 0.9027 - val_loss: 0.3115 - val_acc: 0.9145\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.3094 - acc: 0.8993 - val_loss: 0.3004 - val_acc: 0.9182\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.2783 - acc: 0.9096 - val_loss: 0.2892 - val_acc: 0.9217\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.2738 - acc: 0.9100 - val_loss: 0.3011 - val_acc: 0.9182\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.2709 - acc: 0.9125 - val_loss: 0.2857 - val_acc: 0.9212\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.2647 - acc: 0.9140 - val_loss: 0.2749 - val_acc: 0.9267\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.2495 - acc: 0.9194 - val_loss: 0.2754 - val_acc: 0.9277\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.2541 - acc: 0.9174 - val_loss: 0.2776 - val_acc: 0.9270\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.2428 - acc: 0.9211 - val_loss: 0.2686 - val_acc: 0.9295\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.2385 - acc: 0.9228 - val_loss: 0.2713 - val_acc: 0.9298\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.2327 - acc: 0.9242 - val_loss: 0.2710 - val_acc: 0.9291\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.2203 - acc: 0.9281 - val_loss: 0.2602 - val_acc: 0.9340\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.2203 - acc: 0.9288 - val_loss: 0.2595 - val_acc: 0.9338\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.2184 - acc: 0.9281 - val_loss: 0.2637 - val_acc: 0.9328\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.2122 - acc: 0.9304 - val_loss: 0.2695 - val_acc: 0.9320\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.2064 - acc: 0.9320 - val_loss: 0.2630 - val_acc: 0.9350\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.2003 - acc: 0.9342 - val_loss: 0.2583 - val_acc: 0.9362\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1965 - acc: 0.9352 - val_loss: 0.2517 - val_acc: 0.9393\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1955 - acc: 0.9349 - val_loss: 0.2479 - val_acc: 0.9396\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.1877 - acc: 0.9381 - val_loss: 0.2582 - val_acc: 0.9368\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1902 - acc: 0.9376 - val_loss: 0.2449 - val_acc: 0.9409\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1827 - acc: 0.9395 - val_loss: 0.2520 - val_acc: 0.9394\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1751 - acc: 0.9419 - val_loss: 0.2507 - val_acc: 0.9407\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1754 - acc: 0.9426 - val_loss: 0.2594 - val_acc: 0.9387\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.1803 - acc: 0.9409 - val_loss: 0.2559 - val_acc: 0.9387\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1669 - acc: 0.9450 - val_loss: 0.2440 - val_acc: 0.9438\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.1680 - acc: 0.9437 - val_loss: 0.2436 - val_acc: 0.9441\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1670 - acc: 0.9447 - val_loss: 0.2664 - val_acc: 0.9370\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.1643 - acc: 0.9448 - val_loss: 0.2401 - val_acc: 0.9465\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1542 - acc: 0.9484 - val_loss: 0.2515 - val_acc: 0.9428\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.1558 - acc: 0.9484 - val_loss: 0.2471 - val_acc: 0.9455\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.1614 - acc: 0.9470 - val_loss: 0.2451 - val_acc: 0.9445\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.1464 - acc: 0.9520 - val_loss: 0.2422 - val_acc: 0.9461\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1444 - acc: 0.9532 - val_loss: 0.2410 - val_acc: 0.9477\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1461 - acc: 0.9502 - val_loss: 0.2574 - val_acc: 0.9424\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1550 - acc: 0.9493 - val_loss: 0.2410 - val_acc: 0.9469\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1347 - acc: 0.9547 - val_loss: 0.2571 - val_acc: 0.9432\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1525 - acc: 0.9500 - val_loss: 0.2374 - val_acc: 0.9502\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1408 - acc: 0.9536 - val_loss: 0.2478 - val_acc: 0.9473\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1344 - acc: 0.9545 - val_loss: 0.2453 - val_acc: 0.9474\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.1306 - acc: 0.9560 - val_loss: 0.2459 - val_acc: 0.9481\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1268 - acc: 0.9574 - val_loss: 0.2527 - val_acc: 0.9461\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1336 - acc: 0.9559 - val_loss: 0.2496 - val_acc: 0.9475\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1374 - acc: 0.9546 - val_loss: 0.2354 - val_acc: 0.9528\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.1307 - acc: 0.9569 - val_loss: 0.2516 - val_acc: 0.9480\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1196 - acc: 0.9591 - val_loss: 0.2428 - val_acc: 0.9511\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1273 - acc: 0.9582 - val_loss: 0.2521 - val_acc: 0.9474\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.1185 - acc: 0.9608 - val_loss: 0.2465 - val_acc: 0.9504\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1190 - acc: 0.9605 - val_loss: 0.2581 - val_acc: 0.9475\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.1200 - acc: 0.9599 - val_loss: 0.2533 - val_acc: 0.9488\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.1260 - acc: 0.9588 - val_loss: 0.2469 - val_acc: 0.9511\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1137 - acc: 0.9619 - val_loss: 0.2462 - val_acc: 0.9509\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1100 - acc: 0.9635 - val_loss: 0.2574 - val_acc: 0.9509\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1198 - acc: 0.9610 - val_loss: 0.2499 - val_acc: 0.9511\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1134 - acc: 0.9608 - val_loss: 0.2473 - val_acc: 0.9529\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1129 - acc: 0.9635 - val_loss: 0.2541 - val_acc: 0.9508\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1064 - acc: 0.9648 - val_loss: 0.2529 - val_acc: 0.9516\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1058 - acc: 0.9642 - val_loss: 0.2482 - val_acc: 0.9527\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 5s 107us/sample - loss: 0.1095 - acc: 0.9631 - val_loss: 0.2602 - val_acc: 0.9520\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1037 - acc: 0.9652 - val_loss: 0.2568 - val_acc: 0.9503\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1148 - acc: 0.9621 - val_loss: 0.2472 - val_acc: 0.9541\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.1065 - acc: 0.9649 - val_loss: 0.2613 - val_acc: 0.9507\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1076 - acc: 0.9649 - val_loss: 0.2483 - val_acc: 0.9532\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0971 - acc: 0.9674 - val_loss: 0.2532 - val_acc: 0.9530\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1171 - acc: 0.9612 - val_loss: 0.2511 - val_acc: 0.9530\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1015 - acc: 0.9666 - val_loss: 0.2519 - val_acc: 0.9525\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1070 - acc: 0.9655 - val_loss: 0.2434 - val_acc: 0.9543\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0910 - acc: 0.9699 - val_loss: 0.2580 - val_acc: 0.9535\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0995 - acc: 0.9670 - val_loss: 0.2609 - val_acc: 0.9526\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0992 - acc: 0.9675 - val_loss: 0.2628 - val_acc: 0.9533\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0997 - acc: 0.9674 - val_loss: 0.2567 - val_acc: 0.9522\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.1043 - acc: 0.9650 - val_loss: 0.2563 - val_acc: 0.9552\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0958 - acc: 0.9685 - val_loss: 0.2529 - val_acc: 0.9545\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0942 - acc: 0.9696 - val_loss: 0.2667 - val_acc: 0.9517\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0988 - acc: 0.9667 - val_loss: 0.2539 - val_acc: 0.9551\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0922 - acc: 0.9694 - val_loss: 0.2612 - val_acc: 0.9541\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0890 - acc: 0.9703 - val_loss: 0.2637 - val_acc: 0.9540\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.1031 - acc: 0.9658 - val_loss: 0.2533 - val_acc: 0.9540\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0982 - acc: 0.9678 - val_loss: 0.2607 - val_acc: 0.9520\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0857 - acc: 0.9716 - val_loss: 0.2658 - val_acc: 0.9544\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0911 - acc: 0.9698 - val_loss: 0.2640 - val_acc: 0.9543\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0901 - acc: 0.9705 - val_loss: 0.2670 - val_acc: 0.9535\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0832 - acc: 0.9725 - val_loss: 0.2683 - val_acc: 0.9527\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0879 - acc: 0.9711 - val_loss: 0.2694 - val_acc: 0.9539\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0888 - acc: 0.9703 - val_loss: 0.2689 - val_acc: 0.9541\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0946 - acc: 0.9695 - val_loss: 0.2700 - val_acc: 0.9521\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.0885 - acc: 0.9707 - val_loss: 0.2657 - val_acc: 0.9564\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0834 - acc: 0.9734 - val_loss: 0.2663 - val_acc: 0.9543\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0834 - acc: 0.9729 - val_loss: 0.2718 - val_acc: 0.9549\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0931 - acc: 0.9694 - val_loss: 0.2646 - val_acc: 0.9553\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0825 - acc: 0.9735 - val_loss: 0.2697 - val_acc: 0.9553\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0852 - acc: 0.9719 - val_loss: 0.2758 - val_acc: 0.9536\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0841 - acc: 0.9726 - val_loss: 0.2690 - val_acc: 0.9546\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0767 - acc: 0.9750 - val_loss: 0.2703 - val_acc: 0.9561\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0823 - acc: 0.9730 - val_loss: 0.2720 - val_acc: 0.9560\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0817 - acc: 0.9730 - val_loss: 0.2720 - val_acc: 0.9544\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0747 - acc: 0.9763 - val_loss: 0.2722 - val_acc: 0.9555\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0861 - acc: 0.9724 - val_loss: 0.2729 - val_acc: 0.9542\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0796 - acc: 0.9739 - val_loss: 0.2698 - val_acc: 0.9567\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0794 - acc: 0.9745 - val_loss: 0.2729 - val_acc: 0.9561\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0784 - acc: 0.9741 - val_loss: 0.2766 - val_acc: 0.9544\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0886 - acc: 0.9708 - val_loss: 0.2731 - val_acc: 0.9558\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0759 - acc: 0.9746 - val_loss: 0.2724 - val_acc: 0.9572\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0779 - acc: 0.9737 - val_loss: 0.2780 - val_acc: 0.9556\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0707 - acc: 0.9775 - val_loss: 0.2866 - val_acc: 0.9520\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0762 - acc: 0.9757 - val_loss: 0.2794 - val_acc: 0.9551\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0776 - acc: 0.9740 - val_loss: 0.2824 - val_acc: 0.9552\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0831 - acc: 0.9729 - val_loss: 0.2799 - val_acc: 0.9565\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0678 - acc: 0.9784 - val_loss: 0.2773 - val_acc: 0.9562\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0885 - acc: 0.9733 - val_loss: 0.2732 - val_acc: 0.9558\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0700 - acc: 0.9772 - val_loss: 0.2784 - val_acc: 0.9562\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0772 - acc: 0.9741 - val_loss: 0.2768 - val_acc: 0.9564\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0780 - acc: 0.9750 - val_loss: 0.2902 - val_acc: 0.9535\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0817 - acc: 0.9741 - val_loss: 0.2822 - val_acc: 0.9558\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0693 - acc: 0.9775 - val_loss: 0.2815 - val_acc: 0.9558\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0765 - acc: 0.9743 - val_loss: 0.2798 - val_acc: 0.9566\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0785 - acc: 0.9750 - val_loss: 0.2839 - val_acc: 0.9553\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0773 - acc: 0.9748 - val_loss: 0.2814 - val_acc: 0.9569\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0777 - acc: 0.9755 - val_loss: 0.2779 - val_acc: 0.9566\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0651 - acc: 0.9776 - val_loss: 0.2756 - val_acc: 0.9577\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0640 - acc: 0.9786 - val_loss: 0.2916 - val_acc: 0.9549\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0747 - acc: 0.9758 - val_loss: 0.2872 - val_acc: 0.9573\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0701 - acc: 0.9771 - val_loss: 0.2791 - val_acc: 0.9568\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0693 - acc: 0.9775 - val_loss: 0.2862 - val_acc: 0.9554\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0770 - acc: 0.9751 - val_loss: 0.2813 - val_acc: 0.9579\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0681 - acc: 0.9779 - val_loss: 0.2853 - val_acc: 0.9564\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0769 - acc: 0.9746 - val_loss: 0.2854 - val_acc: 0.9568\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0751 - acc: 0.9758 - val_loss: 0.2818 - val_acc: 0.9584\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0650 - acc: 0.9789 - val_loss: 0.2875 - val_acc: 0.9564\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0672 - acc: 0.9773 - val_loss: 0.2789 - val_acc: 0.9588\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0654 - acc: 0.9790 - val_loss: 0.2862 - val_acc: 0.9575\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0679 - acc: 0.9784 - val_loss: 0.2921 - val_acc: 0.9570\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0685 - acc: 0.9774 - val_loss: 0.2901 - val_acc: 0.9565\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0648 - acc: 0.9790 - val_loss: 0.2887 - val_acc: 0.9574\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0692 - acc: 0.9777 - val_loss: 0.2908 - val_acc: 0.9544\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0707 - acc: 0.9776 - val_loss: 0.2843 - val_acc: 0.9573\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0689 - acc: 0.9776 - val_loss: 0.2877 - val_acc: 0.9570\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0690 - acc: 0.9777 - val_loss: 0.2843 - val_acc: 0.9580\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0703 - acc: 0.9777 - val_loss: 0.2885 - val_acc: 0.9575\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0659 - acc: 0.9784 - val_loss: 0.2944 - val_acc: 0.9579\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0635 - acc: 0.9783 - val_loss: 0.2867 - val_acc: 0.9585\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0567 - acc: 0.9812 - val_loss: 0.2935 - val_acc: 0.9565\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.0709 - acc: 0.9780 - val_loss: 0.2916 - val_acc: 0.9578\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0593 - acc: 0.9811 - val_loss: 0.2921 - val_acc: 0.9576\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0590 - acc: 0.9809 - val_loss: 0.3019 - val_acc: 0.9562\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0625 - acc: 0.9787 - val_loss: 0.2931 - val_acc: 0.9575\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0679 - acc: 0.9785 - val_loss: 0.2848 - val_acc: 0.9582\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0680 - acc: 0.9781 - val_loss: 0.2921 - val_acc: 0.9568\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0644 - acc: 0.9795 - val_loss: 0.2940 - val_acc: 0.9567\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0567 - acc: 0.9816 - val_loss: 0.2942 - val_acc: 0.9586\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0600 - acc: 0.9802 - val_loss: 0.2970 - val_acc: 0.9582\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0650 - acc: 0.9792 - val_loss: 0.2931 - val_acc: 0.9586\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0670 - acc: 0.9780 - val_loss: 0.3052 - val_acc: 0.9559\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0675 - acc: 0.9781 - val_loss: 0.2998 - val_acc: 0.9572\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0614 - acc: 0.9805 - val_loss: 0.2898 - val_acc: 0.9578\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0687 - acc: 0.9778 - val_loss: 0.3008 - val_acc: 0.9551\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0550 - acc: 0.9823 - val_loss: 0.2954 - val_acc: 0.9571\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0576 - acc: 0.9806 - val_loss: 0.3036 - val_acc: 0.9569\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0580 - acc: 0.9802 - val_loss: 0.3010 - val_acc: 0.9575\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0665 - acc: 0.9788 - val_loss: 0.2894 - val_acc: 0.9586\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0596 - acc: 0.9806 - val_loss: 0.2938 - val_acc: 0.9580\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0592 - acc: 0.9806 - val_loss: 0.3019 - val_acc: 0.9574\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0666 - acc: 0.9793 - val_loss: 0.3038 - val_acc: 0.9571\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0594 - acc: 0.9813 - val_loss: 0.3011 - val_acc: 0.9573\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0617 - acc: 0.9805 - val_loss: 0.2929 - val_acc: 0.9593\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0620 - acc: 0.9806 - val_loss: 0.2983 - val_acc: 0.9583\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0619 - acc: 0.9802 - val_loss: 0.3023 - val_acc: 0.9571\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0596 - acc: 0.9815 - val_loss: 0.3074 - val_acc: 0.9579\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0664 - acc: 0.9790 - val_loss: 0.3030 - val_acc: 0.9582\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0622 - acc: 0.9803 - val_loss: 0.3143 - val_acc: 0.9557\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0541 - acc: 0.9826 - val_loss: 0.3066 - val_acc: 0.9572\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0654 - acc: 0.9803 - val_loss: 0.3014 - val_acc: 0.9574\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0535 - acc: 0.9827 - val_loss: 0.3101 - val_acc: 0.9565\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0738 - acc: 0.9765 - val_loss: 0.2985 - val_acc: 0.9584\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0498 - acc: 0.9841 - val_loss: 0.2919 - val_acc: 0.9597\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0507 - acc: 0.9834 - val_loss: 0.3022 - val_acc: 0.9569\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0562 - acc: 0.9823 - val_loss: 0.3064 - val_acc: 0.9577\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0531 - acc: 0.9828 - val_loss: 0.3128 - val_acc: 0.9565\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0669 - acc: 0.9794 - val_loss: 0.3052 - val_acc: 0.9575\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0663 - acc: 0.9792 - val_loss: 0.3078 - val_acc: 0.9586\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0542 - acc: 0.9820 - val_loss: 0.3075 - val_acc: 0.9581\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0723 - acc: 0.9777 - val_loss: 0.2994 - val_acc: 0.9583\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0609 - acc: 0.9800 - val_loss: 0.3029 - val_acc: 0.9587\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0544 - acc: 0.9826 - val_loss: 0.2947 - val_acc: 0.9582\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0572 - acc: 0.9822 - val_loss: 0.3061 - val_acc: 0.9566\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0485 - acc: 0.9845 - val_loss: 0.3130 - val_acc: 0.9586\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0581 - acc: 0.9815 - val_loss: 0.3071 - val_acc: 0.9586\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0532 - acc: 0.9828 - val_loss: 0.3099 - val_acc: 0.9592\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0551 - acc: 0.9826 - val_loss: 0.3042 - val_acc: 0.9580\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0639 - acc: 0.9792 - val_loss: 0.3022 - val_acc: 0.9585\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0506 - acc: 0.9834 - val_loss: 0.3064 - val_acc: 0.9582\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0548 - acc: 0.9820 - val_loss: 0.3128 - val_acc: 0.9576\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0579 - acc: 0.9809 - val_loss: 0.3066 - val_acc: 0.9580\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0620 - acc: 0.9809 - val_loss: 0.3054 - val_acc: 0.9590\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0549 - acc: 0.9821 - val_loss: 0.3107 - val_acc: 0.9590\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0603 - acc: 0.9808 - val_loss: 0.2994 - val_acc: 0.9582\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0520 - acc: 0.9837 - val_loss: 0.3048 - val_acc: 0.9588\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0536 - acc: 0.9827 - val_loss: 0.3079 - val_acc: 0.9576\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.0532 - acc: 0.9832 - val_loss: 0.3068 - val_acc: 0.9585\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0542 - acc: 0.9826 - val_loss: 0.3156 - val_acc: 0.9584\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0521 - acc: 0.9836 - val_loss: 0.3116 - val_acc: 0.9594\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0550 - acc: 0.9830 - val_loss: 0.3148 - val_acc: 0.9580\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0578 - acc: 0.9820 - val_loss: 0.3178 - val_acc: 0.9580\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0536 - acc: 0.9831 - val_loss: 0.3171 - val_acc: 0.9573\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0530 - acc: 0.9830 - val_loss: 0.3113 - val_acc: 0.9588\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0516 - acc: 0.9825 - val_loss: 0.3276 - val_acc: 0.9568\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0525 - acc: 0.9837 - val_loss: 0.3186 - val_acc: 0.9575\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0543 - acc: 0.9831 - val_loss: 0.3190 - val_acc: 0.9575\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0573 - acc: 0.9818 - val_loss: 0.3171 - val_acc: 0.9586\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0505 - acc: 0.9842 - val_loss: 0.3247 - val_acc: 0.9568\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0475 - acc: 0.9850 - val_loss: 0.3096 - val_acc: 0.9590\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0507 - acc: 0.9840 - val_loss: 0.3218 - val_acc: 0.9564\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0677 - acc: 0.9783 - val_loss: 0.3090 - val_acc: 0.9595\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0533 - acc: 0.9826 - val_loss: 0.3176 - val_acc: 0.9592\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0455 - acc: 0.9857 - val_loss: 0.3195 - val_acc: 0.9585\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0486 - acc: 0.9847 - val_loss: 0.3162 - val_acc: 0.9575\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0643 - acc: 0.9798 - val_loss: 0.3100 - val_acc: 0.9583\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0505 - acc: 0.9840 - val_loss: 0.3189 - val_acc: 0.9589\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0678 - acc: 0.9788 - val_loss: 0.3077 - val_acc: 0.9598\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0511 - acc: 0.9842 - val_loss: 0.3222 - val_acc: 0.9593\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0468 - acc: 0.9851 - val_loss: 0.3215 - val_acc: 0.9585\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0552 - acc: 0.9826 - val_loss: 0.3213 - val_acc: 0.9587\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0468 - acc: 0.9855 - val_loss: 0.3230 - val_acc: 0.9560\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0608 - acc: 0.9805 - val_loss: 0.3134 - val_acc: 0.9595\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0451 - acc: 0.9852 - val_loss: 0.3095 - val_acc: 0.9602\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 5s 111us/sample - loss: 0.0460 - acc: 0.9849 - val_loss: 0.3272 - val_acc: 0.9567\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 5s 109us/sample - loss: 0.0463 - acc: 0.9848 - val_loss: 0.3267 - val_acc: 0.9580\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0509 - acc: 0.9841 - val_loss: 0.3214 - val_acc: 0.9583\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0524 - acc: 0.9840 - val_loss: 0.3220 - val_acc: 0.9575\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0448 - acc: 0.9857 - val_loss: 0.3278 - val_acc: 0.9582\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0486 - acc: 0.9844 - val_loss: 0.3337 - val_acc: 0.9582\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0483 - acc: 0.9847 - val_loss: 0.3327 - val_acc: 0.9587\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0514 - acc: 0.9833 - val_loss: 0.3214 - val_acc: 0.9582\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0456 - acc: 0.9855 - val_loss: 0.3216 - val_acc: 0.9584\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0463 - acc: 0.9847 - val_loss: 0.3250 - val_acc: 0.9587\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 4s 107us/sample - loss: 0.0558 - acc: 0.9818 - val_loss: 0.3260 - val_acc: 0.9586\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0541 - acc: 0.9828 - val_loss: 0.3192 - val_acc: 0.9571\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 5s 108us/sample - loss: 0.0444 - acc: 0.9857 - val_loss: 0.3333 - val_acc: 0.9579\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0492 - acc: 0.9844 - val_loss: 0.3236 - val_acc: 0.9596\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0503 - acc: 0.9840 - val_loss: 0.3306 - val_acc: 0.9577\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.0460 - acc: 0.9850 - val_loss: 0.3303 - val_acc: 0.9582\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0478 - acc: 0.9852 - val_loss: 0.3296 - val_acc: 0.9595\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0421 - acc: 0.9867 - val_loss: 0.3300 - val_acc: 0.9592\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0687 - acc: 0.9791 - val_loss: 0.3256 - val_acc: 0.9583\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0484 - acc: 0.9839 - val_loss: 0.3320 - val_acc: 0.9581\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0495 - acc: 0.9839 - val_loss: 0.3263 - val_acc: 0.9589\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0430 - acc: 0.9871 - val_loss: 0.3241 - val_acc: 0.9585\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0409 - acc: 0.9867 - val_loss: 0.3272 - val_acc: 0.9593\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0713 - acc: 0.9790 - val_loss: 0.3199 - val_acc: 0.9586\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0621 - acc: 0.9805 - val_loss: 0.3208 - val_acc: 0.9600\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0482 - acc: 0.9853 - val_loss: 0.3156 - val_acc: 0.9586\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0447 - acc: 0.9860 - val_loss: 0.3199 - val_acc: 0.9588\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0407 - acc: 0.9872 - val_loss: 0.3326 - val_acc: 0.9585\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0557 - acc: 0.9822 - val_loss: 0.3196 - val_acc: 0.9582\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 4s 106us/sample - loss: 0.0560 - acc: 0.9826 - val_loss: 0.3275 - val_acc: 0.9580\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0395 - acc: 0.9883 - val_loss: 0.3199 - val_acc: 0.9577\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0407 - acc: 0.9872 - val_loss: 0.3256 - val_acc: 0.9583\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0555 - acc: 0.9824 - val_loss: 0.3144 - val_acc: 0.9589\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0458 - acc: 0.9848 - val_loss: 0.3247 - val_acc: 0.9571\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0393 - acc: 0.9872 - val_loss: 0.3292 - val_acc: 0.9577\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0457 - acc: 0.9859 - val_loss: 0.3269 - val_acc: 0.9584\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0478 - acc: 0.9846 - val_loss: 0.3301 - val_acc: 0.9585\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0501 - acc: 0.9845 - val_loss: 0.3205 - val_acc: 0.9586\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0391 - acc: 0.9880 - val_loss: 0.3271 - val_acc: 0.9588\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0540 - acc: 0.9825 - val_loss: 0.3297 - val_acc: 0.9595\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0465 - acc: 0.9855 - val_loss: 0.3256 - val_acc: 0.9593\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0420 - acc: 0.9869 - val_loss: 0.3276 - val_acc: 0.9588\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0500 - acc: 0.9847 - val_loss: 0.3217 - val_acc: 0.9582\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0436 - acc: 0.9867 - val_loss: 0.3274 - val_acc: 0.9585\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0433 - acc: 0.9863 - val_loss: 0.3306 - val_acc: 0.9585\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0413 - acc: 0.9874 - val_loss: 0.3355 - val_acc: 0.9589\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0425 - acc: 0.9871 - val_loss: 0.3325 - val_acc: 0.9579\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0520 - acc: 0.9836 - val_loss: 0.3333 - val_acc: 0.9582\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0508 - acc: 0.9846 - val_loss: 0.3317 - val_acc: 0.9580\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0526 - acc: 0.9826 - val_loss: 0.3381 - val_acc: 0.9589\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0485 - acc: 0.9851 - val_loss: 0.3231 - val_acc: 0.9600\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0557 - acc: 0.9820 - val_loss: 0.3262 - val_acc: 0.9576\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0539 - acc: 0.9829 - val_loss: 0.3208 - val_acc: 0.9594\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0498 - acc: 0.9844 - val_loss: 0.3209 - val_acc: 0.9599\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0374 - acc: 0.9878 - val_loss: 0.3233 - val_acc: 0.9599\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0372 - acc: 0.9883 - val_loss: 0.3251 - val_acc: 0.9595\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0521 - acc: 0.9837 - val_loss: 0.3324 - val_acc: 0.9598\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0503 - acc: 0.9848 - val_loss: 0.3238 - val_acc: 0.9589\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0470 - acc: 0.9855 - val_loss: 0.3299 - val_acc: 0.9603\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0412 - acc: 0.9873 - val_loss: 0.3250 - val_acc: 0.9593\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0415 - acc: 0.9870 - val_loss: 0.3290 - val_acc: 0.9595\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0413 - acc: 0.9870 - val_loss: 0.3313 - val_acc: 0.9589\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0508 - acc: 0.9845 - val_loss: 0.3320 - val_acc: 0.9597\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0430 - acc: 0.9866 - val_loss: 0.3396 - val_acc: 0.9592\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0434 - acc: 0.9865 - val_loss: 0.3224 - val_acc: 0.9594\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0410 - acc: 0.9870 - val_loss: 0.3260 - val_acc: 0.9596\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0374 - acc: 0.9886 - val_loss: 0.3398 - val_acc: 0.9590\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0459 - acc: 0.9856 - val_loss: 0.3381 - val_acc: 0.9593\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0434 - acc: 0.9864 - val_loss: 0.3445 - val_acc: 0.9592\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0414 - acc: 0.9871 - val_loss: 0.3346 - val_acc: 0.9600\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0388 - acc: 0.9874 - val_loss: 0.3388 - val_acc: 0.9596\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0392 - acc: 0.9878 - val_loss: 0.3417 - val_acc: 0.9592\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0475 - acc: 0.9851 - val_loss: 0.3390 - val_acc: 0.9593\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0558 - acc: 0.9834 - val_loss: 0.3322 - val_acc: 0.9590\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0404 - acc: 0.9872 - val_loss: 0.3373 - val_acc: 0.9581\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0374 - acc: 0.9880 - val_loss: 0.3389 - val_acc: 0.9583\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0417 - acc: 0.9871 - val_loss: 0.3362 - val_acc: 0.9585\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0501 - acc: 0.9845 - val_loss: 0.3357 - val_acc: 0.9585\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0366 - acc: 0.9885 - val_loss: 0.3371 - val_acc: 0.9595\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0399 - acc: 0.9870 - val_loss: 0.3330 - val_acc: 0.9593\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0522 - acc: 0.9847 - val_loss: 0.3413 - val_acc: 0.9587\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0424 - acc: 0.9866 - val_loss: 0.3385 - val_acc: 0.9590\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0408 - acc: 0.9879 - val_loss: 0.3396 - val_acc: 0.9593\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0425 - acc: 0.9870 - val_loss: 0.3501 - val_acc: 0.9585\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0438 - acc: 0.9861 - val_loss: 0.3344 - val_acc: 0.9603\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0545 - acc: 0.9832 - val_loss: 0.3329 - val_acc: 0.9595\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0389 - acc: 0.9880 - val_loss: 0.3309 - val_acc: 0.9604\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0357 - acc: 0.9884 - val_loss: 0.3386 - val_acc: 0.9593\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0439 - acc: 0.9860 - val_loss: 0.3311 - val_acc: 0.9597\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0497 - acc: 0.9850 - val_loss: 0.3309 - val_acc: 0.9600\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0391 - acc: 0.9880 - val_loss: 0.3358 - val_acc: 0.9605\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0363 - acc: 0.9889 - val_loss: 0.3417 - val_acc: 0.9596\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0422 - acc: 0.9869 - val_loss: 0.3388 - val_acc: 0.9596\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0462 - acc: 0.9860 - val_loss: 0.3300 - val_acc: 0.9592\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0392 - acc: 0.9878 - val_loss: 0.3337 - val_acc: 0.9592\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0343 - acc: 0.9886 - val_loss: 0.3419 - val_acc: 0.9594\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0423 - acc: 0.9870 - val_loss: 0.3415 - val_acc: 0.9571\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0570 - acc: 0.9831 - val_loss: 0.3364 - val_acc: 0.9594\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0398 - acc: 0.9870 - val_loss: 0.3488 - val_acc: 0.9582\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0375 - acc: 0.9880 - val_loss: 0.3440 - val_acc: 0.9583\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0406 - acc: 0.9868 - val_loss: 0.3422 - val_acc: 0.9601\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0351 - acc: 0.9898 - val_loss: 0.3517 - val_acc: 0.9591\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0410 - acc: 0.9870 - val_loss: 0.3463 - val_acc: 0.9595\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0334 - acc: 0.9896 - val_loss: 0.3441 - val_acc: 0.9599\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0559 - acc: 0.9838 - val_loss: 0.3361 - val_acc: 0.9592\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0427 - acc: 0.9866 - val_loss: 0.3381 - val_acc: 0.9593\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0383 - acc: 0.9884 - val_loss: 0.3443 - val_acc: 0.9586\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0376 - acc: 0.9881 - val_loss: 0.3453 - val_acc: 0.9596\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0421 - acc: 0.9873 - val_loss: 0.3447 - val_acc: 0.9584\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0387 - acc: 0.9875 - val_loss: 0.3521 - val_acc: 0.9585\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0481 - acc: 0.9852 - val_loss: 0.3493 - val_acc: 0.9585\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0499 - acc: 0.9842 - val_loss: 0.3396 - val_acc: 0.9605\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0364 - acc: 0.9890 - val_loss: 0.3451 - val_acc: 0.9591\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0356 - acc: 0.9893 - val_loss: 0.3405 - val_acc: 0.9589\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0435 - acc: 0.9864 - val_loss: 0.3455 - val_acc: 0.9601\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0402 - acc: 0.9872 - val_loss: 0.3408 - val_acc: 0.9600\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0338 - acc: 0.9895 - val_loss: 0.3466 - val_acc: 0.9601\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0364 - acc: 0.9887 - val_loss: 0.3484 - val_acc: 0.9595\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0398 - acc: 0.9872 - val_loss: 0.3554 - val_acc: 0.9591\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0357 - acc: 0.9892 - val_loss: 0.3595 - val_acc: 0.9580\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0357 - acc: 0.9890 - val_loss: 0.3511 - val_acc: 0.9597\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0380 - acc: 0.9881 - val_loss: 0.3610 - val_acc: 0.9568\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0451 - acc: 0.9863 - val_loss: 0.3443 - val_acc: 0.9597\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0394 - acc: 0.9869 - val_loss: 0.3471 - val_acc: 0.9599\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0392 - acc: 0.9881 - val_loss: 0.3504 - val_acc: 0.9592\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0382 - acc: 0.9881 - val_loss: 0.3522 - val_acc: 0.9589\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0400 - acc: 0.9872 - val_loss: 0.3521 - val_acc: 0.9596\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0503 - acc: 0.9849 - val_loss: 0.3447 - val_acc: 0.9602\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0312 - acc: 0.9898 - val_loss: 0.3497 - val_acc: 0.9605\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0391 - acc: 0.9880 - val_loss: 0.3376 - val_acc: 0.9595\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0479 - acc: 0.9848 - val_loss: 0.3471 - val_acc: 0.9594\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0351 - acc: 0.9888 - val_loss: 0.3463 - val_acc: 0.9597\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0377 - acc: 0.9886 - val_loss: 0.3475 - val_acc: 0.9599\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0368 - acc: 0.9886 - val_loss: 0.3531 - val_acc: 0.9608\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0356 - acc: 0.9885 - val_loss: 0.3492 - val_acc: 0.9606\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0326 - acc: 0.9899 - val_loss: 0.3503 - val_acc: 0.9610\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0454 - acc: 0.9865 - val_loss: 0.3511 - val_acc: 0.9593\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0430 - acc: 0.9870 - val_loss: 0.3447 - val_acc: 0.9598\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0422 - acc: 0.9870 - val_loss: 0.3534 - val_acc: 0.9599\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0496 - acc: 0.9854 - val_loss: 0.3434 - val_acc: 0.9604\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0329 - acc: 0.9900 - val_loss: 0.3496 - val_acc: 0.9603\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0468 - acc: 0.9857 - val_loss: 0.3352 - val_acc: 0.9599\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0387 - acc: 0.9882 - val_loss: 0.3405 - val_acc: 0.9598\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0339 - acc: 0.9895 - val_loss: 0.3429 - val_acc: 0.9610\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0318 - acc: 0.9899 - val_loss: 0.3532 - val_acc: 0.9594\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0389 - acc: 0.9879 - val_loss: 0.3472 - val_acc: 0.9612\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0386 - acc: 0.9881 - val_loss: 0.3486 - val_acc: 0.9603\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0405 - acc: 0.9869 - val_loss: 0.3552 - val_acc: 0.9592\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0364 - acc: 0.9881 - val_loss: 0.3517 - val_acc: 0.9604\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0396 - acc: 0.9883 - val_loss: 0.3519 - val_acc: 0.9603\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0385 - acc: 0.9881 - val_loss: 0.3423 - val_acc: 0.9604\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0433 - acc: 0.9866 - val_loss: 0.3541 - val_acc: 0.9597\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0347 - acc: 0.9891 - val_loss: 0.3528 - val_acc: 0.9607\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0361 - acc: 0.9893 - val_loss: 0.3557 - val_acc: 0.9597\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0315 - acc: 0.9902 - val_loss: 0.3541 - val_acc: 0.9598\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0390 - acc: 0.9876 - val_loss: 0.3510 - val_acc: 0.9598\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0369 - acc: 0.9892 - val_loss: 0.3637 - val_acc: 0.9594\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0376 - acc: 0.9884 - val_loss: 0.3608 - val_acc: 0.9599\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0344 - acc: 0.9896 - val_loss: 0.3575 - val_acc: 0.9599\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0392 - acc: 0.9881 - val_loss: 0.3535 - val_acc: 0.9596\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.3546 - val_acc: 0.9614\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0316 - acc: 0.9901 - val_loss: 0.3593 - val_acc: 0.9601\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0387 - acc: 0.9870 - val_loss: 0.3532 - val_acc: 0.9595\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0341 - acc: 0.9895 - val_loss: 0.3546 - val_acc: 0.9589\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 4s 105us/sample - loss: 0.0393 - acc: 0.9880 - val_loss: 0.3538 - val_acc: 0.9594\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0340 - acc: 0.9889 - val_loss: 0.3490 - val_acc: 0.9595\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0445 - acc: 0.9863 - val_loss: 0.3530 - val_acc: 0.9600\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0410 - acc: 0.9875 - val_loss: 0.3579 - val_acc: 0.9590\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0396 - acc: 0.9875 - val_loss: 0.3498 - val_acc: 0.9589\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0320 - acc: 0.9904 - val_loss: 0.3521 - val_acc: 0.9604\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0417 - acc: 0.9874 - val_loss: 0.3501 - val_acc: 0.9597\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0317 - acc: 0.9901 - val_loss: 0.3592 - val_acc: 0.9600\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0432 - acc: 0.9869 - val_loss: 0.3547 - val_acc: 0.9601\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0367 - acc: 0.9890 - val_loss: 0.3584 - val_acc: 0.9596\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0429 - acc: 0.9871 - val_loss: 0.3599 - val_acc: 0.9600\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0411 - acc: 0.9874 - val_loss: 0.3504 - val_acc: 0.9595\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0375 - acc: 0.9888 - val_loss: 0.3548 - val_acc: 0.9597\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0278 - acc: 0.9913 - val_loss: 0.3620 - val_acc: 0.9603\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0377 - acc: 0.9883 - val_loss: 0.3612 - val_acc: 0.9590\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0364 - acc: 0.9890 - val_loss: 0.3610 - val_acc: 0.9590\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0346 - acc: 0.9894 - val_loss: 0.3656 - val_acc: 0.9598\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0308 - acc: 0.9912 - val_loss: 0.3515 - val_acc: 0.9603\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0401 - acc: 0.9879 - val_loss: 0.3548 - val_acc: 0.9601\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0324 - acc: 0.9906 - val_loss: 0.3545 - val_acc: 0.9603\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0409 - acc: 0.9879 - val_loss: 0.3504 - val_acc: 0.9609\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0371 - acc: 0.9888 - val_loss: 0.3517 - val_acc: 0.9597\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0371 - acc: 0.9876 - val_loss: 0.3537 - val_acc: 0.9606\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0331 - acc: 0.9900 - val_loss: 0.3573 - val_acc: 0.9602\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0351 - acc: 0.9891 - val_loss: 0.3636 - val_acc: 0.9596\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0405 - acc: 0.9880 - val_loss: 0.3620 - val_acc: 0.9606\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0378 - acc: 0.9884 - val_loss: 0.3525 - val_acc: 0.9602\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0370 - acc: 0.9887 - val_loss: 0.3539 - val_acc: 0.9602\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0343 - acc: 0.9893 - val_loss: 0.3568 - val_acc: 0.9600\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0309 - acc: 0.9902 - val_loss: 0.3598 - val_acc: 0.9604\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0344 - acc: 0.9895 - val_loss: 0.3554 - val_acc: 0.9602\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0458 - acc: 0.9868 - val_loss: 0.3508 - val_acc: 0.9600\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0374 - acc: 0.9885 - val_loss: 0.3477 - val_acc: 0.9593\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0419 - acc: 0.9872 - val_loss: 0.3481 - val_acc: 0.9601\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0340 - acc: 0.9898 - val_loss: 0.3479 - val_acc: 0.9606\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0333 - acc: 0.9900 - val_loss: 0.3537 - val_acc: 0.9598\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0337 - acc: 0.9898 - val_loss: 0.3479 - val_acc: 0.9608\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0330 - acc: 0.9896 - val_loss: 0.3536 - val_acc: 0.9598\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0398 - acc: 0.9873 - val_loss: 0.3508 - val_acc: 0.9593\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0338 - acc: 0.9896 - val_loss: 0.3614 - val_acc: 0.9601\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0333 - acc: 0.9901 - val_loss: 0.3492 - val_acc: 0.9599\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0436 - acc: 0.9874 - val_loss: 0.3499 - val_acc: 0.9606\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 4s 104us/sample - loss: 0.0318 - acc: 0.9906 - val_loss: 0.3553 - val_acc: 0.9600\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.0380 - acc: 0.9882 - val_loss: 0.3605 - val_acc: 0.9587\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0319 - acc: 0.9906 - val_loss: 0.3548 - val_acc: 0.9608\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0367 - acc: 0.9888 - val_loss: 0.3603 - val_acc: 0.9588\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0364 - acc: 0.9888 - val_loss: 0.3456 - val_acc: 0.9607\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0330 - acc: 0.9902 - val_loss: 0.3511 - val_acc: 0.9595\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.0335 - acc: 0.9900 - val_loss: 0.3591 - val_acc: 0.9601\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0311 - acc: 0.9905 - val_loss: 0.3596 - val_acc: 0.9600\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0396 - acc: 0.9876 - val_loss: 0.3598 - val_acc: 0.9595\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0402 - acc: 0.9876 - val_loss: 0.3448 - val_acc: 0.9602\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0387 - acc: 0.9882 - val_loss: 0.3466 - val_acc: 0.9593\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0348 - acc: 0.9900 - val_loss: 0.3491 - val_acc: 0.9607\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0289 - acc: 0.9910 - val_loss: 0.3490 - val_acc: 0.9606\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0318 - acc: 0.9903 - val_loss: 0.3535 - val_acc: 0.9595\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0314 - acc: 0.9905 - val_loss: 0.3582 - val_acc: 0.9595\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0380 - acc: 0.9888 - val_loss: 0.3595 - val_acc: 0.9590\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0408 - acc: 0.9873 - val_loss: 0.3433 - val_acc: 0.9608\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0392 - acc: 0.9874 - val_loss: 0.3588 - val_acc: 0.9604\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0331 - acc: 0.9896 - val_loss: 0.3518 - val_acc: 0.9602\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0309 - acc: 0.9908 - val_loss: 0.3492 - val_acc: 0.9599\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0397 - acc: 0.9881 - val_loss: 0.3534 - val_acc: 0.9596\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0287 - acc: 0.9916 - val_loss: 0.3589 - val_acc: 0.9591\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0374 - acc: 0.9884 - val_loss: 0.3569 - val_acc: 0.9599\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0370 - acc: 0.9886 - val_loss: 0.3554 - val_acc: 0.9606\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0318 - acc: 0.9905 - val_loss: 0.3500 - val_acc: 0.9601\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0346 - acc: 0.9897 - val_loss: 0.3539 - val_acc: 0.9600\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0297 - acc: 0.9906 - val_loss: 0.3512 - val_acc: 0.9599\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0403 - acc: 0.9874 - val_loss: 0.3574 - val_acc: 0.9601\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0305 - acc: 0.9912 - val_loss: 0.3535 - val_acc: 0.9597\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0364 - acc: 0.9896 - val_loss: 0.3532 - val_acc: 0.9600\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0338 - acc: 0.9899 - val_loss: 0.3493 - val_acc: 0.9599\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0361 - acc: 0.9888 - val_loss: 0.3589 - val_acc: 0.9599\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0285 - acc: 0.9914 - val_loss: 0.3600 - val_acc: 0.9594\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0309 - acc: 0.9902 - val_loss: 0.3629 - val_acc: 0.9587\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.0335 - acc: 0.9901 - val_loss: 0.3636 - val_acc: 0.9595\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0335 - acc: 0.9900 - val_loss: 0.3597 - val_acc: 0.9594\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.0414 - acc: 0.9871 - val_loss: 0.3569 - val_acc: 0.9601\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0359 - acc: 0.9897 - val_loss: 0.3554 - val_acc: 0.9607\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 4s 99us/sample - loss: 0.0279 - acc: 0.9911 - val_loss: 0.3492 - val_acc: 0.9600\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0295 - acc: 0.9909 - val_loss: 0.3607 - val_acc: 0.9602\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0341 - acc: 0.9895 - val_loss: 0.3671 - val_acc: 0.9592\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0474 - acc: 0.9862 - val_loss: 0.3638 - val_acc: 0.9599\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 4s 103us/sample - loss: 0.0313 - acc: 0.9901 - val_loss: 0.3681 - val_acc: 0.9595\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 4s 101us/sample - loss: 0.0280 - acc: 0.9916 - val_loss: 0.3676 - val_acc: 0.9597\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 4s 100us/sample - loss: 0.0334 - acc: 0.9897 - val_loss: 0.3607 - val_acc: 0.9600\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 4s 98us/sample - loss: 0.0318 - acc: 0.9904 - val_loss: 0.3657 - val_acc: 0.9598\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0293 - acc: 0.9914 - val_loss: 0.3637 - val_acc: 0.9597\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 4s 102us/sample - loss: 0.0335 - acc: 0.9898 - val_loss: 0.3574 - val_acc: 0.9606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1359e194e0>"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Learning_rate=0.0003221410197800375\n",
    "neurons=[350,250,150,10]\n",
    "Regularize=1.3083277528087247e-07\n",
    "Activation=['relu','relu','relu','softmax']\n",
    "do=0.03572196350233002\n",
    "Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "  \n",
    "model_2=tf.keras.models.Sequential()\n",
    "model_2.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "for i in range(0,len(neurons)):\n",
    "  model_2.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "  if i != (len(neurons)-1):\n",
    "    model_2.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "model_2.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model_2.fit(nfX_train,HY_train,validation_data=(nfX_val,HY_val),batch_size=128,epochs=500,verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ix7Zdv20vph3"
   },
   "source": [
    "Above model seems very good. Lets also try running the model for 500 epochs with following value which has higher accuracy of 90.4% in 30 epochs\n",
    "\n",
    "lr: 0.0005028916162997343, Lambda: 1.427453007116601e-06, Dropout: 0.2285388887981482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1dvIeJMILppy",
    "outputId": "7f438ecb-6d95-465b-844c-65dcfefeb4c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 3s 67us/sample - loss: 1.7423 - acc: 0.3928 - val_loss: 1.1497 - val_acc: 0.6740\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 1.1530 - acc: 0.6310 - val_loss: 0.8667 - val_acc: 0.7373\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.9875 - acc: 0.6906 - val_loss: 0.7736 - val_acc: 0.7673\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.9102 - acc: 0.7144 - val_loss: 0.7028 - val_acc: 0.7896\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.8399 - acc: 0.7417 - val_loss: 0.6404 - val_acc: 0.8089\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.7939 - acc: 0.7539 - val_loss: 0.6177 - val_acc: 0.8121\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.7521 - acc: 0.7687 - val_loss: 0.5900 - val_acc: 0.8248\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.7271 - acc: 0.7765 - val_loss: 0.5374 - val_acc: 0.8432\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6990 - acc: 0.7821 - val_loss: 0.5221 - val_acc: 0.8446\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.6692 - acc: 0.7923 - val_loss: 0.5191 - val_acc: 0.8421\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.6422 - acc: 0.7974 - val_loss: 0.4989 - val_acc: 0.8521\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.6266 - acc: 0.8027 - val_loss: 0.4827 - val_acc: 0.8543\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.6079 - acc: 0.8118 - val_loss: 0.4570 - val_acc: 0.8628\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5951 - acc: 0.8155 - val_loss: 0.4571 - val_acc: 0.8644\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.5808 - acc: 0.8195 - val_loss: 0.4309 - val_acc: 0.8725\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5651 - acc: 0.8226 - val_loss: 0.4219 - val_acc: 0.8740\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5553 - acc: 0.8252 - val_loss: 0.4202 - val_acc: 0.8740\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5437 - acc: 0.8283 - val_loss: 0.4180 - val_acc: 0.8730\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5364 - acc: 0.8291 - val_loss: 0.3957 - val_acc: 0.8826\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.5231 - acc: 0.8361 - val_loss: 0.3905 - val_acc: 0.8826\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5185 - acc: 0.8374 - val_loss: 0.3891 - val_acc: 0.8844\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5051 - acc: 0.8418 - val_loss: 0.3823 - val_acc: 0.8856\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5004 - acc: 0.8432 - val_loss: 0.3729 - val_acc: 0.8893\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4863 - acc: 0.8457 - val_loss: 0.3637 - val_acc: 0.8926\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4777 - acc: 0.8465 - val_loss: 0.3502 - val_acc: 0.8957\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4795 - acc: 0.8475 - val_loss: 0.3504 - val_acc: 0.8959\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4691 - acc: 0.8510 - val_loss: 0.3533 - val_acc: 0.8962\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4565 - acc: 0.8540 - val_loss: 0.3398 - val_acc: 0.9001\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4582 - acc: 0.8541 - val_loss: 0.3306 - val_acc: 0.9038\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4501 - acc: 0.8570 - val_loss: 0.3306 - val_acc: 0.9036\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4431 - acc: 0.8582 - val_loss: 0.3321 - val_acc: 0.9034\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4358 - acc: 0.8600 - val_loss: 0.3196 - val_acc: 0.9068\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4351 - acc: 0.8623 - val_loss: 0.3228 - val_acc: 0.9050\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4334 - acc: 0.8621 - val_loss: 0.3187 - val_acc: 0.9082\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4299 - acc: 0.8625 - val_loss: 0.3111 - val_acc: 0.9117\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4292 - acc: 0.8629 - val_loss: 0.3109 - val_acc: 0.9108\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4082 - acc: 0.8694 - val_loss: 0.3002 - val_acc: 0.9131\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4112 - acc: 0.8678 - val_loss: 0.2939 - val_acc: 0.9147\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4045 - acc: 0.8709 - val_loss: 0.3005 - val_acc: 0.9142\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4004 - acc: 0.8728 - val_loss: 0.2885 - val_acc: 0.9163\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4005 - acc: 0.8730 - val_loss: 0.2901 - val_acc: 0.9168\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3940 - acc: 0.8760 - val_loss: 0.3069 - val_acc: 0.9116\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3921 - acc: 0.8746 - val_loss: 0.2962 - val_acc: 0.9147\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3939 - acc: 0.8752 - val_loss: 0.2959 - val_acc: 0.9143\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3856 - acc: 0.8766 - val_loss: 0.2888 - val_acc: 0.9174\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3866 - acc: 0.8785 - val_loss: 0.2849 - val_acc: 0.9182\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3784 - acc: 0.8799 - val_loss: 0.2771 - val_acc: 0.9217\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3726 - acc: 0.8796 - val_loss: 0.2774 - val_acc: 0.9218\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3698 - acc: 0.8817 - val_loss: 0.2731 - val_acc: 0.9231\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3745 - acc: 0.8800 - val_loss: 0.2912 - val_acc: 0.9186\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3716 - acc: 0.8801 - val_loss: 0.2658 - val_acc: 0.9256\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3597 - acc: 0.8851 - val_loss: 0.2718 - val_acc: 0.9230\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3634 - acc: 0.8841 - val_loss: 0.2822 - val_acc: 0.9191\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3579 - acc: 0.8859 - val_loss: 0.2581 - val_acc: 0.9282\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3517 - acc: 0.8875 - val_loss: 0.2671 - val_acc: 0.9244\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3476 - acc: 0.8885 - val_loss: 0.2643 - val_acc: 0.9260\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3512 - acc: 0.8880 - val_loss: 0.2640 - val_acc: 0.9274\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3451 - acc: 0.8892 - val_loss: 0.2561 - val_acc: 0.9284\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3500 - acc: 0.8882 - val_loss: 0.2535 - val_acc: 0.9299\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3416 - acc: 0.8923 - val_loss: 0.2586 - val_acc: 0.9291\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3434 - acc: 0.8901 - val_loss: 0.2481 - val_acc: 0.9326\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3391 - acc: 0.8920 - val_loss: 0.2601 - val_acc: 0.9270\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3391 - acc: 0.8905 - val_loss: 0.2521 - val_acc: 0.9304\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3385 - acc: 0.8912 - val_loss: 0.2558 - val_acc: 0.9295\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3342 - acc: 0.8929 - val_loss: 0.2539 - val_acc: 0.9301\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3218 - acc: 0.8967 - val_loss: 0.2525 - val_acc: 0.9307\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3366 - acc: 0.8930 - val_loss: 0.2533 - val_acc: 0.9303\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3282 - acc: 0.8944 - val_loss: 0.2484 - val_acc: 0.9323\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3187 - acc: 0.8978 - val_loss: 0.2443 - val_acc: 0.9329\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3299 - acc: 0.8931 - val_loss: 0.2443 - val_acc: 0.9352\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3228 - acc: 0.8978 - val_loss: 0.2391 - val_acc: 0.9349\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3245 - acc: 0.8956 - val_loss: 0.2436 - val_acc: 0.9342\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3199 - acc: 0.8979 - val_loss: 0.2368 - val_acc: 0.9360\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3212 - acc: 0.8965 - val_loss: 0.2424 - val_acc: 0.9345\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3179 - acc: 0.8973 - val_loss: 0.2410 - val_acc: 0.9348\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3177 - acc: 0.8987 - val_loss: 0.2394 - val_acc: 0.9359\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3112 - acc: 0.8997 - val_loss: 0.2441 - val_acc: 0.9342\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3100 - acc: 0.9014 - val_loss: 0.2407 - val_acc: 0.9349\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3097 - acc: 0.9008 - val_loss: 0.2371 - val_acc: 0.9369\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3053 - acc: 0.9015 - val_loss: 0.2361 - val_acc: 0.9368\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3102 - acc: 0.8997 - val_loss: 0.2328 - val_acc: 0.9384\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3020 - acc: 0.9026 - val_loss: 0.2325 - val_acc: 0.9394\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3057 - acc: 0.9035 - val_loss: 0.2361 - val_acc: 0.9377\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3014 - acc: 0.9034 - val_loss: 0.2330 - val_acc: 0.9380\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2983 - acc: 0.9052 - val_loss: 0.2371 - val_acc: 0.9368\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3065 - acc: 0.9023 - val_loss: 0.2279 - val_acc: 0.9404\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2978 - acc: 0.9049 - val_loss: 0.2333 - val_acc: 0.9388\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3006 - acc: 0.9037 - val_loss: 0.2416 - val_acc: 0.9355\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2953 - acc: 0.9065 - val_loss: 0.2344 - val_acc: 0.9388\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2934 - acc: 0.9050 - val_loss: 0.2244 - val_acc: 0.9421\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2931 - acc: 0.9073 - val_loss: 0.2260 - val_acc: 0.9405\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2976 - acc: 0.9049 - val_loss: 0.2253 - val_acc: 0.9410\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2912 - acc: 0.9067 - val_loss: 0.2209 - val_acc: 0.9422\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2908 - acc: 0.9096 - val_loss: 0.2270 - val_acc: 0.9406\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2814 - acc: 0.9106 - val_loss: 0.2235 - val_acc: 0.9412\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2919 - acc: 0.9061 - val_loss: 0.2267 - val_acc: 0.9415\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2753 - acc: 0.9128 - val_loss: 0.2199 - val_acc: 0.9444\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2878 - acc: 0.9087 - val_loss: 0.2243 - val_acc: 0.9420\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2804 - acc: 0.9108 - val_loss: 0.2230 - val_acc: 0.9431\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2830 - acc: 0.9110 - val_loss: 0.2259 - val_acc: 0.9421\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2802 - acc: 0.9104 - val_loss: 0.2163 - val_acc: 0.9449\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2787 - acc: 0.9105 - val_loss: 0.2260 - val_acc: 0.9424\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2820 - acc: 0.9106 - val_loss: 0.2201 - val_acc: 0.9438\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2803 - acc: 0.9098 - val_loss: 0.2185 - val_acc: 0.9445\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2889 - acc: 0.9096 - val_loss: 0.2251 - val_acc: 0.9427\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2742 - acc: 0.9118 - val_loss: 0.2208 - val_acc: 0.9434\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2791 - acc: 0.9107 - val_loss: 0.2303 - val_acc: 0.9405\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2755 - acc: 0.9126 - val_loss: 0.2196 - val_acc: 0.9440\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2700 - acc: 0.9132 - val_loss: 0.2183 - val_acc: 0.9450\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2795 - acc: 0.9111 - val_loss: 0.2168 - val_acc: 0.9450\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2672 - acc: 0.9151 - val_loss: 0.2153 - val_acc: 0.9466\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2774 - acc: 0.9117 - val_loss: 0.2174 - val_acc: 0.9453\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2739 - acc: 0.9120 - val_loss: 0.2143 - val_acc: 0.9470\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2718 - acc: 0.9140 - val_loss: 0.2162 - val_acc: 0.9456\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2730 - acc: 0.9143 - val_loss: 0.2208 - val_acc: 0.9450\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2690 - acc: 0.9128 - val_loss: 0.2154 - val_acc: 0.9456\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2682 - acc: 0.9135 - val_loss: 0.2130 - val_acc: 0.9472\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2630 - acc: 0.9166 - val_loss: 0.2203 - val_acc: 0.9444\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2625 - acc: 0.9164 - val_loss: 0.2136 - val_acc: 0.9457\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2621 - acc: 0.9160 - val_loss: 0.2135 - val_acc: 0.9465\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2685 - acc: 0.9140 - val_loss: 0.2151 - val_acc: 0.9460\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2677 - acc: 0.9155 - val_loss: 0.2064 - val_acc: 0.9498\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2619 - acc: 0.9186 - val_loss: 0.2128 - val_acc: 0.9472\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2590 - acc: 0.9174 - val_loss: 0.2148 - val_acc: 0.9465\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2577 - acc: 0.9178 - val_loss: 0.2071 - val_acc: 0.9489\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2606 - acc: 0.9168 - val_loss: 0.2090 - val_acc: 0.9491\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2597 - acc: 0.9180 - val_loss: 0.2092 - val_acc: 0.9481\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2575 - acc: 0.9182 - val_loss: 0.2079 - val_acc: 0.9487\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2584 - acc: 0.9184 - val_loss: 0.2107 - val_acc: 0.9482\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2612 - acc: 0.9176 - val_loss: 0.2110 - val_acc: 0.9477\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2609 - acc: 0.9181 - val_loss: 0.2133 - val_acc: 0.9475\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2566 - acc: 0.9183 - val_loss: 0.2136 - val_acc: 0.9475\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2550 - acc: 0.9188 - val_loss: 0.2104 - val_acc: 0.9493\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2562 - acc: 0.9192 - val_loss: 0.2062 - val_acc: 0.9506\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2527 - acc: 0.9196 - val_loss: 0.2096 - val_acc: 0.9491\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2481 - acc: 0.9224 - val_loss: 0.2119 - val_acc: 0.9487\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2590 - acc: 0.9182 - val_loss: 0.2074 - val_acc: 0.9507\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2524 - acc: 0.9205 - val_loss: 0.2157 - val_acc: 0.9479\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2477 - acc: 0.9219 - val_loss: 0.2063 - val_acc: 0.9507\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2514 - acc: 0.9210 - val_loss: 0.2066 - val_acc: 0.9508\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2409 - acc: 0.9240 - val_loss: 0.2088 - val_acc: 0.9503\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2493 - acc: 0.9196 - val_loss: 0.2014 - val_acc: 0.9524\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2470 - acc: 0.9218 - val_loss: 0.2049 - val_acc: 0.9516\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2521 - acc: 0.9196 - val_loss: 0.2086 - val_acc: 0.9503\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2490 - acc: 0.9210 - val_loss: 0.2047 - val_acc: 0.9515\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2536 - acc: 0.9196 - val_loss: 0.2039 - val_acc: 0.9520\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2464 - acc: 0.9225 - val_loss: 0.2085 - val_acc: 0.9507\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2485 - acc: 0.9219 - val_loss: 0.2049 - val_acc: 0.9516\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2407 - acc: 0.9242 - val_loss: 0.2036 - val_acc: 0.9513\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2450 - acc: 0.9219 - val_loss: 0.2090 - val_acc: 0.9503\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2484 - acc: 0.9230 - val_loss: 0.2032 - val_acc: 0.9523\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2392 - acc: 0.9249 - val_loss: 0.2115 - val_acc: 0.9488\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2447 - acc: 0.9228 - val_loss: 0.2047 - val_acc: 0.9520\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2414 - acc: 0.9244 - val_loss: 0.2115 - val_acc: 0.9493\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2445 - acc: 0.9235 - val_loss: 0.2074 - val_acc: 0.9514\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2425 - acc: 0.9245 - val_loss: 0.2057 - val_acc: 0.9504\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2424 - acc: 0.9255 - val_loss: 0.2035 - val_acc: 0.9526\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2457 - acc: 0.9238 - val_loss: 0.2034 - val_acc: 0.9519\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2355 - acc: 0.9265 - val_loss: 0.2039 - val_acc: 0.9527\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2399 - acc: 0.9243 - val_loss: 0.2042 - val_acc: 0.9528\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2395 - acc: 0.9253 - val_loss: 0.2085 - val_acc: 0.9498\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2418 - acc: 0.9238 - val_loss: 0.2083 - val_acc: 0.9506\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2388 - acc: 0.9233 - val_loss: 0.2063 - val_acc: 0.9510\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2277 - acc: 0.9295 - val_loss: 0.2019 - val_acc: 0.9533\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2462 - acc: 0.9232 - val_loss: 0.2115 - val_acc: 0.9496\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2331 - acc: 0.9268 - val_loss: 0.2052 - val_acc: 0.9522\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2383 - acc: 0.9256 - val_loss: 0.2033 - val_acc: 0.9526\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2354 - acc: 0.9265 - val_loss: 0.2105 - val_acc: 0.9505\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2364 - acc: 0.9259 - val_loss: 0.1983 - val_acc: 0.9548\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2317 - acc: 0.9280 - val_loss: 0.2078 - val_acc: 0.9522\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 2s 60us/sample - loss: 0.2389 - acc: 0.9259 - val_loss: 0.2044 - val_acc: 0.9522\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2335 - acc: 0.9270 - val_loss: 0.2064 - val_acc: 0.9517\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2350 - acc: 0.9260 - val_loss: 0.2041 - val_acc: 0.9527\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2294 - acc: 0.9285 - val_loss: 0.1983 - val_acc: 0.9544\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2280 - acc: 0.9282 - val_loss: 0.2075 - val_acc: 0.9505\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2327 - acc: 0.9275 - val_loss: 0.2015 - val_acc: 0.9536\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2344 - acc: 0.9267 - val_loss: 0.1992 - val_acc: 0.9540\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2329 - acc: 0.9267 - val_loss: 0.1969 - val_acc: 0.9536\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2294 - acc: 0.9277 - val_loss: 0.2008 - val_acc: 0.9541\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 2s 60us/sample - loss: 0.2363 - acc: 0.9271 - val_loss: 0.2012 - val_acc: 0.9529\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2279 - acc: 0.9279 - val_loss: 0.2045 - val_acc: 0.9523\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2380 - acc: 0.9254 - val_loss: 0.1998 - val_acc: 0.9549\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2313 - acc: 0.9294 - val_loss: 0.2070 - val_acc: 0.9517\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2309 - acc: 0.9286 - val_loss: 0.2042 - val_acc: 0.9524\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2337 - acc: 0.9278 - val_loss: 0.1994 - val_acc: 0.9550\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2235 - acc: 0.9292 - val_loss: 0.2049 - val_acc: 0.9531\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2298 - acc: 0.9286 - val_loss: 0.2006 - val_acc: 0.9541\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2320 - acc: 0.9284 - val_loss: 0.2060 - val_acc: 0.9512\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2285 - acc: 0.9272 - val_loss: 0.2043 - val_acc: 0.9522\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2301 - acc: 0.9285 - val_loss: 0.2033 - val_acc: 0.9530\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2284 - acc: 0.9281 - val_loss: 0.1992 - val_acc: 0.9549\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2288 - acc: 0.9292 - val_loss: 0.2031 - val_acc: 0.9526\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2202 - acc: 0.9311 - val_loss: 0.1974 - val_acc: 0.9546\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2231 - acc: 0.9321 - val_loss: 0.1969 - val_acc: 0.9552\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2190 - acc: 0.9330 - val_loss: 0.2023 - val_acc: 0.9544\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2244 - acc: 0.9294 - val_loss: 0.2006 - val_acc: 0.9548\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2239 - acc: 0.9317 - val_loss: 0.2025 - val_acc: 0.9548\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2251 - acc: 0.9299 - val_loss: 0.2036 - val_acc: 0.9533\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2166 - acc: 0.9328 - val_loss: 0.2011 - val_acc: 0.9539\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2274 - acc: 0.9292 - val_loss: 0.2007 - val_acc: 0.9534\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2232 - acc: 0.9300 - val_loss: 0.1956 - val_acc: 0.9564\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2173 - acc: 0.9320 - val_loss: 0.2021 - val_acc: 0.9539\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2211 - acc: 0.9315 - val_loss: 0.1967 - val_acc: 0.9557\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2229 - acc: 0.9326 - val_loss: 0.2025 - val_acc: 0.9539\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2269 - acc: 0.9297 - val_loss: 0.1989 - val_acc: 0.9558\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2180 - acc: 0.9318 - val_loss: 0.2007 - val_acc: 0.9549\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2163 - acc: 0.9328 - val_loss: 0.2003 - val_acc: 0.9551\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2187 - acc: 0.9318 - val_loss: 0.2026 - val_acc: 0.9545\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2276 - acc: 0.9300 - val_loss: 0.1968 - val_acc: 0.9565\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2130 - acc: 0.9329 - val_loss: 0.2021 - val_acc: 0.9544\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2244 - acc: 0.9308 - val_loss: 0.1990 - val_acc: 0.9552\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2195 - acc: 0.9313 - val_loss: 0.1997 - val_acc: 0.9550\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2165 - acc: 0.9315 - val_loss: 0.2021 - val_acc: 0.9545\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2159 - acc: 0.9328 - val_loss: 0.2073 - val_acc: 0.9529\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2203 - acc: 0.9313 - val_loss: 0.2052 - val_acc: 0.9540\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2154 - acc: 0.9335 - val_loss: 0.1970 - val_acc: 0.9565\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2175 - acc: 0.9345 - val_loss: 0.1997 - val_acc: 0.9547\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2100 - acc: 0.9365 - val_loss: 0.1987 - val_acc: 0.9566\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2137 - acc: 0.9335 - val_loss: 0.1960 - val_acc: 0.9560\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 2s 60us/sample - loss: 0.2139 - acc: 0.9341 - val_loss: 0.1949 - val_acc: 0.9564\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2172 - acc: 0.9328 - val_loss: 0.2062 - val_acc: 0.9520\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2198 - acc: 0.9320 - val_loss: 0.1993 - val_acc: 0.9549\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2177 - acc: 0.9324 - val_loss: 0.1955 - val_acc: 0.9569\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2126 - acc: 0.9344 - val_loss: 0.1960 - val_acc: 0.9562\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2155 - acc: 0.9322 - val_loss: 0.1981 - val_acc: 0.9567\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2157 - acc: 0.9326 - val_loss: 0.1977 - val_acc: 0.9565\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2117 - acc: 0.9348 - val_loss: 0.1957 - val_acc: 0.9566\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2128 - acc: 0.9335 - val_loss: 0.1961 - val_acc: 0.9572\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2185 - acc: 0.9319 - val_loss: 0.2008 - val_acc: 0.9546\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2198 - acc: 0.9321 - val_loss: 0.1989 - val_acc: 0.9559\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2095 - acc: 0.9351 - val_loss: 0.1988 - val_acc: 0.9559\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2160 - acc: 0.9337 - val_loss: 0.1975 - val_acc: 0.9561\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2111 - acc: 0.9347 - val_loss: 0.1945 - val_acc: 0.9581\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2139 - acc: 0.9342 - val_loss: 0.1941 - val_acc: 0.9577\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2152 - acc: 0.9343 - val_loss: 0.2021 - val_acc: 0.9549\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2162 - acc: 0.9337 - val_loss: 0.1929 - val_acc: 0.9574\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2070 - acc: 0.9364 - val_loss: 0.1982 - val_acc: 0.9564\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2123 - acc: 0.9343 - val_loss: 0.1998 - val_acc: 0.9554\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2117 - acc: 0.9355 - val_loss: 0.1944 - val_acc: 0.9575\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2093 - acc: 0.9350 - val_loss: 0.2002 - val_acc: 0.9557\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2074 - acc: 0.9360 - val_loss: 0.1983 - val_acc: 0.9561\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2115 - acc: 0.9344 - val_loss: 0.1939 - val_acc: 0.9579\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2087 - acc: 0.9360 - val_loss: 0.2001 - val_acc: 0.9561\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2121 - acc: 0.9342 - val_loss: 0.2005 - val_acc: 0.9560\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2097 - acc: 0.9356 - val_loss: 0.1956 - val_acc: 0.9576\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2112 - acc: 0.9353 - val_loss: 0.2010 - val_acc: 0.9552\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2133 - acc: 0.9345 - val_loss: 0.1976 - val_acc: 0.9571\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2011 - acc: 0.9375 - val_loss: 0.1944 - val_acc: 0.9577\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2074 - acc: 0.9374 - val_loss: 0.1972 - val_acc: 0.9571\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2087 - acc: 0.9371 - val_loss: 0.2012 - val_acc: 0.9560\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2107 - acc: 0.9355 - val_loss: 0.1955 - val_acc: 0.9567\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2048 - acc: 0.9377 - val_loss: 0.1990 - val_acc: 0.9562\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2091 - acc: 0.9361 - val_loss: 0.2012 - val_acc: 0.9557\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2089 - acc: 0.9363 - val_loss: 0.1975 - val_acc: 0.9567\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2094 - acc: 0.9367 - val_loss: 0.1987 - val_acc: 0.9565\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2087 - acc: 0.9356 - val_loss: 0.1960 - val_acc: 0.9578\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1956 - acc: 0.9404 - val_loss: 0.1968 - val_acc: 0.9586\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2084 - acc: 0.9361 - val_loss: 0.1967 - val_acc: 0.9571\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2081 - acc: 0.9369 - val_loss: 0.1972 - val_acc: 0.9578\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2056 - acc: 0.9360 - val_loss: 0.1937 - val_acc: 0.9585\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2087 - acc: 0.9380 - val_loss: 0.1954 - val_acc: 0.9581\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2074 - acc: 0.9373 - val_loss: 0.2007 - val_acc: 0.9569\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2097 - acc: 0.9369 - val_loss: 0.1956 - val_acc: 0.9584\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2077 - acc: 0.9365 - val_loss: 0.1957 - val_acc: 0.9575\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1983 - acc: 0.9378 - val_loss: 0.1974 - val_acc: 0.9565\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2094 - acc: 0.9356 - val_loss: 0.1961 - val_acc: 0.9576\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2085 - acc: 0.9376 - val_loss: 0.1948 - val_acc: 0.9589\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2023 - acc: 0.9389 - val_loss: 0.1977 - val_acc: 0.9578\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2056 - acc: 0.9377 - val_loss: 0.1980 - val_acc: 0.9578\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2046 - acc: 0.9374 - val_loss: 0.1972 - val_acc: 0.9584\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2004 - acc: 0.9387 - val_loss: 0.1976 - val_acc: 0.9582\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2039 - acc: 0.9369 - val_loss: 0.1997 - val_acc: 0.9565\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2083 - acc: 0.9370 - val_loss: 0.1979 - val_acc: 0.9575\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2013 - acc: 0.9379 - val_loss: 0.1950 - val_acc: 0.9594\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2039 - acc: 0.9389 - val_loss: 0.1988 - val_acc: 0.9574\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2016 - acc: 0.9382 - val_loss: 0.2000 - val_acc: 0.9568\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2012 - acc: 0.9387 - val_loss: 0.1944 - val_acc: 0.9584\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1956 - acc: 0.9404 - val_loss: 0.2007 - val_acc: 0.9575\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2005 - acc: 0.9401 - val_loss: 0.1953 - val_acc: 0.9584\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2047 - acc: 0.9392 - val_loss: 0.1976 - val_acc: 0.9577\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2041 - acc: 0.9375 - val_loss: 0.1968 - val_acc: 0.9583\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2053 - acc: 0.9384 - val_loss: 0.1966 - val_acc: 0.9586\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2006 - acc: 0.9388 - val_loss: 0.1944 - val_acc: 0.9586\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1975 - acc: 0.9404 - val_loss: 0.2026 - val_acc: 0.9569\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2007 - acc: 0.9393 - val_loss: 0.1933 - val_acc: 0.9589\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2010 - acc: 0.9390 - val_loss: 0.1990 - val_acc: 0.9570\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1983 - acc: 0.9389 - val_loss: 0.1975 - val_acc: 0.9582\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2043 - acc: 0.9373 - val_loss: 0.1956 - val_acc: 0.9586\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1981 - acc: 0.9411 - val_loss: 0.1959 - val_acc: 0.9587\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2009 - acc: 0.9391 - val_loss: 0.1952 - val_acc: 0.9586\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2009 - acc: 0.9391 - val_loss: 0.1970 - val_acc: 0.9580\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1937 - acc: 0.9411 - val_loss: 0.1963 - val_acc: 0.9593\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1977 - acc: 0.9400 - val_loss: 0.1961 - val_acc: 0.9599\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2023 - acc: 0.9391 - val_loss: 0.1977 - val_acc: 0.9582\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1975 - acc: 0.9400 - val_loss: 0.2020 - val_acc: 0.9572\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1980 - acc: 0.9401 - val_loss: 0.1954 - val_acc: 0.9584\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2031 - acc: 0.9390 - val_loss: 0.1988 - val_acc: 0.9578\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2007 - acc: 0.9400 - val_loss: 0.1940 - val_acc: 0.9591\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2013 - acc: 0.9393 - val_loss: 0.1968 - val_acc: 0.9588\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.1969 - acc: 0.9408 - val_loss: 0.1949 - val_acc: 0.9591\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1935 - acc: 0.9417 - val_loss: 0.1989 - val_acc: 0.9581\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1959 - acc: 0.9406 - val_loss: 0.2034 - val_acc: 0.9568\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1930 - acc: 0.9422 - val_loss: 0.1987 - val_acc: 0.9588\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1970 - acc: 0.9409 - val_loss: 0.1996 - val_acc: 0.9576\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1947 - acc: 0.9416 - val_loss: 0.1979 - val_acc: 0.9583\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2011 - acc: 0.9383 - val_loss: 0.1955 - val_acc: 0.9589\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1961 - acc: 0.9404 - val_loss: 0.1939 - val_acc: 0.9591\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1932 - acc: 0.9425 - val_loss: 0.1963 - val_acc: 0.9590\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1985 - acc: 0.9399 - val_loss: 0.1984 - val_acc: 0.9583\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1952 - acc: 0.9420 - val_loss: 0.1970 - val_acc: 0.9587\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1963 - acc: 0.9419 - val_loss: 0.1977 - val_acc: 0.9585\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1986 - acc: 0.9405 - val_loss: 0.1960 - val_acc: 0.9591\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1960 - acc: 0.9413 - val_loss: 0.1946 - val_acc: 0.9593\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1923 - acc: 0.9418 - val_loss: 0.2000 - val_acc: 0.9580\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1973 - acc: 0.9408 - val_loss: 0.1958 - val_acc: 0.9588\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1982 - acc: 0.9407 - val_loss: 0.1967 - val_acc: 0.9589\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1954 - acc: 0.9412 - val_loss: 0.1959 - val_acc: 0.9592\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1919 - acc: 0.9424 - val_loss: 0.1936 - val_acc: 0.9594\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1925 - acc: 0.9412 - val_loss: 0.1961 - val_acc: 0.9593\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.1993 - acc: 0.9397 - val_loss: 0.1984 - val_acc: 0.9583\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1900 - acc: 0.9433 - val_loss: 0.1982 - val_acc: 0.9585\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1937 - acc: 0.9421 - val_loss: 0.1946 - val_acc: 0.9595\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1953 - acc: 0.9406 - val_loss: 0.1969 - val_acc: 0.9592\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1913 - acc: 0.9426 - val_loss: 0.1994 - val_acc: 0.9586\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2003 - acc: 0.9399 - val_loss: 0.1962 - val_acc: 0.9593\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1965 - acc: 0.9412 - val_loss: 0.1955 - val_acc: 0.9595\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1846 - acc: 0.9441 - val_loss: 0.1958 - val_acc: 0.9600\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1950 - acc: 0.9411 - val_loss: 0.1968 - val_acc: 0.9594\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1939 - acc: 0.9406 - val_loss: 0.1981 - val_acc: 0.9605\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1890 - acc: 0.9433 - val_loss: 0.1992 - val_acc: 0.9585\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1913 - acc: 0.9424 - val_loss: 0.1951 - val_acc: 0.9591\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1847 - acc: 0.9439 - val_loss: 0.1959 - val_acc: 0.9600\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1909 - acc: 0.9433 - val_loss: 0.1981 - val_acc: 0.9590\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1882 - acc: 0.9442 - val_loss: 0.1963 - val_acc: 0.9595\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2027 - acc: 0.9392 - val_loss: 0.1951 - val_acc: 0.9599\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1946 - acc: 0.9414 - val_loss: 0.1960 - val_acc: 0.9597\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1960 - acc: 0.9410 - val_loss: 0.1950 - val_acc: 0.9596\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1955 - acc: 0.9424 - val_loss: 0.1947 - val_acc: 0.9600\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1893 - acc: 0.9428 - val_loss: 0.1974 - val_acc: 0.9597\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1917 - acc: 0.9419 - val_loss: 0.1966 - val_acc: 0.9590\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1974 - acc: 0.9411 - val_loss: 0.1964 - val_acc: 0.9590\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1857 - acc: 0.9444 - val_loss: 0.1955 - val_acc: 0.9598\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1906 - acc: 0.9436 - val_loss: 0.1980 - val_acc: 0.9587\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1900 - acc: 0.9439 - val_loss: 0.1920 - val_acc: 0.9610\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1847 - acc: 0.9447 - val_loss: 0.1986 - val_acc: 0.9586\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1910 - acc: 0.9424 - val_loss: 0.2004 - val_acc: 0.9583\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1825 - acc: 0.9455 - val_loss: 0.2012 - val_acc: 0.9588\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1884 - acc: 0.9445 - val_loss: 0.1997 - val_acc: 0.9589\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2013 - acc: 0.9396 - val_loss: 0.1971 - val_acc: 0.9584\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1908 - acc: 0.9437 - val_loss: 0.1940 - val_acc: 0.9598\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1910 - acc: 0.9425 - val_loss: 0.1974 - val_acc: 0.9592\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1960 - acc: 0.9417 - val_loss: 0.1949 - val_acc: 0.9598\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1955 - acc: 0.9431 - val_loss: 0.1937 - val_acc: 0.9600\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1908 - acc: 0.9431 - val_loss: 0.1963 - val_acc: 0.9602\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1851 - acc: 0.9450 - val_loss: 0.1941 - val_acc: 0.9606\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1940 - acc: 0.9428 - val_loss: 0.2005 - val_acc: 0.9585\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1880 - acc: 0.9445 - val_loss: 0.1958 - val_acc: 0.9594\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1890 - acc: 0.9437 - val_loss: 0.1953 - val_acc: 0.9608\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1895 - acc: 0.9439 - val_loss: 0.1939 - val_acc: 0.9595\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1841 - acc: 0.9458 - val_loss: 0.1968 - val_acc: 0.9601\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1895 - acc: 0.9445 - val_loss: 0.1947 - val_acc: 0.9602\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1916 - acc: 0.9436 - val_loss: 0.1976 - val_acc: 0.9591\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1933 - acc: 0.9433 - val_loss: 0.1956 - val_acc: 0.9599\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1868 - acc: 0.9448 - val_loss: 0.1971 - val_acc: 0.9599\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1903 - acc: 0.9424 - val_loss: 0.1941 - val_acc: 0.9600\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1859 - acc: 0.9439 - val_loss: 0.1980 - val_acc: 0.9599\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1898 - acc: 0.9435 - val_loss: 0.2004 - val_acc: 0.9592\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1887 - acc: 0.9436 - val_loss: 0.1944 - val_acc: 0.9607\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1749 - acc: 0.9485 - val_loss: 0.1984 - val_acc: 0.9598\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1908 - acc: 0.9429 - val_loss: 0.1966 - val_acc: 0.9592\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1879 - acc: 0.9446 - val_loss: 0.1978 - val_acc: 0.9603\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1862 - acc: 0.9453 - val_loss: 0.1948 - val_acc: 0.9606\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1883 - acc: 0.9439 - val_loss: 0.1995 - val_acc: 0.9592\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1875 - acc: 0.9446 - val_loss: 0.1963 - val_acc: 0.9596\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1831 - acc: 0.9449 - val_loss: 0.1972 - val_acc: 0.9593\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1798 - acc: 0.9469 - val_loss: 0.1986 - val_acc: 0.9599\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1867 - acc: 0.9465 - val_loss: 0.1985 - val_acc: 0.9594\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1883 - acc: 0.9434 - val_loss: 0.1993 - val_acc: 0.9596\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1927 - acc: 0.9435 - val_loss: 0.1951 - val_acc: 0.9600\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1900 - acc: 0.9448 - val_loss: 0.1978 - val_acc: 0.9592\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1919 - acc: 0.9437 - val_loss: 0.1981 - val_acc: 0.9591\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1905 - acc: 0.9445 - val_loss: 0.2008 - val_acc: 0.9588\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1841 - acc: 0.9456 - val_loss: 0.1964 - val_acc: 0.9594\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 2s 60us/sample - loss: 0.1883 - acc: 0.9455 - val_loss: 0.1962 - val_acc: 0.9607\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1835 - acc: 0.9456 - val_loss: 0.1964 - val_acc: 0.9609\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1835 - acc: 0.9459 - val_loss: 0.1969 - val_acc: 0.9607\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1839 - acc: 0.9469 - val_loss: 0.1994 - val_acc: 0.9595\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1836 - acc: 0.9458 - val_loss: 0.1996 - val_acc: 0.9589\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1878 - acc: 0.9445 - val_loss: 0.1939 - val_acc: 0.9612\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1845 - acc: 0.9459 - val_loss: 0.1952 - val_acc: 0.9606\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1897 - acc: 0.9451 - val_loss: 0.1955 - val_acc: 0.9606\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1837 - acc: 0.9445 - val_loss: 0.1951 - val_acc: 0.9612\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1837 - acc: 0.9450 - val_loss: 0.2030 - val_acc: 0.9584\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1844 - acc: 0.9457 - val_loss: 0.1977 - val_acc: 0.9601\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1861 - acc: 0.9446 - val_loss: 0.1970 - val_acc: 0.9600\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1867 - acc: 0.9452 - val_loss: 0.1994 - val_acc: 0.9602\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1876 - acc: 0.9456 - val_loss: 0.1982 - val_acc: 0.9599\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1855 - acc: 0.9463 - val_loss: 0.1969 - val_acc: 0.9602\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1832 - acc: 0.9470 - val_loss: 0.2008 - val_acc: 0.9589\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1846 - acc: 0.9454 - val_loss: 0.1943 - val_acc: 0.9613\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1812 - acc: 0.9481 - val_loss: 0.1988 - val_acc: 0.9601\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1855 - acc: 0.9462 - val_loss: 0.1988 - val_acc: 0.9596\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1900 - acc: 0.9448 - val_loss: 0.1964 - val_acc: 0.9601\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1830 - acc: 0.9469 - val_loss: 0.1938 - val_acc: 0.9614\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1864 - acc: 0.9448 - val_loss: 0.1959 - val_acc: 0.9600\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1879 - acc: 0.9455 - val_loss: 0.1945 - val_acc: 0.9601\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1843 - acc: 0.9460 - val_loss: 0.1957 - val_acc: 0.9607\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1775 - acc: 0.9479 - val_loss: 0.2016 - val_acc: 0.9585\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1866 - acc: 0.9463 - val_loss: 0.1977 - val_acc: 0.9605\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1920 - acc: 0.9425 - val_loss: 0.1961 - val_acc: 0.9607\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1744 - acc: 0.9492 - val_loss: 0.2010 - val_acc: 0.9600\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1759 - acc: 0.9483 - val_loss: 0.1973 - val_acc: 0.9607\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1835 - acc: 0.9471 - val_loss: 0.1977 - val_acc: 0.9601\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1849 - acc: 0.9460 - val_loss: 0.1998 - val_acc: 0.9595\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1755 - acc: 0.9478 - val_loss: 0.1981 - val_acc: 0.9609\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1851 - acc: 0.9452 - val_loss: 0.1989 - val_acc: 0.9599\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1836 - acc: 0.9464 - val_loss: 0.1981 - val_acc: 0.9602\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1831 - acc: 0.9476 - val_loss: 0.1971 - val_acc: 0.9606\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1794 - acc: 0.9476 - val_loss: 0.1971 - val_acc: 0.9606\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1795 - acc: 0.9485 - val_loss: 0.1965 - val_acc: 0.9606\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1824 - acc: 0.9467 - val_loss: 0.1973 - val_acc: 0.9614\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1813 - acc: 0.9478 - val_loss: 0.1983 - val_acc: 0.9606\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1773 - acc: 0.9485 - val_loss: 0.1984 - val_acc: 0.9617\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1791 - acc: 0.9486 - val_loss: 0.1972 - val_acc: 0.9608\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1750 - acc: 0.9491 - val_loss: 0.1954 - val_acc: 0.9614\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1828 - acc: 0.9465 - val_loss: 0.1979 - val_acc: 0.9605\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1819 - acc: 0.9480 - val_loss: 0.1990 - val_acc: 0.9605\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1799 - acc: 0.9466 - val_loss: 0.2004 - val_acc: 0.9594\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1786 - acc: 0.9476 - val_loss: 0.1995 - val_acc: 0.9585\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1803 - acc: 0.9463 - val_loss: 0.1956 - val_acc: 0.9612\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1803 - acc: 0.9480 - val_loss: 0.1961 - val_acc: 0.9613\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1805 - acc: 0.9471 - val_loss: 0.1940 - val_acc: 0.9613\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1741 - acc: 0.9497 - val_loss: 0.1974 - val_acc: 0.9611\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1782 - acc: 0.9479 - val_loss: 0.1968 - val_acc: 0.9612\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1868 - acc: 0.9455 - val_loss: 0.1946 - val_acc: 0.9622\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1830 - acc: 0.9466 - val_loss: 0.1946 - val_acc: 0.9622\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1822 - acc: 0.9469 - val_loss: 0.1973 - val_acc: 0.9622\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1741 - acc: 0.9486 - val_loss: 0.2009 - val_acc: 0.9601\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1794 - acc: 0.9476 - val_loss: 0.1986 - val_acc: 0.9611\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1796 - acc: 0.9482 - val_loss: 0.1980 - val_acc: 0.9608\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1832 - acc: 0.9472 - val_loss: 0.2003 - val_acc: 0.9604\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1813 - acc: 0.9476 - val_loss: 0.2020 - val_acc: 0.9607\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1843 - acc: 0.9466 - val_loss: 0.1972 - val_acc: 0.9610\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1783 - acc: 0.9477 - val_loss: 0.1965 - val_acc: 0.9614\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1745 - acc: 0.9492 - val_loss: 0.1977 - val_acc: 0.9607\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1817 - acc: 0.9487 - val_loss: 0.1995 - val_acc: 0.9603\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1861 - acc: 0.9459 - val_loss: 0.1964 - val_acc: 0.9608\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1818 - acc: 0.9471 - val_loss: 0.1950 - val_acc: 0.9614\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1758 - acc: 0.9494 - val_loss: 0.1961 - val_acc: 0.9621\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1872 - acc: 0.9466 - val_loss: 0.1972 - val_acc: 0.9609\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1825 - acc: 0.9466 - val_loss: 0.1954 - val_acc: 0.9614\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1781 - acc: 0.9483 - val_loss: 0.1982 - val_acc: 0.9612\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1737 - acc: 0.9497 - val_loss: 0.2001 - val_acc: 0.9604\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1818 - acc: 0.9474 - val_loss: 0.1993 - val_acc: 0.9613\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1814 - acc: 0.9473 - val_loss: 0.1935 - val_acc: 0.9620\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1833 - acc: 0.9477 - val_loss: 0.1934 - val_acc: 0.9621\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1830 - acc: 0.9480 - val_loss: 0.1971 - val_acc: 0.9608\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1854 - acc: 0.9469 - val_loss: 0.1951 - val_acc: 0.9619\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1738 - acc: 0.9498 - val_loss: 0.2003 - val_acc: 0.9614\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1764 - acc: 0.9505 - val_loss: 0.1979 - val_acc: 0.9611\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1899 - acc: 0.9453 - val_loss: 0.1948 - val_acc: 0.9617\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1811 - acc: 0.9489 - val_loss: 0.1960 - val_acc: 0.9614\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1789 - acc: 0.9485 - val_loss: 0.1946 - val_acc: 0.9621\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1739 - acc: 0.9508 - val_loss: 0.1981 - val_acc: 0.9610\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1743 - acc: 0.9503 - val_loss: 0.1949 - val_acc: 0.9625\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1763 - acc: 0.9485 - val_loss: 0.1961 - val_acc: 0.9611\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1736 - acc: 0.9507 - val_loss: 0.1982 - val_acc: 0.9606\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1798 - acc: 0.9478 - val_loss: 0.1984 - val_acc: 0.9613\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1724 - acc: 0.9501 - val_loss: 0.1981 - val_acc: 0.9615\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1735 - acc: 0.9504 - val_loss: 0.1945 - val_acc: 0.9623\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1877 - acc: 0.9451 - val_loss: 0.2002 - val_acc: 0.9596\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1759 - acc: 0.9499 - val_loss: 0.1984 - val_acc: 0.9611\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1841 - acc: 0.9473 - val_loss: 0.1987 - val_acc: 0.9614\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1789 - acc: 0.9493 - val_loss: 0.1955 - val_acc: 0.9614\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1727 - acc: 0.9497 - val_loss: 0.1973 - val_acc: 0.9614\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1797 - acc: 0.9478 - val_loss: 0.1981 - val_acc: 0.9611\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1804 - acc: 0.9484 - val_loss: 0.1941 - val_acc: 0.9622\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1797 - acc: 0.9489 - val_loss: 0.1962 - val_acc: 0.9622\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1730 - acc: 0.9506 - val_loss: 0.1984 - val_acc: 0.9615\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1867 - acc: 0.9468 - val_loss: 0.1971 - val_acc: 0.9610\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1769 - acc: 0.9495 - val_loss: 0.1966 - val_acc: 0.9618\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1839 - acc: 0.9466 - val_loss: 0.1928 - val_acc: 0.9621\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1812 - acc: 0.9470 - val_loss: 0.1953 - val_acc: 0.9611\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1743 - acc: 0.9516 - val_loss: 0.1941 - val_acc: 0.9621\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1768 - acc: 0.9502 - val_loss: 0.1976 - val_acc: 0.9621\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1718 - acc: 0.9513 - val_loss: 0.1958 - val_acc: 0.9616\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1704 - acc: 0.9506 - val_loss: 0.1963 - val_acc: 0.9615\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1705 - acc: 0.9513 - val_loss: 0.1999 - val_acc: 0.9615\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1840 - acc: 0.9474 - val_loss: 0.1990 - val_acc: 0.9616\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1805 - acc: 0.9480 - val_loss: 0.2016 - val_acc: 0.9605\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1830 - acc: 0.9461 - val_loss: 0.1971 - val_acc: 0.9604\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1774 - acc: 0.9497 - val_loss: 0.1955 - val_acc: 0.9617\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1757 - acc: 0.9509 - val_loss: 0.1965 - val_acc: 0.9619\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1779 - acc: 0.9500 - val_loss: 0.1960 - val_acc: 0.9624\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1786 - acc: 0.9498 - val_loss: 0.1973 - val_acc: 0.9614\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.1737 - acc: 0.9494 - val_loss: 0.1994 - val_acc: 0.9611\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1720 - acc: 0.9517 - val_loss: 0.2000 - val_acc: 0.9610\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1786 - acc: 0.9490 - val_loss: 0.1939 - val_acc: 0.9625\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.1770 - acc: 0.9504 - val_loss: 0.1964 - val_acc: 0.9622\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.1728 - acc: 0.9510 - val_loss: 0.1983 - val_acc: 0.9620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc7f8692940>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "Learning_rate=0.0005028916162997343\n",
    "neurons=[350,250,150,10]\n",
    "Regularize=1.427453007116601e-06\n",
    "Activation=['relu','relu','relu','softmax']\n",
    "do=0.2285388887981482\n",
    "Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=False, patience=5)\n",
    "\n",
    "model_3=tf.keras.models.Sequential()\n",
    "model_3.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "for i in range(0,len(neurons)):\n",
    "  model_3.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "  if i != (len(neurons)-1):\n",
    "    model_3.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "model_3.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model_3.fit(nfX_train,HY_train,validation_data=(nfX_val,HY_val),batch_size=128,epochs=500,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ECJpkPIQqhI7"
   },
   "source": [
    "Validation accuracy is greater than the training accuracy. This could be due to distribution in the dataset.But this model is good too.\n",
    "\n",
    "Lets try with different parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vZMZspSo3Bmt",
    "outputId": "4f7c95a1-2936-4a71-fb34-5a0ee47c47d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 3s 68us/sample - loss: 2.3232 - acc: 0.1566 - val_loss: 2.0531 - val_acc: 0.4083\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 1.8712 - acc: 0.3379 - val_loss: 1.3745 - val_acc: 0.6251\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 1.5177 - acc: 0.4872 - val_loss: 1.1052 - val_acc: 0.6830\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 1.3376 - acc: 0.5656 - val_loss: 0.9842 - val_acc: 0.7183\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 1.2303 - acc: 0.6084 - val_loss: 0.8987 - val_acc: 0.7409\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 1.1520 - acc: 0.6384 - val_loss: 0.8433 - val_acc: 0.7542\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 1.0883 - acc: 0.6584 - val_loss: 0.8011 - val_acc: 0.7662\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 1.0348 - acc: 0.6794 - val_loss: 0.7603 - val_acc: 0.7802\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.9996 - acc: 0.6882 - val_loss: 0.7250 - val_acc: 0.7906\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.9599 - acc: 0.7011 - val_loss: 0.6930 - val_acc: 0.7997\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.9276 - acc: 0.7142 - val_loss: 0.6756 - val_acc: 0.8046\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.9003 - acc: 0.7235 - val_loss: 0.6568 - val_acc: 0.8126\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.8731 - acc: 0.7297 - val_loss: 0.6414 - val_acc: 0.8120\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.8500 - acc: 0.7371 - val_loss: 0.6071 - val_acc: 0.8224\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.8285 - acc: 0.7450 - val_loss: 0.6027 - val_acc: 0.8218\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.8077 - acc: 0.7525 - val_loss: 0.5856 - val_acc: 0.8266\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.7898 - acc: 0.7586 - val_loss: 0.5676 - val_acc: 0.8359\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.7753 - acc: 0.7614 - val_loss: 0.5541 - val_acc: 0.8385\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.7553 - acc: 0.7690 - val_loss: 0.5395 - val_acc: 0.8443\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.7431 - acc: 0.7714 - val_loss: 0.5317 - val_acc: 0.8468\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.7284 - acc: 0.7770 - val_loss: 0.5145 - val_acc: 0.8501\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.7148 - acc: 0.7820 - val_loss: 0.5034 - val_acc: 0.8522\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.7032 - acc: 0.7823 - val_loss: 0.4983 - val_acc: 0.8543\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.6983 - acc: 0.7869 - val_loss: 0.4993 - val_acc: 0.8575\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 3s 64us/sample - loss: 0.6975 - acc: 0.7855 - val_loss: 0.4927 - val_acc: 0.8572\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.6770 - acc: 0.7912 - val_loss: 0.4734 - val_acc: 0.8612\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.6684 - acc: 0.7934 - val_loss: 0.4763 - val_acc: 0.8619\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6573 - acc: 0.7959 - val_loss: 0.4636 - val_acc: 0.8648\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.6506 - acc: 0.8003 - val_loss: 0.4603 - val_acc: 0.8676\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.6422 - acc: 0.8019 - val_loss: 0.4628 - val_acc: 0.8641\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.6420 - acc: 0.8039 - val_loss: 0.4411 - val_acc: 0.8706\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6350 - acc: 0.8025 - val_loss: 0.4442 - val_acc: 0.8710\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6293 - acc: 0.8062 - val_loss: 0.4422 - val_acc: 0.8717\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.6140 - acc: 0.8088 - val_loss: 0.4334 - val_acc: 0.8740\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.6162 - acc: 0.8099 - val_loss: 0.4328 - val_acc: 0.8743\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.6049 - acc: 0.8126 - val_loss: 0.4284 - val_acc: 0.8749\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5984 - acc: 0.8147 - val_loss: 0.4192 - val_acc: 0.8776\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5957 - acc: 0.8150 - val_loss: 0.4073 - val_acc: 0.8805\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5920 - acc: 0.8167 - val_loss: 0.4133 - val_acc: 0.8786\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5843 - acc: 0.8191 - val_loss: 0.4039 - val_acc: 0.8827\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5759 - acc: 0.8200 - val_loss: 0.4041 - val_acc: 0.8830\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5721 - acc: 0.8214 - val_loss: 0.3998 - val_acc: 0.8825\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.5693 - acc: 0.8230 - val_loss: 0.3925 - val_acc: 0.8846\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5721 - acc: 0.8241 - val_loss: 0.3961 - val_acc: 0.8834\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5623 - acc: 0.8252 - val_loss: 0.3835 - val_acc: 0.8892\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.5602 - acc: 0.8258 - val_loss: 0.3811 - val_acc: 0.8899\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5496 - acc: 0.8292 - val_loss: 0.3826 - val_acc: 0.8894\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5394 - acc: 0.8323 - val_loss: 0.3819 - val_acc: 0.8895\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5407 - acc: 0.8312 - val_loss: 0.3710 - val_acc: 0.8916\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5445 - acc: 0.8308 - val_loss: 0.3845 - val_acc: 0.8871\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5377 - acc: 0.8340 - val_loss: 0.3636 - val_acc: 0.8959\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5374 - acc: 0.8324 - val_loss: 0.3762 - val_acc: 0.8901\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5304 - acc: 0.8361 - val_loss: 0.3675 - val_acc: 0.8930\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5312 - acc: 0.8317 - val_loss: 0.3600 - val_acc: 0.8956\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5304 - acc: 0.8358 - val_loss: 0.3642 - val_acc: 0.8939\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.5249 - acc: 0.8363 - val_loss: 0.3562 - val_acc: 0.8961\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5179 - acc: 0.8395 - val_loss: 0.3534 - val_acc: 0.8961\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5161 - acc: 0.8393 - val_loss: 0.3581 - val_acc: 0.8949\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5178 - acc: 0.8392 - val_loss: 0.3495 - val_acc: 0.8994\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5144 - acc: 0.8393 - val_loss: 0.3472 - val_acc: 0.9003\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5020 - acc: 0.8431 - val_loss: 0.3513 - val_acc: 0.8992\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5009 - acc: 0.8426 - val_loss: 0.3358 - val_acc: 0.9038\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.5029 - acc: 0.8433 - val_loss: 0.3434 - val_acc: 0.8988\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.5008 - acc: 0.8425 - val_loss: 0.3377 - val_acc: 0.9018\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4967 - acc: 0.8452 - val_loss: 0.3468 - val_acc: 0.8996\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4939 - acc: 0.8471 - val_loss: 0.3301 - val_acc: 0.9043\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4910 - acc: 0.8468 - val_loss: 0.3334 - val_acc: 0.9042\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4864 - acc: 0.8462 - val_loss: 0.3345 - val_acc: 0.9021\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4877 - acc: 0.8484 - val_loss: 0.3346 - val_acc: 0.9020\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4816 - acc: 0.8491 - val_loss: 0.3329 - val_acc: 0.9027\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4821 - acc: 0.8479 - val_loss: 0.3294 - val_acc: 0.9041\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4795 - acc: 0.8506 - val_loss: 0.3257 - val_acc: 0.9044\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4804 - acc: 0.8496 - val_loss: 0.3292 - val_acc: 0.9050\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4754 - acc: 0.8513 - val_loss: 0.3239 - val_acc: 0.9048\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.4736 - acc: 0.8508 - val_loss: 0.3282 - val_acc: 0.9044\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4738 - acc: 0.8517 - val_loss: 0.3200 - val_acc: 0.9072\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4707 - acc: 0.8535 - val_loss: 0.3211 - val_acc: 0.9067\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4686 - acc: 0.8538 - val_loss: 0.3167 - val_acc: 0.9084\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4751 - acc: 0.8508 - val_loss: 0.3149 - val_acc: 0.9095\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4603 - acc: 0.8551 - val_loss: 0.3096 - val_acc: 0.9106\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4650 - acc: 0.8547 - val_loss: 0.3078 - val_acc: 0.9105\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4607 - acc: 0.8549 - val_loss: 0.3114 - val_acc: 0.9092\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4582 - acc: 0.8570 - val_loss: 0.3097 - val_acc: 0.9111\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4596 - acc: 0.8560 - val_loss: 0.3128 - val_acc: 0.9110\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4571 - acc: 0.8565 - val_loss: 0.3073 - val_acc: 0.9114\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4508 - acc: 0.8581 - val_loss: 0.3036 - val_acc: 0.9122\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4507 - acc: 0.8595 - val_loss: 0.3049 - val_acc: 0.9125\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4536 - acc: 0.8586 - val_loss: 0.3016 - val_acc: 0.9132\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4505 - acc: 0.8583 - val_loss: 0.3014 - val_acc: 0.9134\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4557 - acc: 0.8575 - val_loss: 0.3079 - val_acc: 0.9112\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4467 - acc: 0.8585 - val_loss: 0.3007 - val_acc: 0.9125\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4403 - acc: 0.8611 - val_loss: 0.2980 - val_acc: 0.9137\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4426 - acc: 0.8608 - val_loss: 0.2959 - val_acc: 0.9137\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4373 - acc: 0.8597 - val_loss: 0.2946 - val_acc: 0.9158\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4431 - acc: 0.8605 - val_loss: 0.2902 - val_acc: 0.9180\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4397 - acc: 0.8614 - val_loss: 0.2877 - val_acc: 0.9179\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4384 - acc: 0.8625 - val_loss: 0.2872 - val_acc: 0.9182\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4294 - acc: 0.8645 - val_loss: 0.2852 - val_acc: 0.9185\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4391 - acc: 0.8616 - val_loss: 0.2855 - val_acc: 0.9184\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.4304 - acc: 0.8643 - val_loss: 0.2926 - val_acc: 0.9168\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4330 - acc: 0.8631 - val_loss: 0.2869 - val_acc: 0.9177\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4324 - acc: 0.8651 - val_loss: 0.2848 - val_acc: 0.9187\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4278 - acc: 0.8652 - val_loss: 0.2880 - val_acc: 0.9171\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4321 - acc: 0.8662 - val_loss: 0.2822 - val_acc: 0.9198\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4248 - acc: 0.8662 - val_loss: 0.2870 - val_acc: 0.9175\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4328 - acc: 0.8639 - val_loss: 0.2801 - val_acc: 0.9190\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4201 - acc: 0.8686 - val_loss: 0.2800 - val_acc: 0.9202\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4245 - acc: 0.8682 - val_loss: 0.2761 - val_acc: 0.9211\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4168 - acc: 0.8684 - val_loss: 0.2793 - val_acc: 0.9194\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4181 - acc: 0.8675 - val_loss: 0.2773 - val_acc: 0.9207\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4192 - acc: 0.8680 - val_loss: 0.2737 - val_acc: 0.9219\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4189 - acc: 0.8697 - val_loss: 0.2725 - val_acc: 0.9227\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4181 - acc: 0.8691 - val_loss: 0.2773 - val_acc: 0.9215\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4193 - acc: 0.8690 - val_loss: 0.2751 - val_acc: 0.9207\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4189 - acc: 0.8705 - val_loss: 0.2717 - val_acc: 0.9226\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4116 - acc: 0.8696 - val_loss: 0.2691 - val_acc: 0.9238\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4141 - acc: 0.8705 - val_loss: 0.2659 - val_acc: 0.9249\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4087 - acc: 0.8719 - val_loss: 0.2653 - val_acc: 0.9252\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4083 - acc: 0.8716 - val_loss: 0.2695 - val_acc: 0.9227\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4061 - acc: 0.8719 - val_loss: 0.2710 - val_acc: 0.9229\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4093 - acc: 0.8727 - val_loss: 0.2732 - val_acc: 0.9215\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4071 - acc: 0.8705 - val_loss: 0.2627 - val_acc: 0.9255\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.4084 - acc: 0.8716 - val_loss: 0.2670 - val_acc: 0.9244\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3998 - acc: 0.8748 - val_loss: 0.2661 - val_acc: 0.9252\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.4012 - acc: 0.8731 - val_loss: 0.2614 - val_acc: 0.9255\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.4048 - acc: 0.8729 - val_loss: 0.2660 - val_acc: 0.9244\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3980 - acc: 0.8746 - val_loss: 0.2596 - val_acc: 0.9272\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4028 - acc: 0.8735 - val_loss: 0.2626 - val_acc: 0.9257\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3978 - acc: 0.8739 - val_loss: 0.2578 - val_acc: 0.9271\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.4012 - acc: 0.8743 - val_loss: 0.2610 - val_acc: 0.9261\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3950 - acc: 0.8765 - val_loss: 0.2602 - val_acc: 0.9270\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3952 - acc: 0.8751 - val_loss: 0.2551 - val_acc: 0.9284\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.4013 - acc: 0.8735 - val_loss: 0.2589 - val_acc: 0.9269\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3904 - acc: 0.8772 - val_loss: 0.2557 - val_acc: 0.9272\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3930 - acc: 0.8756 - val_loss: 0.2553 - val_acc: 0.9277\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3900 - acc: 0.8760 - val_loss: 0.2521 - val_acc: 0.9287\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3926 - acc: 0.8768 - val_loss: 0.2542 - val_acc: 0.9289\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3920 - acc: 0.8746 - val_loss: 0.2513 - val_acc: 0.9295\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3948 - acc: 0.8754 - val_loss: 0.2535 - val_acc: 0.9284\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3853 - acc: 0.8798 - val_loss: 0.2509 - val_acc: 0.9282\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3883 - acc: 0.8765 - val_loss: 0.2571 - val_acc: 0.9270\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3891 - acc: 0.8768 - val_loss: 0.2506 - val_acc: 0.9296\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3920 - acc: 0.8767 - val_loss: 0.2565 - val_acc: 0.9267\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3850 - acc: 0.8782 - val_loss: 0.2531 - val_acc: 0.9287\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3858 - acc: 0.8786 - val_loss: 0.2544 - val_acc: 0.9285\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3848 - acc: 0.8782 - val_loss: 0.2522 - val_acc: 0.9283\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3809 - acc: 0.8800 - val_loss: 0.2502 - val_acc: 0.9298\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3744 - acc: 0.8814 - val_loss: 0.2500 - val_acc: 0.9291\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3891 - acc: 0.8756 - val_loss: 0.2504 - val_acc: 0.9291\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3814 - acc: 0.8789 - val_loss: 0.2568 - val_acc: 0.9276\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3797 - acc: 0.8797 - val_loss: 0.2476 - val_acc: 0.9300\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3789 - acc: 0.8802 - val_loss: 0.2486 - val_acc: 0.9302\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3766 - acc: 0.8816 - val_loss: 0.2501 - val_acc: 0.9297\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3771 - acc: 0.8811 - val_loss: 0.2402 - val_acc: 0.9334\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3795 - acc: 0.8810 - val_loss: 0.2452 - val_acc: 0.9318\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3778 - acc: 0.8812 - val_loss: 0.2440 - val_acc: 0.9323\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3746 - acc: 0.8809 - val_loss: 0.2404 - val_acc: 0.9322\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3762 - acc: 0.8807 - val_loss: 0.2424 - val_acc: 0.9325\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3716 - acc: 0.8827 - val_loss: 0.2466 - val_acc: 0.9298\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3701 - acc: 0.8835 - val_loss: 0.2414 - val_acc: 0.9318\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3782 - acc: 0.8794 - val_loss: 0.2400 - val_acc: 0.9326\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3687 - acc: 0.8827 - val_loss: 0.2371 - val_acc: 0.9334\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3749 - acc: 0.8817 - val_loss: 0.2390 - val_acc: 0.9327\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3681 - acc: 0.8824 - val_loss: 0.2374 - val_acc: 0.9336\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3691 - acc: 0.8828 - val_loss: 0.2413 - val_acc: 0.9320\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3662 - acc: 0.8847 - val_loss: 0.2373 - val_acc: 0.9338\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3696 - acc: 0.8818 - val_loss: 0.2413 - val_acc: 0.9319\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3666 - acc: 0.8820 - val_loss: 0.2425 - val_acc: 0.9312\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3666 - acc: 0.8839 - val_loss: 0.2363 - val_acc: 0.9338\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3705 - acc: 0.8827 - val_loss: 0.2414 - val_acc: 0.9323\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3691 - acc: 0.8811 - val_loss: 0.2362 - val_acc: 0.9344\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3660 - acc: 0.8851 - val_loss: 0.2347 - val_acc: 0.9345\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3617 - acc: 0.8855 - val_loss: 0.2396 - val_acc: 0.9330\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3677 - acc: 0.8834 - val_loss: 0.2324 - val_acc: 0.9354\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3636 - acc: 0.8845 - val_loss: 0.2413 - val_acc: 0.9328\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3693 - acc: 0.8817 - val_loss: 0.2396 - val_acc: 0.9325\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3641 - acc: 0.8852 - val_loss: 0.2363 - val_acc: 0.9335\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3664 - acc: 0.8846 - val_loss: 0.2285 - val_acc: 0.9365\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3591 - acc: 0.8858 - val_loss: 0.2351 - val_acc: 0.9340\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3607 - acc: 0.8857 - val_loss: 0.2312 - val_acc: 0.9355\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3588 - acc: 0.8860 - val_loss: 0.2329 - val_acc: 0.9351\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3634 - acc: 0.8851 - val_loss: 0.2301 - val_acc: 0.9355\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3581 - acc: 0.8851 - val_loss: 0.2342 - val_acc: 0.9341\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3526 - acc: 0.8887 - val_loss: 0.2292 - val_acc: 0.9363\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3547 - acc: 0.8865 - val_loss: 0.2263 - val_acc: 0.9373\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3571 - acc: 0.8867 - val_loss: 0.2267 - val_acc: 0.9363\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3547 - acc: 0.8856 - val_loss: 0.2262 - val_acc: 0.9370\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3565 - acc: 0.8862 - val_loss: 0.2276 - val_acc: 0.9374\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3564 - acc: 0.8861 - val_loss: 0.2274 - val_acc: 0.9370\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3544 - acc: 0.8877 - val_loss: 0.2223 - val_acc: 0.9386\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3491 - acc: 0.8893 - val_loss: 0.2236 - val_acc: 0.9377\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3536 - acc: 0.8870 - val_loss: 0.2292 - val_acc: 0.9363\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3508 - acc: 0.8894 - val_loss: 0.2256 - val_acc: 0.9372\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3584 - acc: 0.8869 - val_loss: 0.2278 - val_acc: 0.9373\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3485 - acc: 0.8885 - val_loss: 0.2235 - val_acc: 0.9384\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3466 - acc: 0.8888 - val_loss: 0.2243 - val_acc: 0.9374\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3519 - acc: 0.8906 - val_loss: 0.2243 - val_acc: 0.9376\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3491 - acc: 0.8894 - val_loss: 0.2257 - val_acc: 0.9375\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3497 - acc: 0.8894 - val_loss: 0.2215 - val_acc: 0.9386\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3473 - acc: 0.8878 - val_loss: 0.2172 - val_acc: 0.9401\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3460 - acc: 0.8906 - val_loss: 0.2234 - val_acc: 0.9387\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3455 - acc: 0.8900 - val_loss: 0.2223 - val_acc: 0.9385\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3484 - acc: 0.8912 - val_loss: 0.2227 - val_acc: 0.9387\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3480 - acc: 0.8898 - val_loss: 0.2235 - val_acc: 0.9384\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3459 - acc: 0.8905 - val_loss: 0.2176 - val_acc: 0.9403\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3503 - acc: 0.8894 - val_loss: 0.2253 - val_acc: 0.9373\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3506 - acc: 0.8885 - val_loss: 0.2227 - val_acc: 0.9393\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3417 - acc: 0.8925 - val_loss: 0.2213 - val_acc: 0.9387\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3486 - acc: 0.8881 - val_loss: 0.2210 - val_acc: 0.9392\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3445 - acc: 0.8902 - val_loss: 0.2223 - val_acc: 0.9387\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3392 - acc: 0.8921 - val_loss: 0.2219 - val_acc: 0.9389\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3439 - acc: 0.8928 - val_loss: 0.2213 - val_acc: 0.9386\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3393 - acc: 0.8917 - val_loss: 0.2179 - val_acc: 0.9392\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3478 - acc: 0.8904 - val_loss: 0.2192 - val_acc: 0.9393\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3431 - acc: 0.8907 - val_loss: 0.2166 - val_acc: 0.9402\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3397 - acc: 0.8937 - val_loss: 0.2174 - val_acc: 0.9402\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3405 - acc: 0.8921 - val_loss: 0.2142 - val_acc: 0.9411\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3406 - acc: 0.8926 - val_loss: 0.2187 - val_acc: 0.9398\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3425 - acc: 0.8922 - val_loss: 0.2191 - val_acc: 0.9394\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3392 - acc: 0.8932 - val_loss: 0.2158 - val_acc: 0.9407\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 2s 60us/sample - loss: 0.3452 - acc: 0.8915 - val_loss: 0.2193 - val_acc: 0.9399\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3426 - acc: 0.8916 - val_loss: 0.2173 - val_acc: 0.9409\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3332 - acc: 0.8947 - val_loss: 0.2193 - val_acc: 0.9390\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3397 - acc: 0.8917 - val_loss: 0.2170 - val_acc: 0.9400\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3355 - acc: 0.8946 - val_loss: 0.2180 - val_acc: 0.9401\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3362 - acc: 0.8941 - val_loss: 0.2147 - val_acc: 0.9406\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3367 - acc: 0.8941 - val_loss: 0.2187 - val_acc: 0.9391\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3341 - acc: 0.8929 - val_loss: 0.2131 - val_acc: 0.9412\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3374 - acc: 0.8930 - val_loss: 0.2194 - val_acc: 0.9386\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3409 - acc: 0.8912 - val_loss: 0.2239 - val_acc: 0.9375\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3385 - acc: 0.8924 - val_loss: 0.2161 - val_acc: 0.9404\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3388 - acc: 0.8909 - val_loss: 0.2112 - val_acc: 0.9424\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3350 - acc: 0.8943 - val_loss: 0.2154 - val_acc: 0.9406\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3322 - acc: 0.8945 - val_loss: 0.2131 - val_acc: 0.9411\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3292 - acc: 0.8940 - val_loss: 0.2131 - val_acc: 0.9413\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3335 - acc: 0.8948 - val_loss: 0.2207 - val_acc: 0.9393\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3341 - acc: 0.8927 - val_loss: 0.2130 - val_acc: 0.9414\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3381 - acc: 0.8926 - val_loss: 0.2088 - val_acc: 0.9426\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3320 - acc: 0.8941 - val_loss: 0.2126 - val_acc: 0.9420\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3385 - acc: 0.8938 - val_loss: 0.2124 - val_acc: 0.9423\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3335 - acc: 0.8928 - val_loss: 0.2111 - val_acc: 0.9418\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3270 - acc: 0.8969 - val_loss: 0.2110 - val_acc: 0.9428\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3289 - acc: 0.8952 - val_loss: 0.2137 - val_acc: 0.9407\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3360 - acc: 0.8935 - val_loss: 0.2113 - val_acc: 0.9418\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3347 - acc: 0.8937 - val_loss: 0.2102 - val_acc: 0.9427\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.3264 - acc: 0.8966 - val_loss: 0.2088 - val_acc: 0.9426\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3356 - acc: 0.8912 - val_loss: 0.2091 - val_acc: 0.9429\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3250 - acc: 0.8949 - val_loss: 0.2100 - val_acc: 0.9426\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3338 - acc: 0.8926 - val_loss: 0.2114 - val_acc: 0.9424\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3285 - acc: 0.8954 - val_loss: 0.2055 - val_acc: 0.9441\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3259 - acc: 0.8965 - val_loss: 0.2158 - val_acc: 0.9406\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3295 - acc: 0.8947 - val_loss: 0.2111 - val_acc: 0.9432\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3307 - acc: 0.8948 - val_loss: 0.2098 - val_acc: 0.9434\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3220 - acc: 0.8970 - val_loss: 0.2083 - val_acc: 0.9433\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3235 - acc: 0.8969 - val_loss: 0.2126 - val_acc: 0.9412\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3292 - acc: 0.8957 - val_loss: 0.2096 - val_acc: 0.9427\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3251 - acc: 0.8968 - val_loss: 0.2098 - val_acc: 0.9433\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 2s 60us/sample - loss: 0.3270 - acc: 0.8946 - val_loss: 0.2106 - val_acc: 0.9424\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3298 - acc: 0.8957 - val_loss: 0.2057 - val_acc: 0.9442\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3209 - acc: 0.8989 - val_loss: 0.2082 - val_acc: 0.9433\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3286 - acc: 0.8946 - val_loss: 0.2047 - val_acc: 0.9451\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3260 - acc: 0.8961 - val_loss: 0.2101 - val_acc: 0.9438\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3223 - acc: 0.8970 - val_loss: 0.2087 - val_acc: 0.9432\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3244 - acc: 0.8976 - val_loss: 0.2072 - val_acc: 0.9442\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3236 - acc: 0.8962 - val_loss: 0.2066 - val_acc: 0.9440\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3186 - acc: 0.8983 - val_loss: 0.2063 - val_acc: 0.9444\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3226 - acc: 0.8964 - val_loss: 0.2018 - val_acc: 0.9455\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3213 - acc: 0.8972 - val_loss: 0.2086 - val_acc: 0.9433\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3216 - acc: 0.8985 - val_loss: 0.2043 - val_acc: 0.9444\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3180 - acc: 0.8980 - val_loss: 0.2052 - val_acc: 0.9445\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3164 - acc: 0.8993 - val_loss: 0.2066 - val_acc: 0.9438\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3197 - acc: 0.8978 - val_loss: 0.2052 - val_acc: 0.9434\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 3s 66us/sample - loss: 0.3241 - acc: 0.8964 - val_loss: 0.2026 - val_acc: 0.9445\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3184 - acc: 0.8991 - val_loss: 0.2023 - val_acc: 0.9453\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3208 - acc: 0.8970 - val_loss: 0.2040 - val_acc: 0.9449\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3174 - acc: 0.8993 - val_loss: 0.2028 - val_acc: 0.9451\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3149 - acc: 0.9000 - val_loss: 0.2021 - val_acc: 0.9455\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3173 - acc: 0.8991 - val_loss: 0.2014 - val_acc: 0.9453\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3207 - acc: 0.8993 - val_loss: 0.2042 - val_acc: 0.9450\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3152 - acc: 0.8989 - val_loss: 0.2005 - val_acc: 0.9463\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3175 - acc: 0.8995 - val_loss: 0.2026 - val_acc: 0.9455\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3176 - acc: 0.8987 - val_loss: 0.1997 - val_acc: 0.9463\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3149 - acc: 0.9004 - val_loss: 0.2047 - val_acc: 0.9445\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3213 - acc: 0.8966 - val_loss: 0.2042 - val_acc: 0.9441\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3152 - acc: 0.8998 - val_loss: 0.2123 - val_acc: 0.9410\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3200 - acc: 0.8990 - val_loss: 0.2011 - val_acc: 0.9451\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3153 - acc: 0.9004 - val_loss: 0.2021 - val_acc: 0.9452\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3140 - acc: 0.8988 - val_loss: 0.2013 - val_acc: 0.9459\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3104 - acc: 0.9011 - val_loss: 0.2012 - val_acc: 0.9457\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3118 - acc: 0.9004 - val_loss: 0.1977 - val_acc: 0.9466\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3145 - acc: 0.8995 - val_loss: 0.2020 - val_acc: 0.9445\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3149 - acc: 0.9003 - val_loss: 0.2033 - val_acc: 0.9441\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3135 - acc: 0.8987 - val_loss: 0.2025 - val_acc: 0.9459\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3142 - acc: 0.8998 - val_loss: 0.2011 - val_acc: 0.9458\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3042 - acc: 0.9039 - val_loss: 0.2015 - val_acc: 0.9448\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3127 - acc: 0.9002 - val_loss: 0.2047 - val_acc: 0.9439\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3191 - acc: 0.8978 - val_loss: 0.2024 - val_acc: 0.9456\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3141 - acc: 0.9005 - val_loss: 0.1988 - val_acc: 0.9463\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3161 - acc: 0.9005 - val_loss: 0.2007 - val_acc: 0.9458\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3111 - acc: 0.9018 - val_loss: 0.1997 - val_acc: 0.9464\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3107 - acc: 0.9007 - val_loss: 0.1977 - val_acc: 0.9475\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3073 - acc: 0.9012 - val_loss: 0.1948 - val_acc: 0.9476\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3159 - acc: 0.8999 - val_loss: 0.1984 - val_acc: 0.9465\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3084 - acc: 0.9039 - val_loss: 0.1965 - val_acc: 0.9473\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3106 - acc: 0.9006 - val_loss: 0.1985 - val_acc: 0.9466\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3123 - acc: 0.9007 - val_loss: 0.2004 - val_acc: 0.9456\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3098 - acc: 0.9014 - val_loss: 0.1969 - val_acc: 0.9467\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3067 - acc: 0.9023 - val_loss: 0.1963 - val_acc: 0.9466\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3104 - acc: 0.9015 - val_loss: 0.2007 - val_acc: 0.9457\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3172 - acc: 0.8983 - val_loss: 0.1957 - val_acc: 0.9470\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3036 - acc: 0.9030 - val_loss: 0.1999 - val_acc: 0.9455\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3054 - acc: 0.9030 - val_loss: 0.1944 - val_acc: 0.9481\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3083 - acc: 0.9028 - val_loss: 0.1974 - val_acc: 0.9465\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3090 - acc: 0.9017 - val_loss: 0.1989 - val_acc: 0.9459\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3112 - acc: 0.9013 - val_loss: 0.1942 - val_acc: 0.9483\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3085 - acc: 0.9021 - val_loss: 0.1964 - val_acc: 0.9474\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3039 - acc: 0.9032 - val_loss: 0.1944 - val_acc: 0.9473\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3139 - acc: 0.9003 - val_loss: 0.1948 - val_acc: 0.9482\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3100 - acc: 0.9001 - val_loss: 0.1972 - val_acc: 0.9472\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2992 - acc: 0.9048 - val_loss: 0.1961 - val_acc: 0.9473\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3041 - acc: 0.9023 - val_loss: 0.1963 - val_acc: 0.9468\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3053 - acc: 0.9044 - val_loss: 0.1993 - val_acc: 0.9456\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3155 - acc: 0.8997 - val_loss: 0.1980 - val_acc: 0.9464\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3001 - acc: 0.9045 - val_loss: 0.1974 - val_acc: 0.9473\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3042 - acc: 0.9039 - val_loss: 0.1964 - val_acc: 0.9473\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3025 - acc: 0.9035 - val_loss: 0.1937 - val_acc: 0.9484\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3070 - acc: 0.9027 - val_loss: 0.1949 - val_acc: 0.9480\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3089 - acc: 0.9006 - val_loss: 0.1980 - val_acc: 0.9466\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3123 - acc: 0.9003 - val_loss: 0.1949 - val_acc: 0.9478\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3015 - acc: 0.9036 - val_loss: 0.1913 - val_acc: 0.9490\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3094 - acc: 0.9008 - val_loss: 0.1943 - val_acc: 0.9476\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3022 - acc: 0.9039 - val_loss: 0.1934 - val_acc: 0.9483\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3045 - acc: 0.9024 - val_loss: 0.1939 - val_acc: 0.9482\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3059 - acc: 0.9038 - val_loss: 0.1948 - val_acc: 0.9476\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3074 - acc: 0.9029 - val_loss: 0.1923 - val_acc: 0.9487\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3058 - acc: 0.9027 - val_loss: 0.1976 - val_acc: 0.9470\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2983 - acc: 0.9043 - val_loss: 0.1942 - val_acc: 0.9482\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3039 - acc: 0.9034 - val_loss: 0.1917 - val_acc: 0.9492\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3039 - acc: 0.9041 - val_loss: 0.1955 - val_acc: 0.9479\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3046 - acc: 0.9034 - val_loss: 0.1945 - val_acc: 0.9478\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.3080 - acc: 0.8996 - val_loss: 0.1938 - val_acc: 0.9477\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2993 - acc: 0.9045 - val_loss: 0.1951 - val_acc: 0.9482\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3026 - acc: 0.9035 - val_loss: 0.1967 - val_acc: 0.9474\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2987 - acc: 0.9049 - val_loss: 0.1894 - val_acc: 0.9498\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3009 - acc: 0.9033 - val_loss: 0.1919 - val_acc: 0.9493\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.3075 - acc: 0.9020 - val_loss: 0.1945 - val_acc: 0.9475\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3074 - acc: 0.9017 - val_loss: 0.1932 - val_acc: 0.9485\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3022 - acc: 0.9039 - val_loss: 0.1894 - val_acc: 0.9498\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2955 - acc: 0.9068 - val_loss: 0.1904 - val_acc: 0.9500\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2969 - acc: 0.9058 - val_loss: 0.1916 - val_acc: 0.9493\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3026 - acc: 0.9040 - val_loss: 0.1895 - val_acc: 0.9499\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3038 - acc: 0.9020 - val_loss: 0.1928 - val_acc: 0.9481\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2983 - acc: 0.9044 - val_loss: 0.1896 - val_acc: 0.9491\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2952 - acc: 0.9070 - val_loss: 0.1936 - val_acc: 0.9488\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2951 - acc: 0.9052 - val_loss: 0.1954 - val_acc: 0.9481\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3000 - acc: 0.9059 - val_loss: 0.1915 - val_acc: 0.9487\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2993 - acc: 0.9053 - val_loss: 0.1915 - val_acc: 0.9493\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2986 - acc: 0.9059 - val_loss: 0.1919 - val_acc: 0.9487\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2986 - acc: 0.9050 - val_loss: 0.1878 - val_acc: 0.9503\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2919 - acc: 0.9073 - val_loss: 0.1897 - val_acc: 0.9493\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.3003 - acc: 0.9047 - val_loss: 0.1929 - val_acc: 0.9485\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2996 - acc: 0.9053 - val_loss: 0.1903 - val_acc: 0.9494\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2951 - acc: 0.9052 - val_loss: 0.1940 - val_acc: 0.9480\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2942 - acc: 0.9074 - val_loss: 0.1934 - val_acc: 0.9488\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2957 - acc: 0.9074 - val_loss: 0.1891 - val_acc: 0.9500\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2951 - acc: 0.9057 - val_loss: 0.1923 - val_acc: 0.9488\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2951 - acc: 0.9072 - val_loss: 0.1896 - val_acc: 0.9504\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2992 - acc: 0.9049 - val_loss: 0.1907 - val_acc: 0.9493\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2972 - acc: 0.9050 - val_loss: 0.1934 - val_acc: 0.9486\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2981 - acc: 0.9044 - val_loss: 0.1850 - val_acc: 0.9515\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2948 - acc: 0.9066 - val_loss: 0.1891 - val_acc: 0.9496\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2958 - acc: 0.9066 - val_loss: 0.1873 - val_acc: 0.9498\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2975 - acc: 0.9052 - val_loss: 0.1886 - val_acc: 0.9505\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2879 - acc: 0.9069 - val_loss: 0.1878 - val_acc: 0.9505\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2986 - acc: 0.9054 - val_loss: 0.1899 - val_acc: 0.9488\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2955 - acc: 0.9055 - val_loss: 0.1890 - val_acc: 0.9505\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2983 - acc: 0.9052 - val_loss: 0.1884 - val_acc: 0.9504\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2918 - acc: 0.9072 - val_loss: 0.1872 - val_acc: 0.9506\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.2919 - acc: 0.9048 - val_loss: 0.1917 - val_acc: 0.9487\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2903 - acc: 0.9073 - val_loss: 0.1917 - val_acc: 0.9488\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2919 - acc: 0.9065 - val_loss: 0.1893 - val_acc: 0.9495\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2940 - acc: 0.9047 - val_loss: 0.1859 - val_acc: 0.9511\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2966 - acc: 0.9057 - val_loss: 0.1883 - val_acc: 0.9503\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2905 - acc: 0.9063 - val_loss: 0.1944 - val_acc: 0.9478\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2955 - acc: 0.9055 - val_loss: 0.1891 - val_acc: 0.9497\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.3007 - acc: 0.9041 - val_loss: 0.1907 - val_acc: 0.9493\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2934 - acc: 0.9073 - val_loss: 0.1893 - val_acc: 0.9495\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2969 - acc: 0.9057 - val_loss: 0.1899 - val_acc: 0.9493\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2947 - acc: 0.9060 - val_loss: 0.1856 - val_acc: 0.9518\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2932 - acc: 0.9065 - val_loss: 0.1871 - val_acc: 0.9506\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2921 - acc: 0.9059 - val_loss: 0.1842 - val_acc: 0.9515\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2860 - acc: 0.9094 - val_loss: 0.1872 - val_acc: 0.9506\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2934 - acc: 0.9050 - val_loss: 0.1882 - val_acc: 0.9501\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2904 - acc: 0.9080 - val_loss: 0.1872 - val_acc: 0.9510\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2941 - acc: 0.9071 - val_loss: 0.1903 - val_acc: 0.9495\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2876 - acc: 0.9074 - val_loss: 0.1876 - val_acc: 0.9506\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2901 - acc: 0.9076 - val_loss: 0.1884 - val_acc: 0.9505\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2921 - acc: 0.9070 - val_loss: 0.1919 - val_acc: 0.9484\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2866 - acc: 0.9074 - val_loss: 0.1858 - val_acc: 0.9507\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2890 - acc: 0.9086 - val_loss: 0.1851 - val_acc: 0.9508\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2858 - acc: 0.9083 - val_loss: 0.1859 - val_acc: 0.9510\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2877 - acc: 0.9078 - val_loss: 0.1871 - val_acc: 0.9500\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2864 - acc: 0.9100 - val_loss: 0.1869 - val_acc: 0.9503\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2863 - acc: 0.9078 - val_loss: 0.1860 - val_acc: 0.9509\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2854 - acc: 0.9074 - val_loss: 0.1858 - val_acc: 0.9507\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2890 - acc: 0.9093 - val_loss: 0.1852 - val_acc: 0.9514\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2919 - acc: 0.9062 - val_loss: 0.1852 - val_acc: 0.9511\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2901 - acc: 0.9077 - val_loss: 0.1846 - val_acc: 0.9518\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2858 - acc: 0.9100 - val_loss: 0.1868 - val_acc: 0.9505\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2885 - acc: 0.9089 - val_loss: 0.1888 - val_acc: 0.9498\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2887 - acc: 0.9075 - val_loss: 0.1840 - val_acc: 0.9520\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2876 - acc: 0.9090 - val_loss: 0.1855 - val_acc: 0.9513\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2926 - acc: 0.9077 - val_loss: 0.1883 - val_acc: 0.9503\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2888 - acc: 0.9084 - val_loss: 0.1859 - val_acc: 0.9508\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2851 - acc: 0.9091 - val_loss: 0.1835 - val_acc: 0.9522\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2864 - acc: 0.9081 - val_loss: 0.1824 - val_acc: 0.9527\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2869 - acc: 0.9092 - val_loss: 0.1825 - val_acc: 0.9523\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.2846 - acc: 0.9076 - val_loss: 0.1856 - val_acc: 0.9509\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2856 - acc: 0.9093 - val_loss: 0.1869 - val_acc: 0.9505\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2809 - acc: 0.9096 - val_loss: 0.1848 - val_acc: 0.9504\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2825 - acc: 0.9091 - val_loss: 0.1833 - val_acc: 0.9511\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2886 - acc: 0.9090 - val_loss: 0.1836 - val_acc: 0.9512\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2854 - acc: 0.9104 - val_loss: 0.1863 - val_acc: 0.9505\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2875 - acc: 0.9103 - val_loss: 0.1871 - val_acc: 0.9504\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2817 - acc: 0.9106 - val_loss: 0.1841 - val_acc: 0.9513\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2821 - acc: 0.9091 - val_loss: 0.1835 - val_acc: 0.9520\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2782 - acc: 0.9125 - val_loss: 0.1877 - val_acc: 0.9506\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2853 - acc: 0.9092 - val_loss: 0.1820 - val_acc: 0.9528\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2846 - acc: 0.9096 - val_loss: 0.1844 - val_acc: 0.9517\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2854 - acc: 0.9086 - val_loss: 0.1811 - val_acc: 0.9529\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2891 - acc: 0.9096 - val_loss: 0.1839 - val_acc: 0.9515\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2834 - acc: 0.9083 - val_loss: 0.1810 - val_acc: 0.9530\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2824 - acc: 0.9093 - val_loss: 0.1890 - val_acc: 0.9501\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2880 - acc: 0.9077 - val_loss: 0.1811 - val_acc: 0.9524\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2847 - acc: 0.9080 - val_loss: 0.1815 - val_acc: 0.9528\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2741 - acc: 0.9133 - val_loss: 0.1842 - val_acc: 0.9515\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2882 - acc: 0.9092 - val_loss: 0.1859 - val_acc: 0.9511\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2831 - acc: 0.9108 - val_loss: 0.1878 - val_acc: 0.9508\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2856 - acc: 0.9098 - val_loss: 0.1852 - val_acc: 0.9508\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2886 - acc: 0.9089 - val_loss: 0.1840 - val_acc: 0.9521\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2894 - acc: 0.9071 - val_loss: 0.1844 - val_acc: 0.9519\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2805 - acc: 0.9085 - val_loss: 0.1813 - val_acc: 0.9527\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2833 - acc: 0.9095 - val_loss: 0.1846 - val_acc: 0.9517\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2807 - acc: 0.9086 - val_loss: 0.1805 - val_acc: 0.9526\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2814 - acc: 0.9108 - val_loss: 0.1838 - val_acc: 0.9516\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 2s 55us/sample - loss: 0.2834 - acc: 0.9098 - val_loss: 0.1809 - val_acc: 0.9524\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2834 - acc: 0.9087 - val_loss: 0.1842 - val_acc: 0.9514\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2824 - acc: 0.9092 - val_loss: 0.1878 - val_acc: 0.9504\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2839 - acc: 0.9096 - val_loss: 0.1819 - val_acc: 0.9532\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2794 - acc: 0.9105 - val_loss: 0.1812 - val_acc: 0.9534\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2780 - acc: 0.9098 - val_loss: 0.1813 - val_acc: 0.9528\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2795 - acc: 0.9134 - val_loss: 0.1842 - val_acc: 0.9515\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2835 - acc: 0.9095 - val_loss: 0.1825 - val_acc: 0.9522\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2779 - acc: 0.9125 - val_loss: 0.1820 - val_acc: 0.9521\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2791 - acc: 0.9108 - val_loss: 0.1801 - val_acc: 0.9531\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2805 - acc: 0.9099 - val_loss: 0.1803 - val_acc: 0.9528\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2799 - acc: 0.9116 - val_loss: 0.1789 - val_acc: 0.9541\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2811 - acc: 0.9116 - val_loss: 0.1800 - val_acc: 0.9533\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2813 - acc: 0.9091 - val_loss: 0.1795 - val_acc: 0.9535\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2766 - acc: 0.9120 - val_loss: 0.1813 - val_acc: 0.9529\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2719 - acc: 0.9131 - val_loss: 0.1813 - val_acc: 0.9527\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2768 - acc: 0.9108 - val_loss: 0.1838 - val_acc: 0.9519\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2794 - acc: 0.9121 - val_loss: 0.1843 - val_acc: 0.9524\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2817 - acc: 0.9102 - val_loss: 0.1828 - val_acc: 0.9523\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2766 - acc: 0.9127 - val_loss: 0.1785 - val_acc: 0.9544\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2727 - acc: 0.9142 - val_loss: 0.1785 - val_acc: 0.9542\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2776 - acc: 0.9116 - val_loss: 0.1794 - val_acc: 0.9535\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2806 - acc: 0.9103 - val_loss: 0.1783 - val_acc: 0.9537\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2764 - acc: 0.9116 - val_loss: 0.1805 - val_acc: 0.9531\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2844 - acc: 0.9096 - val_loss: 0.1810 - val_acc: 0.9532\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2760 - acc: 0.9107 - val_loss: 0.1843 - val_acc: 0.9517\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2799 - acc: 0.9109 - val_loss: 0.1817 - val_acc: 0.9524\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2802 - acc: 0.9101 - val_loss: 0.1805 - val_acc: 0.9527\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2792 - acc: 0.9124 - val_loss: 0.1810 - val_acc: 0.9527\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2733 - acc: 0.9122 - val_loss: 0.1797 - val_acc: 0.9532\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2772 - acc: 0.9109 - val_loss: 0.1804 - val_acc: 0.9536\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2757 - acc: 0.9122 - val_loss: 0.1796 - val_acc: 0.9536\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2840 - acc: 0.9091 - val_loss: 0.1800 - val_acc: 0.9538\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 3s 60us/sample - loss: 0.2781 - acc: 0.9108 - val_loss: 0.1791 - val_acc: 0.9537\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2756 - acc: 0.9127 - val_loss: 0.1800 - val_acc: 0.9539\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2780 - acc: 0.9115 - val_loss: 0.1817 - val_acc: 0.9529\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2798 - acc: 0.9111 - val_loss: 0.1786 - val_acc: 0.9538\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2731 - acc: 0.9128 - val_loss: 0.1789 - val_acc: 0.9531\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2770 - acc: 0.9121 - val_loss: 0.1824 - val_acc: 0.9523\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2781 - acc: 0.9112 - val_loss: 0.1821 - val_acc: 0.9526\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2770 - acc: 0.9118 - val_loss: 0.1789 - val_acc: 0.9537\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2730 - acc: 0.9140 - val_loss: 0.1808 - val_acc: 0.9528\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2767 - acc: 0.9113 - val_loss: 0.1835 - val_acc: 0.9515\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2796 - acc: 0.9117 - val_loss: 0.1813 - val_acc: 0.9528\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2739 - acc: 0.9140 - val_loss: 0.1787 - val_acc: 0.9539\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2722 - acc: 0.9126 - val_loss: 0.1823 - val_acc: 0.9526\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2710 - acc: 0.9143 - val_loss: 0.1786 - val_acc: 0.9539\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2768 - acc: 0.9133 - val_loss: 0.1794 - val_acc: 0.9534\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 2s 59us/sample - loss: 0.2765 - acc: 0.9126 - val_loss: 0.1791 - val_acc: 0.9537\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2805 - acc: 0.9107 - val_loss: 0.1765 - val_acc: 0.9544\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2810 - acc: 0.9098 - val_loss: 0.1761 - val_acc: 0.9537\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2724 - acc: 0.9130 - val_loss: 0.1760 - val_acc: 0.9546\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 2s 57us/sample - loss: 0.2762 - acc: 0.9136 - val_loss: 0.1745 - val_acc: 0.9546\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 2s 56us/sample - loss: 0.2727 - acc: 0.9138 - val_loss: 0.1781 - val_acc: 0.9542\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 2s 58us/sample - loss: 0.2726 - acc: 0.9140 - val_loss: 0.1776 - val_acc: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f13b0197b70>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "Learning_rate=0.0001750664096246245\n",
    "neurons=[350,250,150,10]\n",
    "Regularize=4.148615142025894e-07\n",
    "Activation=['relu','relu','relu','softmax']\n",
    "do=0.38286522356959884\n",
    "Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=False, patience=5)\n",
    "\n",
    "model_7=tf.keras.models.Sequential()\n",
    "model_7.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "for i in range(0,len(neurons)):\n",
    "  model_7.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "  if i != (len(neurons)-1):\n",
    "    model_7.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "model_7.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model_7.fit(nfX_train,HY_train,validation_data=(nfX_val,HY_val),batch_size=128,epochs=500,verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_z0WkXvpqgY"
   },
   "source": [
    "This model accuracy is less than previous one so lets drop it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUC3UQy2q9id"
   },
   "source": [
    "# The number of samples in validation set is greater than the number of samples in the training set. Which makes less sense. So lets combine the train and Validation dataset and split it in 70:30 proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4G84qyaOrn7"
   },
   "outputs": [],
   "source": [
    "XX=np.append(X_train,X_val,axis=0)\n",
    "HY=np.append(HY_train,HY_val,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J2PTw5iPVtY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XX_train,XX_val,HHy_train,HHy_val=train_test_split(XX,HY,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "VJIg8Z3UPZul",
    "outputId": "9688b631-cfd9-4da3-b145-51357b5cd0d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set after flattening (71400, 1024)\n",
      "number of samples in train set 71400\n",
      "Size of a image in train set (32, 32)\n",
      "Size of test set after flattening (18000, 1024)\n",
      "number of samples in test set 18000\n",
      "Size of a image in test set (32, 32)\n",
      "Size of Validaiton set after flattening (30600, 1024)\n",
      "number of samples in Validation set 30600\n",
      "Size of a image in Validation set (32, 32)\n"
     ]
    }
   ],
   "source": [
    "fnX_train=XX_train.reshape(XX_train.shape[0],1024)\n",
    "fnX_test=X_test.reshape(X_test.shape[0],1024)\n",
    "fnX_val=XX_val.reshape(XX_val.shape[0],1024)\n",
    "\n",
    "nnfX_train=fnX_train/255.0\n",
    "nnfX_test=fnX_test/255.0\n",
    "nnfX_val=fnX_val/255.0\n",
    "\n",
    "print('Size of train set after flattening',nnfX_train.shape)\n",
    "print('number of samples in train set',XX_train.shape[0])\n",
    "print('Size of a image in train set',XX_train.shape[1:])\n",
    "\n",
    "print('Size of test set after flattening',nnfX_test.shape)\n",
    "print('number of samples in test set',X_test.shape[0])\n",
    "print('Size of a image in test set',X_test.shape[1:])\n",
    "\n",
    "print('Size of Validaiton set after flattening',nnfX_val.shape)\n",
    "print('number of samples in Validation set',XX_val.shape[0])\n",
    "print('Size of a image in Validation set',XX_val.shape[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pyWZQH8rrWGr"
   },
   "source": [
    "The above portion makes sense. Validation dataset is lesser in number than the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EfmHQ6FNPyLk",
    "outputId": "ef41e099-8caf-4b21-9e70-a85162ed097e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1.2316 - acc: 0.8041\n",
      "Try 1/50: Best_val_acc: [1.2316141668955485, 0.80405], lr: 0.000477876469842351, Lambda: 0.013681855934948094, Dropout: 0.09838070760642087\n",
      "\n",
      "60000/60000 [==============================] - 6s 94us/sample - loss: 1.7875 - acc: 0.7628\n",
      "Try 2/50: Best_val_acc: [1.7874560356140137, 0.76283336], lr: 3.210379529634468e-05, Lambda: 0.024680617818807073, Dropout: 0.03219941461419645\n",
      "\n",
      "60000/60000 [==============================] - 6s 93us/sample - loss: 0.6004 - acc: 0.8945\n",
      "Try 3/50: Best_val_acc: [0.6004279303073883, 0.89445], lr: 0.0004992788270625213, Lambda: 0.0010370185781516902, Dropout: 0.22800119704264665\n",
      "\n",
      "60000/60000 [==============================] - 6s 95us/sample - loss: 1.0621 - acc: 0.8468\n",
      "Try 4/50: Best_val_acc: [1.0621002674102784, 0.84678334], lr: 5.710635761069378e-05, Lambda: 0.005742993051744455, Dropout: 0.4664855487835374\n",
      "\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 1.0737 - acc: 0.8154\n",
      "Try 5/50: Best_val_acc: [1.0737384007453918, 0.8153667], lr: 0.0008670392639145442, Lambda: 0.008056212219263515, Dropout: 0.1582465730826434\n",
      "\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.9064 - acc: 0.8823\n",
      "Try 6/50: Best_val_acc: [0.9064076821009318, 0.8823], lr: 5.9703896496378944e-05, Lambda: 0.0013239279154897792, Dropout: 0.3174945330628008\n",
      "\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 2.3029 - acc: 0.1000\n",
      "Try 7/50: Best_val_acc: [2.3028524636586507, 0.1], lr: 1.3507319154511536e-05, Lambda: 0.06597060281579953, Dropout: 0.21874390735714133\n",
      "\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 1.2269 - acc: 0.8363\n",
      "Try 8/50: Best_val_acc: [1.2268512370109559, 0.8363], lr: 3.5308063689189097e-05, Lambda: 0.008210199277604614, Dropout: 0.3564664182572833\n",
      "\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.9309 - acc: 0.8386\n",
      "Try 9/50: Best_val_acc: [0.930891148519516, 0.83865], lr: 0.00031649718756958844, Lambda: 0.004056309906163576, Dropout: 0.4618074144016706\n",
      "\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 2.1348 - acc: 0.4098\n",
      "Try 10/50: Best_val_acc: [2.134796764755249, 0.40978333], lr: 8.152194394234525e-05, Lambda: 0.04043634106480138, Dropout: 0.4435975797448657\n",
      "\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 11/50: Best_val_acc: [2.302620267868042, 0.1], lr: 7.768836656423017e-05, Lambda: 0.055679750012946144, Dropout: 0.1663914303355482\n",
      "\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.6389 - acc: 0.8823\n",
      "Try 12/50: Best_val_acc: [0.6388886974016825, 0.88226664], lr: 0.0005346166192134578, Lambda: 0.0015697380865957768, Dropout: 0.010335209657480748\n",
      "\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 13/50: Best_val_acc: [2.3026200055440267, 0.1], lr: 0.000288704340786085, Lambda: 0.05872901064912041, Dropout: 0.4209520633060271\n",
      "\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 1.4221 - acc: 0.7514\n",
      "Try 14/50: Best_val_acc: [1.4221230190594991, 0.7513667], lr: 0.0007106749245854195, Lambda: 0.014875484936272542, Dropout: 0.42353021005993974\n",
      "\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 1.8553 - acc: 0.5106\n",
      "Try 15/50: Best_val_acc: [1.8553026009877522, 0.51058334], lr: 0.00042668239202248116, Lambda: 0.029736029495748415, Dropout: 0.32867534624350464\n",
      "\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.7739 - acc: 0.8823\n",
      "Try 16/50: Best_val_acc: [0.7739107782522837, 0.88228333], lr: 9.721723225632963e-05, Lambda: 0.002318273053236525, Dropout: 0.373367912714547\n",
      "\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 1.8566 - acc: 0.4866\n",
      "Try 17/50: Best_val_acc: [1.8566095166524252, 0.48658332], lr: 0.0007517131966497787, Lambda: 0.03350093474933404, Dropout: 0.15976999037376227\n",
      "\n",
      "60000/60000 [==============================] - 6s 106us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 18/50: Best_val_acc: [2.3026115179697673, 0.1], lr: 0.000217830383381301, Lambda: 0.07445449775018159, Dropout: 0.42352500555394196\n",
      "\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 2.6768 - acc: 0.7539\n",
      "Try 19/50: Best_val_acc: [2.6768332964579264, 0.7539167], lr: 1.2770830169755281e-05, Lambda: 0.003868648431144269, Dropout: 0.4266977393057068\n",
      "\n",
      "60000/60000 [==============================] - 6s 107us/sample - loss: 0.9362 - acc: 0.8346\n",
      "Try 20/50: Best_val_acc: [0.9361505393346151, 0.8346], lr: 0.0004960454612665715, Lambda: 0.005342794770831822, Dropout: 0.06473572575857439\n",
      "\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 1.0511 - acc: 0.8667\n",
      "Try 21/50: Best_val_acc: [1.051133902645111, 0.8666667], lr: 4.528133319841982e-05, Lambda: 0.005344303957747622, Dropout: 0.3031400962659542\n",
      "\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 1.3123 - acc: 0.7811\n",
      "Try 22/50: Best_val_acc: [1.3122988868077596, 0.78108335], lr: 0.0005055404525976537, Lambda: 0.013849423196229291, Dropout: 0.297902931927357\n",
      "\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.6762 - acc: 0.8709\n",
      "Try 23/50: Best_val_acc: [0.6761825654506683, 0.87095], lr: 0.0005250534221015598, Lambda: 0.0012300674493165968, Dropout: 0.28165106818934793\n",
      "\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 1.0174 - acc: 0.8749\n",
      "Try 24/50: Best_val_acc: [1.0173996987024942, 0.8749], lr: 4.937206213035812e-05, Lambda: 0.0020925790771896815, Dropout: 0.29568505258609334\n",
      "\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 2.3053 - acc: 0.7881\n",
      "Try 25/50: Best_val_acc: [2.305260379155477, 0.7880833], lr: 1.257488725754965e-05, Lambda: 0.008629111390239799, Dropout: 0.29895726760829394\n",
      "\n",
      "60000/60000 [==============================] - 7s 112us/sample - loss: 2.0900 - acc: 0.6022\n",
      "Try 26/50: Best_val_acc: [2.089983042907715, 0.60225], lr: 2.6543184917916896e-05, Lambda: 0.035786381556732076, Dropout: 0.19122010318531935\n",
      "\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 1.9971 - acc: 0.8440\n",
      "Try 27/50: Best_val_acc: [1.9970532537460328, 0.844], lr: 1.3184084120764499e-05, Lambda: 0.002374985587239457, Dropout: 0.03613631301748993\n",
      "\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 1.3280 - acc: 0.8399\n",
      "Try 28/50: Best_val_acc: [1.3280387580871582, 0.8398833], lr: 2.7835354860756262e-05, Lambda: 0.006729142036924046, Dropout: 0.32779119352224034\n",
      "\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 2.2573 - acc: 0.2224\n",
      "Try 29/50: Best_val_acc: [2.2573327234903973, 0.22243333], lr: 4.598860056773896e-05, Lambda: 0.050976447732814144, Dropout: 0.24437445232215183\n",
      "\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 1.4047 - acc: 0.8246\n",
      "Try 30/50: Best_val_acc: [1.4046616298675536, 0.82465], lr: 3.7522920787985583e-05, Lambda: 0.014474887175607604, Dropout: 0.06580543823653418\n",
      "\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 1.0706 - acc: 0.8282\n",
      "Try 31/50: Best_val_acc: [1.0705698254585265, 0.8282], lr: 0.00044020106655991685, Lambda: 0.008784163262912571, Dropout: 0.16566011914059653\n",
      "\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 32/50: Best_val_acc: [2.302611422475179, 0.1], lr: 0.00014483673447989164, Lambda: 0.09988078925080433, Dropout: 0.23672817231313853\n",
      "\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 1.1892 - acc: 0.7925\n",
      "Try 33/50: Best_val_acc: [1.1891779397646587, 0.79251665], lr: 0.0009467337836464388, Lambda: 0.010869083014130573, Dropout: 0.1260463078615051\n",
      "\n",
      "60000/60000 [==============================] - 7s 119us/sample - loss: 1.8034 - acc: 0.7007\n",
      "Try 34/50: Best_val_acc: [1.8033538651148477, 0.70068336], lr: 0.0001518333446759067, Lambda: 0.03235833459532924, Dropout: 0.045387741900287604\n",
      "\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 2.5474 - acc: 0.7775\n",
      "Try 35/50: Best_val_acc: [2.5473602474212647, 0.77746665], lr: 1.043725012845247e-05, Lambda: 0.009835028581099158, Dropout: 0.246452532742664\n",
      "\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 36/50: Best_val_acc: [2.3026336194356283, 0.1], lr: 0.0006911968944406136, Lambda: 0.09280402787079924, Dropout: 0.4520066262374534\n",
      "\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.7034 - acc: 0.8751\n",
      "Try 37/50: Best_val_acc: [0.7033910770257314, 0.87511665], lr: 0.00038550009924671284, Lambda: 0.001860142560308123, Dropout: 0.2910249181228764\n",
      "\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 38/50: Best_val_acc: [2.3026244402567544, 0.1], lr: 0.0004916398279856378, Lambda: 0.06741870356213853, Dropout: 0.16460063706203276\n",
      "\n",
      "60000/60000 [==============================] - 7s 125us/sample - loss: 1.3659 - acc: 0.7961\n",
      "Try 39/50: Best_val_acc: [1.3659311107317607, 0.79608333], lr: 0.00032825526358846806, Lambda: 0.017120126915441833, Dropout: 0.181071495825958\n",
      "\n",
      "60000/60000 [==============================] - 7s 123us/sample - loss: 0.6754 - acc: 0.8874\n",
      "Try 40/50: Best_val_acc: [0.675410442908605, 0.88745], lr: 0.00014350153057830417, Lambda: 0.0017451056579731656, Dropout: 0.35484180598153486\n",
      "\n",
      "60000/60000 [==============================] - 7s 124us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 41/50: Best_val_acc: [2.302635931905111, 0.1], lr: 0.0003720630508307096, Lambda: 0.04402603125916054, Dropout: 0.3770128226494613\n",
      "\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 0.7019 - acc: 0.8716\n",
      "Try 42/50: Best_val_acc: [0.7018530548731486, 0.87158334], lr: 0.0007653316616734308, Lambda: 0.0018201160884547122, Dropout: 0.10671302085839462\n",
      "\n",
      "60000/60000 [==============================] - 8s 129us/sample - loss: 1.1223 - acc: 0.8469\n",
      "Try 43/50: Best_val_acc: [1.1222866719881694, 0.84686667], lr: 4.5250508074863305e-05, Lambda: 0.006513588793558604, Dropout: 0.3889728809890806\n",
      "\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 1.3248 - acc: 0.8140\n",
      "Try 44/50: Best_val_acc: [1.32481251633962, 0.8139667], lr: 4.345148289403139e-05, Lambda: 0.011586381636950034, Dropout: 0.3856890870206174\n",
      "\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.9044 - acc: 0.8615\n",
      "Try 45/50: Best_val_acc: [0.9043507130940756, 0.86145], lr: 0.0001596844914811779, Lambda: 0.004727961265166578, Dropout: 0.3879446304933478\n",
      "\n",
      "60000/60000 [==============================] - 8s 131us/sample - loss: 1.3189 - acc: 0.8051\n",
      "Try 46/50: Best_val_acc: [1.3188769238471985, 0.80515], lr: 0.00019231686258019037, Lambda: 0.014088669444990811, Dropout: 0.26226282795534256\n",
      "\n",
      "60000/60000 [==============================] - 8s 132us/sample - loss: 1.6382 - acc: 0.8053\n",
      "Try 47/50: Best_val_acc: [1.6381510677337647, 0.8053167], lr: 1.6192184470592153e-05, Lambda: 0.013849102718604699, Dropout: 0.16829942825327265\n",
      "\n",
      "60000/60000 [==============================] - 8s 133us/sample - loss: 1.6717 - acc: 0.7797\n",
      "Try 48/50: Best_val_acc: [1.6716592765172322, 0.77973336], lr: 4.9332542826778435e-05, Lambda: 0.022442500127900472, Dropout: 0.10270544447096025\n",
      "\n",
      "60000/60000 [==============================] - 8s 137us/sample - loss: 2.3026 - acc: 0.1000\n",
      "Try 49/50: Best_val_acc: [2.3026142836252848, 0.1], lr: 0.00047149004966900353, Lambda: 0.09451152099452291, Dropout: 0.03340832785735609\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from keras.callbacks import EarlyStopping\n",
    "for k in range(1,50):\n",
    "  Learning_rate=math.pow(10, np.random.uniform(-5,-3))\n",
    "  neurons=[350,250,150,10]\n",
    "  Regularize=math.pow(10, np.random.uniform(-3,-1))\n",
    "  Activation=['relu','relu','relu','softmax']\n",
    "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=False, patience=5)\n",
    "  do=np.random.uniform(0,0.5)\n",
    "  Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "\n",
    "  modelx=tf.keras.models.Sequential()\n",
    "  modelx.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "  for i in range(0,len(neurons)):\n",
    "    modelx.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "    if i != (len(neurons)-1):\n",
    "      modelx.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "  modelx.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "  modelx.fit(nnfX_train,HHy_train,validation_data=(nnfX_val,HHy_val),batch_size=128,epochs=30,verbose=False,callbacks=[es])\n",
    "\n",
    "  print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}, Dropout: {5}\\n\".format(k, 50, modelx.evaluate(nfX_val,HY_val),Learning_rate,Regularize,do))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XjjJS_Bwu4Ta"
   },
   "source": [
    "Higher accuracy of 0.89445 occured at\n",
    "\n",
    "lr: 0.0004992788270625213, Lambda: 0.0010370185781516902, Dropout: 0.22800119704264665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pmaqe5pDTQ8F",
    "outputId": "51b7dddd-4f66-4987-feef-07d42f7c23d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71400 samples, validate on 30600 samples\n",
      "Epoch 1/500\n",
      "71400/71400 [==============================] - 9s 130us/sample - loss: 2.4113 - acc: 0.4922 - val_loss: 1.6978 - val_acc: 0.7323\n",
      "Epoch 2/500\n",
      "71400/71400 [==============================] - 5s 67us/sample - loss: 1.6679 - acc: 0.6980 - val_loss: 1.3526 - val_acc: 0.7792\n",
      "Epoch 3/500\n",
      "71400/71400 [==============================] - 5s 66us/sample - loss: 1.3938 - acc: 0.7400 - val_loss: 1.1612 - val_acc: 0.8030\n",
      "Epoch 4/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 1.2197 - acc: 0.7636 - val_loss: 1.0471 - val_acc: 0.8086\n",
      "Epoch 5/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 1.1075 - acc: 0.7763 - val_loss: 0.9516 - val_acc: 0.8251\n",
      "Epoch 6/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 1.0286 - acc: 0.7878 - val_loss: 0.8772 - val_acc: 0.8340\n",
      "Epoch 7/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.9672 - acc: 0.7964 - val_loss: 0.8437 - val_acc: 0.8342\n",
      "Epoch 8/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.9329 - acc: 0.7994 - val_loss: 0.7917 - val_acc: 0.8487\n",
      "Epoch 9/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.8972 - acc: 0.8082 - val_loss: 0.7763 - val_acc: 0.8500\n",
      "Epoch 10/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.8736 - acc: 0.8131 - val_loss: 0.7521 - val_acc: 0.8545\n",
      "Epoch 11/500\n",
      "71400/71400 [==============================] - 5s 65us/sample - loss: 0.8570 - acc: 0.8153 - val_loss: 0.7583 - val_acc: 0.8495\n",
      "Epoch 12/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.8377 - acc: 0.8195 - val_loss: 0.7782 - val_acc: 0.8423\n",
      "Epoch 13/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.8276 - acc: 0.8218 - val_loss: 0.7259 - val_acc: 0.8575\n",
      "Epoch 14/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.8139 - acc: 0.8258 - val_loss: 0.7273 - val_acc: 0.8587\n",
      "Epoch 15/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.8077 - acc: 0.8261 - val_loss: 0.7089 - val_acc: 0.8619\n",
      "Epoch 16/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.7992 - acc: 0.8292 - val_loss: 0.6988 - val_acc: 0.8681\n",
      "Epoch 17/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7903 - acc: 0.8315 - val_loss: 0.6841 - val_acc: 0.8699\n",
      "Epoch 18/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7871 - acc: 0.8328 - val_loss: 0.6960 - val_acc: 0.8648\n",
      "Epoch 19/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7806 - acc: 0.8333 - val_loss: 0.6861 - val_acc: 0.8692\n",
      "Epoch 20/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7786 - acc: 0.8332 - val_loss: 0.6831 - val_acc: 0.8701\n",
      "Epoch 21/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7685 - acc: 0.8379 - val_loss: 0.6794 - val_acc: 0.8704\n",
      "Epoch 22/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7699 - acc: 0.8367 - val_loss: 0.6896 - val_acc: 0.8663\n",
      "Epoch 23/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7692 - acc: 0.8363 - val_loss: 0.6957 - val_acc: 0.8620\n",
      "Epoch 24/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7601 - acc: 0.8391 - val_loss: 0.6713 - val_acc: 0.8753\n",
      "Epoch 25/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7572 - acc: 0.8397 - val_loss: 0.6760 - val_acc: 0.8708\n",
      "Epoch 26/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7510 - acc: 0.8417 - val_loss: 0.6669 - val_acc: 0.8726\n",
      "Epoch 27/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7500 - acc: 0.8406 - val_loss: 0.6591 - val_acc: 0.8751\n",
      "Epoch 28/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7430 - acc: 0.8432 - val_loss: 0.6692 - val_acc: 0.8714\n",
      "Epoch 29/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7424 - acc: 0.8432 - val_loss: 0.6698 - val_acc: 0.8700\n",
      "Epoch 30/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.7426 - acc: 0.8438 - val_loss: 0.6682 - val_acc: 0.8685\n",
      "Epoch 31/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.7381 - acc: 0.8440 - val_loss: 0.6657 - val_acc: 0.8711\n",
      "Epoch 32/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7404 - acc: 0.8440 - val_loss: 0.6558 - val_acc: 0.8772\n",
      "Epoch 33/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7387 - acc: 0.8436 - val_loss: 0.6560 - val_acc: 0.8760\n",
      "Epoch 34/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7334 - acc: 0.8456 - val_loss: 0.6514 - val_acc: 0.8775\n",
      "Epoch 35/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7312 - acc: 0.8446 - val_loss: 0.6430 - val_acc: 0.8782\n",
      "Epoch 36/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7289 - acc: 0.8458 - val_loss: 0.6537 - val_acc: 0.8740\n",
      "Epoch 37/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.7302 - acc: 0.8473 - val_loss: 0.6437 - val_acc: 0.8796\n",
      "Epoch 38/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7265 - acc: 0.8469 - val_loss: 0.6529 - val_acc: 0.8771\n",
      "Epoch 39/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7206 - acc: 0.8489 - val_loss: 0.6639 - val_acc: 0.8699\n",
      "Epoch 40/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7228 - acc: 0.8485 - val_loss: 0.6624 - val_acc: 0.8705\n",
      "Epoch 41/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7232 - acc: 0.8483 - val_loss: 0.6364 - val_acc: 0.8805\n",
      "Epoch 42/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7178 - acc: 0.8494 - val_loss: 0.6401 - val_acc: 0.8789\n",
      "Epoch 43/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.7232 - acc: 0.8478 - val_loss: 0.6498 - val_acc: 0.8763\n",
      "Epoch 44/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7166 - acc: 0.8495 - val_loss: 0.6464 - val_acc: 0.8765\n",
      "Epoch 45/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7180 - acc: 0.8502 - val_loss: 0.6686 - val_acc: 0.8702\n",
      "Epoch 46/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7202 - acc: 0.8476 - val_loss: 0.6445 - val_acc: 0.8770\n",
      "Epoch 47/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7148 - acc: 0.8512 - val_loss: 0.6445 - val_acc: 0.8781\n",
      "Epoch 48/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7204 - acc: 0.8473 - val_loss: 0.6466 - val_acc: 0.8778\n",
      "Epoch 49/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.7149 - acc: 0.8501 - val_loss: 0.6253 - val_acc: 0.8856\n",
      "Epoch 50/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.7109 - acc: 0.8522 - val_loss: 0.6595 - val_acc: 0.8700\n",
      "Epoch 51/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7133 - acc: 0.8505 - val_loss: 0.6447 - val_acc: 0.8772\n",
      "Epoch 52/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7089 - acc: 0.8525 - val_loss: 0.6283 - val_acc: 0.8830\n",
      "Epoch 53/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7093 - acc: 0.8523 - val_loss: 0.6315 - val_acc: 0.8803\n",
      "Epoch 54/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7088 - acc: 0.8514 - val_loss: 0.6437 - val_acc: 0.8738\n",
      "Epoch 55/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7084 - acc: 0.8516 - val_loss: 0.6492 - val_acc: 0.8726\n",
      "Epoch 56/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7086 - acc: 0.8527 - val_loss: 0.6404 - val_acc: 0.8775\n",
      "Epoch 57/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7041 - acc: 0.8532 - val_loss: 0.6272 - val_acc: 0.8840\n",
      "Epoch 58/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7016 - acc: 0.8535 - val_loss: 0.6350 - val_acc: 0.8788\n",
      "Epoch 59/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7023 - acc: 0.8529 - val_loss: 0.6337 - val_acc: 0.8818\n",
      "Epoch 60/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6995 - acc: 0.8536 - val_loss: 0.6186 - val_acc: 0.8855\n",
      "Epoch 61/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.7027 - acc: 0.8525 - val_loss: 0.6198 - val_acc: 0.8869\n",
      "Epoch 62/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.7047 - acc: 0.8513 - val_loss: 0.6363 - val_acc: 0.8794\n",
      "Epoch 63/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.7026 - acc: 0.8532 - val_loss: 0.6285 - val_acc: 0.8809\n",
      "Epoch 64/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6993 - acc: 0.8537 - val_loss: 0.6412 - val_acc: 0.8759\n",
      "Epoch 65/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6978 - acc: 0.8558 - val_loss: 0.6287 - val_acc: 0.8807\n",
      "Epoch 66/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6983 - acc: 0.8543 - val_loss: 0.6172 - val_acc: 0.8851\n",
      "Epoch 67/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.7020 - acc: 0.8536 - val_loss: 0.6159 - val_acc: 0.8851\n",
      "Epoch 68/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6962 - acc: 0.8545 - val_loss: 0.6318 - val_acc: 0.8794\n",
      "Epoch 69/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6946 - acc: 0.8560 - val_loss: 0.6346 - val_acc: 0.8766\n",
      "Epoch 70/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6933 - acc: 0.8555 - val_loss: 0.6326 - val_acc: 0.8787\n",
      "Epoch 71/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6960 - acc: 0.8533 - val_loss: 0.6166 - val_acc: 0.8877\n",
      "Epoch 72/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6962 - acc: 0.8544 - val_loss: 0.6291 - val_acc: 0.8799\n",
      "Epoch 73/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6924 - acc: 0.8567 - val_loss: 0.6089 - val_acc: 0.8888\n",
      "Epoch 74/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6929 - acc: 0.8554 - val_loss: 0.6315 - val_acc: 0.8785\n",
      "Epoch 75/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6945 - acc: 0.8547 - val_loss: 0.6084 - val_acc: 0.8892\n",
      "Epoch 76/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6922 - acc: 0.8561 - val_loss: 0.6168 - val_acc: 0.8846\n",
      "Epoch 77/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6898 - acc: 0.8576 - val_loss: 0.6097 - val_acc: 0.8868\n",
      "Epoch 78/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6903 - acc: 0.8556 - val_loss: 0.6051 - val_acc: 0.8875\n",
      "Epoch 79/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6899 - acc: 0.8558 - val_loss: 0.6183 - val_acc: 0.8821\n",
      "Epoch 80/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6885 - acc: 0.8571 - val_loss: 0.6208 - val_acc: 0.8829\n",
      "Epoch 81/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6923 - acc: 0.8559 - val_loss: 0.6128 - val_acc: 0.8857\n",
      "Epoch 82/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6823 - acc: 0.8590 - val_loss: 0.6074 - val_acc: 0.8871\n",
      "Epoch 83/500\n",
      "71400/71400 [==============================] - 5s 65us/sample - loss: 0.6881 - acc: 0.8562 - val_loss: 0.6241 - val_acc: 0.8797\n",
      "Epoch 84/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6917 - acc: 0.8554 - val_loss: 0.6189 - val_acc: 0.8825\n",
      "Epoch 85/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6891 - acc: 0.8567 - val_loss: 0.6348 - val_acc: 0.8768\n",
      "Epoch 86/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6845 - acc: 0.8582 - val_loss: 0.6045 - val_acc: 0.8878\n",
      "Epoch 87/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6866 - acc: 0.8569 - val_loss: 0.6364 - val_acc: 0.8775\n",
      "Epoch 88/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6931 - acc: 0.8545 - val_loss: 0.6072 - val_acc: 0.8867\n",
      "Epoch 89/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6858 - acc: 0.8571 - val_loss: 0.6023 - val_acc: 0.8900\n",
      "Epoch 90/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6884 - acc: 0.8561 - val_loss: 0.6239 - val_acc: 0.8806\n",
      "Epoch 91/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6883 - acc: 0.8573 - val_loss: 0.6075 - val_acc: 0.8878\n",
      "Epoch 92/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6835 - acc: 0.8573 - val_loss: 0.6094 - val_acc: 0.8846\n",
      "Epoch 93/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6858 - acc: 0.8566 - val_loss: 0.6112 - val_acc: 0.8878\n",
      "Epoch 94/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6842 - acc: 0.8585 - val_loss: 0.6234 - val_acc: 0.8817\n",
      "Epoch 95/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6836 - acc: 0.8575 - val_loss: 0.6099 - val_acc: 0.8860\n",
      "Epoch 96/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6817 - acc: 0.8569 - val_loss: 0.6157 - val_acc: 0.8846\n",
      "Epoch 97/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6842 - acc: 0.8567 - val_loss: 0.6116 - val_acc: 0.8853\n",
      "Epoch 98/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6800 - acc: 0.8590 - val_loss: 0.6176 - val_acc: 0.8832\n",
      "Epoch 99/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6780 - acc: 0.8588 - val_loss: 0.5994 - val_acc: 0.8893\n",
      "Epoch 100/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6841 - acc: 0.8557 - val_loss: 0.6100 - val_acc: 0.8861\n",
      "Epoch 101/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6805 - acc: 0.8570 - val_loss: 0.6094 - val_acc: 0.8858\n",
      "Epoch 102/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6783 - acc: 0.8589 - val_loss: 0.5928 - val_acc: 0.8907\n",
      "Epoch 103/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6786 - acc: 0.8593 - val_loss: 0.5987 - val_acc: 0.8877\n",
      "Epoch 104/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6792 - acc: 0.8585 - val_loss: 0.6070 - val_acc: 0.8875\n",
      "Epoch 105/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6834 - acc: 0.8570 - val_loss: 0.6063 - val_acc: 0.8862\n",
      "Epoch 106/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6778 - acc: 0.8589 - val_loss: 0.6042 - val_acc: 0.8855\n",
      "Epoch 107/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6759 - acc: 0.8587 - val_loss: 0.5939 - val_acc: 0.8912\n",
      "Epoch 108/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6774 - acc: 0.8577 - val_loss: 0.6005 - val_acc: 0.8906\n",
      "Epoch 109/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6760 - acc: 0.8596 - val_loss: 0.6107 - val_acc: 0.8840\n",
      "Epoch 110/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6763 - acc: 0.8576 - val_loss: 0.6010 - val_acc: 0.8879\n",
      "Epoch 111/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6757 - acc: 0.8601 - val_loss: 0.6041 - val_acc: 0.8873\n",
      "Epoch 112/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6795 - acc: 0.8583 - val_loss: 0.6059 - val_acc: 0.8862\n",
      "Epoch 113/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6743 - acc: 0.8599 - val_loss: 0.6027 - val_acc: 0.8866\n",
      "Epoch 114/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6804 - acc: 0.8571 - val_loss: 0.5994 - val_acc: 0.8879\n",
      "Epoch 115/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6737 - acc: 0.8576 - val_loss: 0.5970 - val_acc: 0.8898\n",
      "Epoch 116/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6749 - acc: 0.8600 - val_loss: 0.6089 - val_acc: 0.8846\n",
      "Epoch 117/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6728 - acc: 0.8595 - val_loss: 0.5988 - val_acc: 0.8876\n",
      "Epoch 118/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6755 - acc: 0.8585 - val_loss: 0.6052 - val_acc: 0.8884\n",
      "Epoch 119/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6726 - acc: 0.8597 - val_loss: 0.6068 - val_acc: 0.8875\n",
      "Epoch 120/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6746 - acc: 0.8586 - val_loss: 0.6129 - val_acc: 0.8847\n",
      "Epoch 121/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6712 - acc: 0.8595 - val_loss: 0.6092 - val_acc: 0.8854\n",
      "Epoch 122/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6710 - acc: 0.8613 - val_loss: 0.5938 - val_acc: 0.8906\n",
      "Epoch 123/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6741 - acc: 0.8593 - val_loss: 0.6036 - val_acc: 0.8837\n",
      "Epoch 124/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6705 - acc: 0.8607 - val_loss: 0.6084 - val_acc: 0.8859\n",
      "Epoch 125/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6688 - acc: 0.8604 - val_loss: 0.5979 - val_acc: 0.8899\n",
      "Epoch 126/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6754 - acc: 0.8575 - val_loss: 0.6502 - val_acc: 0.8679\n",
      "Epoch 127/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6731 - acc: 0.8593 - val_loss: 0.6035 - val_acc: 0.8875\n",
      "Epoch 128/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6652 - acc: 0.8625 - val_loss: 0.5958 - val_acc: 0.8900\n",
      "Epoch 129/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6746 - acc: 0.8584 - val_loss: 0.6036 - val_acc: 0.8864\n",
      "Epoch 130/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6686 - acc: 0.8606 - val_loss: 0.5964 - val_acc: 0.8897\n",
      "Epoch 131/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6697 - acc: 0.8604 - val_loss: 0.6074 - val_acc: 0.8857\n",
      "Epoch 132/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6705 - acc: 0.8601 - val_loss: 0.5854 - val_acc: 0.8928\n",
      "Epoch 133/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6723 - acc: 0.8597 - val_loss: 0.5876 - val_acc: 0.8908\n",
      "Epoch 134/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6707 - acc: 0.8609 - val_loss: 0.5894 - val_acc: 0.8895\n",
      "Epoch 135/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6694 - acc: 0.8611 - val_loss: 0.5848 - val_acc: 0.8922\n",
      "Epoch 136/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6645 - acc: 0.8614 - val_loss: 0.6008 - val_acc: 0.8861\n",
      "Epoch 137/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6713 - acc: 0.8600 - val_loss: 0.6028 - val_acc: 0.8869\n",
      "Epoch 138/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6634 - acc: 0.8627 - val_loss: 0.6032 - val_acc: 0.8850\n",
      "Epoch 139/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6669 - acc: 0.8612 - val_loss: 0.6044 - val_acc: 0.8836\n",
      "Epoch 140/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6684 - acc: 0.8610 - val_loss: 0.5781 - val_acc: 0.8946\n",
      "Epoch 141/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6674 - acc: 0.8600 - val_loss: 0.5892 - val_acc: 0.8924\n",
      "Epoch 142/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6625 - acc: 0.8623 - val_loss: 0.5906 - val_acc: 0.8900\n",
      "Epoch 143/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6660 - acc: 0.8608 - val_loss: 0.5983 - val_acc: 0.8899\n",
      "Epoch 144/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6646 - acc: 0.8614 - val_loss: 0.5934 - val_acc: 0.8890\n",
      "Epoch 145/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6624 - acc: 0.8624 - val_loss: 0.6007 - val_acc: 0.8871\n",
      "Epoch 146/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6688 - acc: 0.8608 - val_loss: 0.5873 - val_acc: 0.8912\n",
      "Epoch 147/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6621 - acc: 0.8617 - val_loss: 0.6049 - val_acc: 0.8852\n",
      "Epoch 148/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6634 - acc: 0.8614 - val_loss: 0.5827 - val_acc: 0.8922\n",
      "Epoch 149/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6637 - acc: 0.8605 - val_loss: 0.5887 - val_acc: 0.8903\n",
      "Epoch 150/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6643 - acc: 0.8602 - val_loss: 0.6011 - val_acc: 0.8843\n",
      "Epoch 151/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6613 - acc: 0.8625 - val_loss: 0.5857 - val_acc: 0.8912\n",
      "Epoch 152/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6633 - acc: 0.8623 - val_loss: 0.5807 - val_acc: 0.8938\n",
      "Epoch 153/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6631 - acc: 0.8616 - val_loss: 0.5855 - val_acc: 0.8908\n",
      "Epoch 154/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6616 - acc: 0.8612 - val_loss: 0.5897 - val_acc: 0.8884\n",
      "Epoch 155/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6590 - acc: 0.8632 - val_loss: 0.5964 - val_acc: 0.8902\n",
      "Epoch 156/500\n",
      "71400/71400 [==============================] - 5s 65us/sample - loss: 0.6640 - acc: 0.8620 - val_loss: 0.5902 - val_acc: 0.8889\n",
      "Epoch 157/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6627 - acc: 0.8629 - val_loss: 0.5964 - val_acc: 0.8880\n",
      "Epoch 158/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6596 - acc: 0.8623 - val_loss: 0.5950 - val_acc: 0.8875\n",
      "Epoch 159/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6619 - acc: 0.8613 - val_loss: 0.5900 - val_acc: 0.8892\n",
      "Epoch 160/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6589 - acc: 0.8631 - val_loss: 0.5870 - val_acc: 0.8901\n",
      "Epoch 161/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6609 - acc: 0.8630 - val_loss: 0.5922 - val_acc: 0.8901\n",
      "Epoch 162/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6591 - acc: 0.8613 - val_loss: 0.6078 - val_acc: 0.8839\n",
      "Epoch 163/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6597 - acc: 0.8631 - val_loss: 0.5835 - val_acc: 0.8897\n",
      "Epoch 164/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6596 - acc: 0.8621 - val_loss: 0.5908 - val_acc: 0.8911\n",
      "Epoch 165/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6563 - acc: 0.8636 - val_loss: 0.5858 - val_acc: 0.8907\n",
      "Epoch 166/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6587 - acc: 0.8637 - val_loss: 0.6052 - val_acc: 0.8837\n",
      "Epoch 167/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6562 - acc: 0.8629 - val_loss: 0.5775 - val_acc: 0.8932\n",
      "Epoch 168/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6543 - acc: 0.8642 - val_loss: 0.5945 - val_acc: 0.8897\n",
      "Epoch 169/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6587 - acc: 0.8620 - val_loss: 0.5842 - val_acc: 0.8910\n",
      "Epoch 170/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6594 - acc: 0.8627 - val_loss: 0.5846 - val_acc: 0.8927\n",
      "Epoch 171/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6546 - acc: 0.8636 - val_loss: 0.5937 - val_acc: 0.8880\n",
      "Epoch 172/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6576 - acc: 0.8641 - val_loss: 0.5856 - val_acc: 0.8929\n",
      "Epoch 173/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6569 - acc: 0.8623 - val_loss: 0.5865 - val_acc: 0.8908\n",
      "Epoch 174/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6596 - acc: 0.8620 - val_loss: 0.5915 - val_acc: 0.8892\n",
      "Epoch 175/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6552 - acc: 0.8642 - val_loss: 0.5724 - val_acc: 0.8973\n",
      "Epoch 176/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6563 - acc: 0.8627 - val_loss: 0.5937 - val_acc: 0.8882\n",
      "Epoch 177/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6569 - acc: 0.8629 - val_loss: 0.5891 - val_acc: 0.8891\n",
      "Epoch 178/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6555 - acc: 0.8629 - val_loss: 0.5957 - val_acc: 0.8875\n",
      "Epoch 179/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6517 - acc: 0.8638 - val_loss: 0.5856 - val_acc: 0.8914\n",
      "Epoch 180/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6501 - acc: 0.8658 - val_loss: 0.5711 - val_acc: 0.8951\n",
      "Epoch 181/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6581 - acc: 0.8644 - val_loss: 0.5807 - val_acc: 0.8925\n",
      "Epoch 182/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6557 - acc: 0.8631 - val_loss: 0.5786 - val_acc: 0.8921\n",
      "Epoch 183/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6543 - acc: 0.8630 - val_loss: 0.6033 - val_acc: 0.8853\n",
      "Epoch 184/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6523 - acc: 0.8632 - val_loss: 0.5862 - val_acc: 0.8917\n",
      "Epoch 185/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6510 - acc: 0.8653 - val_loss: 0.6008 - val_acc: 0.8829\n",
      "Epoch 186/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6558 - acc: 0.8619 - val_loss: 0.5810 - val_acc: 0.8917\n",
      "Epoch 187/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6556 - acc: 0.8640 - val_loss: 0.6164 - val_acc: 0.8785\n",
      "Epoch 188/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6524 - acc: 0.8627 - val_loss: 0.5803 - val_acc: 0.8927\n",
      "Epoch 189/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6506 - acc: 0.8635 - val_loss: 0.5846 - val_acc: 0.8918\n",
      "Epoch 190/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6530 - acc: 0.8633 - val_loss: 0.5786 - val_acc: 0.8927\n",
      "Epoch 191/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6460 - acc: 0.8663 - val_loss: 0.5831 - val_acc: 0.8906\n",
      "Epoch 192/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6514 - acc: 0.8639 - val_loss: 0.5937 - val_acc: 0.8874\n",
      "Epoch 193/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6530 - acc: 0.8640 - val_loss: 0.5899 - val_acc: 0.8904\n",
      "Epoch 194/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6499 - acc: 0.8649 - val_loss: 0.5859 - val_acc: 0.8906\n",
      "Epoch 195/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6461 - acc: 0.8665 - val_loss: 0.5828 - val_acc: 0.8917\n",
      "Epoch 196/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6525 - acc: 0.8637 - val_loss: 0.5987 - val_acc: 0.8842\n",
      "Epoch 197/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6511 - acc: 0.8658 - val_loss: 0.6057 - val_acc: 0.8812\n",
      "Epoch 198/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6502 - acc: 0.8648 - val_loss: 0.6063 - val_acc: 0.8817\n",
      "Epoch 199/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6549 - acc: 0.8633 - val_loss: 0.5947 - val_acc: 0.8887\n",
      "Epoch 200/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6540 - acc: 0.8637 - val_loss: 0.5820 - val_acc: 0.8922\n",
      "Epoch 201/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6494 - acc: 0.8650 - val_loss: 0.5788 - val_acc: 0.8927\n",
      "Epoch 202/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6467 - acc: 0.8659 - val_loss: 0.5891 - val_acc: 0.8892\n",
      "Epoch 203/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6490 - acc: 0.8644 - val_loss: 0.5854 - val_acc: 0.8892\n",
      "Epoch 204/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6458 - acc: 0.8664 - val_loss: 0.5884 - val_acc: 0.8895\n",
      "Epoch 205/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6459 - acc: 0.8662 - val_loss: 0.5807 - val_acc: 0.8929\n",
      "Epoch 206/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6471 - acc: 0.8654 - val_loss: 0.5774 - val_acc: 0.8916\n",
      "Epoch 207/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6492 - acc: 0.8651 - val_loss: 0.5823 - val_acc: 0.8927\n",
      "Epoch 208/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6438 - acc: 0.8661 - val_loss: 0.5844 - val_acc: 0.8909\n",
      "Epoch 209/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6517 - acc: 0.8642 - val_loss: 0.5802 - val_acc: 0.8927\n",
      "Epoch 210/500\n",
      "71400/71400 [==============================] - 5s 66us/sample - loss: 0.6492 - acc: 0.8639 - val_loss: 0.5641 - val_acc: 0.8978\n",
      "Epoch 211/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6428 - acc: 0.8664 - val_loss: 0.5885 - val_acc: 0.8895\n",
      "Epoch 212/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6480 - acc: 0.8651 - val_loss: 0.5763 - val_acc: 0.8938\n",
      "Epoch 213/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6503 - acc: 0.8649 - val_loss: 0.5795 - val_acc: 0.8929\n",
      "Epoch 214/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6469 - acc: 0.8650 - val_loss: 0.5728 - val_acc: 0.8956\n",
      "Epoch 215/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6487 - acc: 0.8648 - val_loss: 0.5923 - val_acc: 0.8866\n",
      "Epoch 216/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6455 - acc: 0.8657 - val_loss: 0.5790 - val_acc: 0.8944\n",
      "Epoch 217/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6471 - acc: 0.8656 - val_loss: 0.5686 - val_acc: 0.8955\n",
      "Epoch 218/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6472 - acc: 0.8665 - val_loss: 0.5915 - val_acc: 0.8874\n",
      "Epoch 219/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6492 - acc: 0.8649 - val_loss: 0.5671 - val_acc: 0.8964\n",
      "Epoch 220/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6432 - acc: 0.8669 - val_loss: 0.5802 - val_acc: 0.8922\n",
      "Epoch 221/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6446 - acc: 0.8648 - val_loss: 0.5891 - val_acc: 0.8887\n",
      "Epoch 222/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6473 - acc: 0.8652 - val_loss: 0.5770 - val_acc: 0.8916\n",
      "Epoch 223/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6452 - acc: 0.8665 - val_loss: 0.5766 - val_acc: 0.8947\n",
      "Epoch 224/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6412 - acc: 0.8673 - val_loss: 0.5871 - val_acc: 0.8898\n",
      "Epoch 225/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6422 - acc: 0.8659 - val_loss: 0.5846 - val_acc: 0.8910\n",
      "Epoch 226/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6446 - acc: 0.8645 - val_loss: 0.5732 - val_acc: 0.8947\n",
      "Epoch 227/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6416 - acc: 0.8660 - val_loss: 0.5886 - val_acc: 0.8881\n",
      "Epoch 228/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6427 - acc: 0.8663 - val_loss: 0.6108 - val_acc: 0.8843\n",
      "Epoch 229/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6451 - acc: 0.8652 - val_loss: 0.5860 - val_acc: 0.8870\n",
      "Epoch 230/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6460 - acc: 0.8659 - val_loss: 0.5734 - val_acc: 0.8950\n",
      "Epoch 231/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6460 - acc: 0.8658 - val_loss: 0.5839 - val_acc: 0.8877\n",
      "Epoch 232/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6465 - acc: 0.8665 - val_loss: 0.5763 - val_acc: 0.8923\n",
      "Epoch 233/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6419 - acc: 0.8681 - val_loss: 0.5804 - val_acc: 0.8922\n",
      "Epoch 234/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6403 - acc: 0.8671 - val_loss: 0.5892 - val_acc: 0.8890\n",
      "Epoch 235/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6425 - acc: 0.8657 - val_loss: 0.5842 - val_acc: 0.8908\n",
      "Epoch 236/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6381 - acc: 0.8678 - val_loss: 0.5648 - val_acc: 0.8981\n",
      "Epoch 237/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6470 - acc: 0.8653 - val_loss: 0.6028 - val_acc: 0.8835\n",
      "Epoch 238/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6430 - acc: 0.8657 - val_loss: 0.5876 - val_acc: 0.8884\n",
      "Epoch 239/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6392 - acc: 0.8674 - val_loss: 0.5807 - val_acc: 0.8925\n",
      "Epoch 240/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6435 - acc: 0.8661 - val_loss: 0.5903 - val_acc: 0.8880\n",
      "Epoch 241/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6389 - acc: 0.8685 - val_loss: 0.5662 - val_acc: 0.8954\n",
      "Epoch 242/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6388 - acc: 0.8676 - val_loss: 0.5641 - val_acc: 0.8983\n",
      "Epoch 243/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6396 - acc: 0.8664 - val_loss: 0.5751 - val_acc: 0.8949\n",
      "Epoch 244/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6375 - acc: 0.8679 - val_loss: 0.5689 - val_acc: 0.8957\n",
      "Epoch 245/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6431 - acc: 0.8659 - val_loss: 0.5633 - val_acc: 0.8977\n",
      "Epoch 246/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6413 - acc: 0.8672 - val_loss: 0.5751 - val_acc: 0.8939\n",
      "Epoch 247/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6413 - acc: 0.8665 - val_loss: 0.5683 - val_acc: 0.8945\n",
      "Epoch 248/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6422 - acc: 0.8658 - val_loss: 0.5830 - val_acc: 0.8895\n",
      "Epoch 249/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6377 - acc: 0.8679 - val_loss: 0.5709 - val_acc: 0.8943\n",
      "Epoch 250/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6405 - acc: 0.8667 - val_loss: 0.5738 - val_acc: 0.8938\n",
      "Epoch 251/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6386 - acc: 0.8679 - val_loss: 0.5668 - val_acc: 0.8943\n",
      "Epoch 252/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6387 - acc: 0.8671 - val_loss: 0.5655 - val_acc: 0.8964\n",
      "Epoch 253/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6390 - acc: 0.8668 - val_loss: 0.5645 - val_acc: 0.8990\n",
      "Epoch 254/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6363 - acc: 0.8695 - val_loss: 0.5856 - val_acc: 0.8896\n",
      "Epoch 255/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6378 - acc: 0.8669 - val_loss: 0.5844 - val_acc: 0.8889\n",
      "Epoch 256/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6402 - acc: 0.8671 - val_loss: 0.5853 - val_acc: 0.8895\n",
      "Epoch 257/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6409 - acc: 0.8682 - val_loss: 0.5641 - val_acc: 0.8963\n",
      "Epoch 258/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6373 - acc: 0.8683 - val_loss: 0.5819 - val_acc: 0.8908\n",
      "Epoch 259/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6406 - acc: 0.8662 - val_loss: 0.5987 - val_acc: 0.8838\n",
      "Epoch 260/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6404 - acc: 0.8672 - val_loss: 0.5766 - val_acc: 0.8928\n",
      "Epoch 261/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6376 - acc: 0.8677 - val_loss: 0.5797 - val_acc: 0.8936\n",
      "Epoch 262/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6380 - acc: 0.8677 - val_loss: 0.5684 - val_acc: 0.8960\n",
      "Epoch 263/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6375 - acc: 0.8678 - val_loss: 0.5801 - val_acc: 0.8897\n",
      "Epoch 264/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6348 - acc: 0.8675 - val_loss: 0.5709 - val_acc: 0.8926\n",
      "Epoch 265/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6373 - acc: 0.8682 - val_loss: 0.5753 - val_acc: 0.8911\n",
      "Epoch 266/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6342 - acc: 0.8691 - val_loss: 0.5783 - val_acc: 0.8909\n",
      "Epoch 267/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6303 - acc: 0.8702 - val_loss: 0.5684 - val_acc: 0.8959\n",
      "Epoch 268/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6330 - acc: 0.8682 - val_loss: 0.5733 - val_acc: 0.8922\n",
      "Epoch 269/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6335 - acc: 0.8678 - val_loss: 0.5751 - val_acc: 0.8912\n",
      "Epoch 270/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6354 - acc: 0.8685 - val_loss: 0.5758 - val_acc: 0.8912\n",
      "Epoch 271/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6362 - acc: 0.8679 - val_loss: 0.5626 - val_acc: 0.8979\n",
      "Epoch 272/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6327 - acc: 0.8690 - val_loss: 0.5721 - val_acc: 0.8938\n",
      "Epoch 273/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6392 - acc: 0.8670 - val_loss: 0.5809 - val_acc: 0.8887\n",
      "Epoch 274/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6344 - acc: 0.8681 - val_loss: 0.5601 - val_acc: 0.8983\n",
      "Epoch 275/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6361 - acc: 0.8659 - val_loss: 0.5777 - val_acc: 0.8919\n",
      "Epoch 276/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6353 - acc: 0.8678 - val_loss: 0.5792 - val_acc: 0.8905\n",
      "Epoch 277/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6348 - acc: 0.8685 - val_loss: 0.5544 - val_acc: 0.9009\n",
      "Epoch 278/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6352 - acc: 0.8677 - val_loss: 0.5786 - val_acc: 0.8915\n",
      "Epoch 279/500\n",
      "71400/71400 [==============================] - 5s 65us/sample - loss: 0.6366 - acc: 0.8694 - val_loss: 0.5709 - val_acc: 0.8937\n",
      "Epoch 280/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6332 - acc: 0.8690 - val_loss: 0.5734 - val_acc: 0.8917\n",
      "Epoch 281/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6315 - acc: 0.8682 - val_loss: 0.5857 - val_acc: 0.8889\n",
      "Epoch 282/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6364 - acc: 0.8677 - val_loss: 0.5542 - val_acc: 0.8976\n",
      "Epoch 283/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6311 - acc: 0.8694 - val_loss: 0.5712 - val_acc: 0.8945\n",
      "Epoch 284/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6333 - acc: 0.8678 - val_loss: 0.5693 - val_acc: 0.8967\n",
      "Epoch 285/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6324 - acc: 0.8678 - val_loss: 0.5774 - val_acc: 0.8891\n",
      "Epoch 286/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6321 - acc: 0.8688 - val_loss: 0.5848 - val_acc: 0.8861\n",
      "Epoch 287/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6345 - acc: 0.8670 - val_loss: 0.5546 - val_acc: 0.8982\n",
      "Epoch 288/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6352 - acc: 0.8678 - val_loss: 0.5636 - val_acc: 0.8965\n",
      "Epoch 289/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6330 - acc: 0.8684 - val_loss: 0.5738 - val_acc: 0.8931\n",
      "Epoch 290/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6305 - acc: 0.8684 - val_loss: 0.5770 - val_acc: 0.8900\n",
      "Epoch 291/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6318 - acc: 0.8696 - val_loss: 0.5727 - val_acc: 0.8944\n",
      "Epoch 292/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6347 - acc: 0.8682 - val_loss: 0.5736 - val_acc: 0.8923\n",
      "Epoch 293/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6301 - acc: 0.8684 - val_loss: 0.5631 - val_acc: 0.8968\n",
      "Epoch 294/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6357 - acc: 0.8690 - val_loss: 0.5796 - val_acc: 0.8906\n",
      "Epoch 295/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6311 - acc: 0.8689 - val_loss: 0.5770 - val_acc: 0.8892\n",
      "Epoch 296/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6314 - acc: 0.8687 - val_loss: 0.5772 - val_acc: 0.8912\n",
      "Epoch 297/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6329 - acc: 0.8691 - val_loss: 0.5734 - val_acc: 0.8942\n",
      "Epoch 298/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6331 - acc: 0.8693 - val_loss: 0.5756 - val_acc: 0.8908\n",
      "Epoch 299/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6325 - acc: 0.8690 - val_loss: 0.5659 - val_acc: 0.8968\n",
      "Epoch 300/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6348 - acc: 0.8677 - val_loss: 0.5608 - val_acc: 0.8986\n",
      "Epoch 301/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6308 - acc: 0.8698 - val_loss: 0.5659 - val_acc: 0.8947\n",
      "Epoch 302/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6318 - acc: 0.8690 - val_loss: 0.5690 - val_acc: 0.8936\n",
      "Epoch 303/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6339 - acc: 0.8665 - val_loss: 0.5605 - val_acc: 0.8975\n",
      "Epoch 304/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6306 - acc: 0.8688 - val_loss: 0.5619 - val_acc: 0.8978\n",
      "Epoch 305/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6281 - acc: 0.8696 - val_loss: 0.5723 - val_acc: 0.8934\n",
      "Epoch 306/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6276 - acc: 0.8703 - val_loss: 0.5630 - val_acc: 0.8952\n",
      "Epoch 307/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6291 - acc: 0.8703 - val_loss: 0.5639 - val_acc: 0.8948\n",
      "Epoch 308/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6323 - acc: 0.8693 - val_loss: 0.5615 - val_acc: 0.8968\n",
      "Epoch 309/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6326 - acc: 0.8679 - val_loss: 0.5715 - val_acc: 0.8935\n",
      "Epoch 310/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6273 - acc: 0.8693 - val_loss: 0.5670 - val_acc: 0.8937\n",
      "Epoch 311/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6304 - acc: 0.8685 - val_loss: 0.5578 - val_acc: 0.8966\n",
      "Epoch 312/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6304 - acc: 0.8682 - val_loss: 0.5716 - val_acc: 0.8917\n",
      "Epoch 313/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6325 - acc: 0.8683 - val_loss: 0.5633 - val_acc: 0.8956\n",
      "Epoch 314/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6286 - acc: 0.8699 - val_loss: 0.5821 - val_acc: 0.8887\n",
      "Epoch 315/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6296 - acc: 0.8688 - val_loss: 0.5830 - val_acc: 0.8890\n",
      "Epoch 316/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6282 - acc: 0.8699 - val_loss: 0.5705 - val_acc: 0.8931\n",
      "Epoch 317/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6258 - acc: 0.8710 - val_loss: 0.5593 - val_acc: 0.8963\n",
      "Epoch 318/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6280 - acc: 0.8699 - val_loss: 0.5825 - val_acc: 0.8895\n",
      "Epoch 319/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6284 - acc: 0.8706 - val_loss: 0.5619 - val_acc: 0.8953\n",
      "Epoch 320/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6229 - acc: 0.8717 - val_loss: 0.5736 - val_acc: 0.8921\n",
      "Epoch 321/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6291 - acc: 0.8708 - val_loss: 0.5720 - val_acc: 0.8909\n",
      "Epoch 322/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6226 - acc: 0.8715 - val_loss: 0.5600 - val_acc: 0.8966\n",
      "Epoch 323/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6294 - acc: 0.8703 - val_loss: 0.5744 - val_acc: 0.8910\n",
      "Epoch 324/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6267 - acc: 0.8703 - val_loss: 0.5623 - val_acc: 0.8960\n",
      "Epoch 325/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6282 - acc: 0.8707 - val_loss: 0.5615 - val_acc: 0.8967\n",
      "Epoch 326/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6233 - acc: 0.8703 - val_loss: 0.5761 - val_acc: 0.8902\n",
      "Epoch 327/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6279 - acc: 0.8687 - val_loss: 0.5619 - val_acc: 0.8956\n",
      "Epoch 328/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6270 - acc: 0.8702 - val_loss: 0.5673 - val_acc: 0.8944\n",
      "Epoch 329/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6260 - acc: 0.8712 - val_loss: 0.5731 - val_acc: 0.8915\n",
      "Epoch 330/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6295 - acc: 0.8683 - val_loss: 0.5730 - val_acc: 0.8900\n",
      "Epoch 331/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6311 - acc: 0.8690 - val_loss: 0.5750 - val_acc: 0.8909\n",
      "Epoch 332/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6240 - acc: 0.8718 - val_loss: 0.5526 - val_acc: 0.9001\n",
      "Epoch 333/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6272 - acc: 0.8704 - val_loss: 0.5633 - val_acc: 0.8953\n",
      "Epoch 334/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6253 - acc: 0.8697 - val_loss: 0.5757 - val_acc: 0.8928\n",
      "Epoch 335/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6282 - acc: 0.8701 - val_loss: 0.5652 - val_acc: 0.8954\n",
      "Epoch 336/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6254 - acc: 0.8700 - val_loss: 0.5719 - val_acc: 0.8923\n",
      "Epoch 337/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6273 - acc: 0.8706 - val_loss: 0.5527 - val_acc: 0.8983\n",
      "Epoch 338/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6260 - acc: 0.8699 - val_loss: 0.5657 - val_acc: 0.8958\n",
      "Epoch 339/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6245 - acc: 0.8693 - val_loss: 0.5670 - val_acc: 0.8941\n",
      "Epoch 340/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6254 - acc: 0.8703 - val_loss: 0.5685 - val_acc: 0.8949\n",
      "Epoch 341/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6266 - acc: 0.8692 - val_loss: 0.5612 - val_acc: 0.8968\n",
      "Epoch 342/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6270 - acc: 0.8707 - val_loss: 0.5662 - val_acc: 0.8926\n",
      "Epoch 343/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6268 - acc: 0.8708 - val_loss: 0.5609 - val_acc: 0.8961\n",
      "Epoch 344/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6257 - acc: 0.8696 - val_loss: 0.5641 - val_acc: 0.8961\n",
      "Epoch 345/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6255 - acc: 0.8695 - val_loss: 0.5624 - val_acc: 0.8959\n",
      "Epoch 346/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6254 - acc: 0.8699 - val_loss: 0.5564 - val_acc: 0.8969\n",
      "Epoch 347/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6265 - acc: 0.8694 - val_loss: 0.5704 - val_acc: 0.8937\n",
      "Epoch 348/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6297 - acc: 0.8682 - val_loss: 0.5590 - val_acc: 0.8981\n",
      "Epoch 349/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6242 - acc: 0.8708 - val_loss: 0.5592 - val_acc: 0.8975\n",
      "Epoch 350/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6221 - acc: 0.8716 - val_loss: 0.5692 - val_acc: 0.8929\n",
      "Epoch 351/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6266 - acc: 0.8681 - val_loss: 0.5844 - val_acc: 0.8879\n",
      "Epoch 352/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6231 - acc: 0.8703 - val_loss: 0.5620 - val_acc: 0.8957\n",
      "Epoch 353/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6223 - acc: 0.8719 - val_loss: 0.5601 - val_acc: 0.8975\n",
      "Epoch 354/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6268 - acc: 0.8705 - val_loss: 0.5536 - val_acc: 0.8997\n",
      "Epoch 355/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6251 - acc: 0.8699 - val_loss: 0.5539 - val_acc: 0.8977\n",
      "Epoch 356/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6234 - acc: 0.8709 - val_loss: 0.5602 - val_acc: 0.8954\n",
      "Epoch 357/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6244 - acc: 0.8707 - val_loss: 0.5771 - val_acc: 0.8897\n",
      "Epoch 358/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6240 - acc: 0.8711 - val_loss: 0.5598 - val_acc: 0.8985\n",
      "Epoch 359/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6205 - acc: 0.8720 - val_loss: 0.5709 - val_acc: 0.8912\n",
      "Epoch 360/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6240 - acc: 0.8716 - val_loss: 0.5920 - val_acc: 0.8855\n",
      "Epoch 361/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6244 - acc: 0.8709 - val_loss: 0.5636 - val_acc: 0.8966\n",
      "Epoch 362/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6206 - acc: 0.8710 - val_loss: 0.5706 - val_acc: 0.8940\n",
      "Epoch 363/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6241 - acc: 0.8708 - val_loss: 0.5717 - val_acc: 0.8919\n",
      "Epoch 364/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6244 - acc: 0.8712 - val_loss: 0.5729 - val_acc: 0.8913\n",
      "Epoch 365/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6230 - acc: 0.8701 - val_loss: 0.5640 - val_acc: 0.8968\n",
      "Epoch 366/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6198 - acc: 0.8712 - val_loss: 0.5584 - val_acc: 0.8959\n",
      "Epoch 367/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6241 - acc: 0.8709 - val_loss: 0.5546 - val_acc: 0.8977\n",
      "Epoch 368/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6259 - acc: 0.8704 - val_loss: 0.5642 - val_acc: 0.8955\n",
      "Epoch 369/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6227 - acc: 0.8711 - val_loss: 0.5726 - val_acc: 0.8908\n",
      "Epoch 370/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6200 - acc: 0.8707 - val_loss: 0.5617 - val_acc: 0.8957\n",
      "Epoch 371/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6224 - acc: 0.8715 - val_loss: 0.5502 - val_acc: 0.8987\n",
      "Epoch 372/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6196 - acc: 0.8714 - val_loss: 0.5550 - val_acc: 0.8966\n",
      "Epoch 373/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6198 - acc: 0.8719 - val_loss: 0.5629 - val_acc: 0.8955\n",
      "Epoch 374/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6189 - acc: 0.8722 - val_loss: 0.5534 - val_acc: 0.8988\n",
      "Epoch 375/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6219 - acc: 0.8709 - val_loss: 0.5552 - val_acc: 0.8967\n",
      "Epoch 376/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6224 - acc: 0.8702 - val_loss: 0.5741 - val_acc: 0.8904\n",
      "Epoch 377/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6219 - acc: 0.8711 - val_loss: 0.5743 - val_acc: 0.8924\n",
      "Epoch 378/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6246 - acc: 0.8698 - val_loss: 0.5676 - val_acc: 0.8945\n",
      "Epoch 379/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6240 - acc: 0.8715 - val_loss: 0.5674 - val_acc: 0.8920\n",
      "Epoch 380/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6228 - acc: 0.8691 - val_loss: 0.5535 - val_acc: 0.8981\n",
      "Epoch 381/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6205 - acc: 0.8714 - val_loss: 0.5617 - val_acc: 0.8953\n",
      "Epoch 382/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6183 - acc: 0.8719 - val_loss: 0.5606 - val_acc: 0.8956\n",
      "Epoch 383/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6226 - acc: 0.8702 - val_loss: 0.5638 - val_acc: 0.8947\n",
      "Epoch 384/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6240 - acc: 0.8703 - val_loss: 0.5771 - val_acc: 0.8880\n",
      "Epoch 385/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6220 - acc: 0.8720 - val_loss: 0.5654 - val_acc: 0.8924\n",
      "Epoch 386/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6210 - acc: 0.8705 - val_loss: 0.5463 - val_acc: 0.8993\n",
      "Epoch 387/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6225 - acc: 0.8693 - val_loss: 0.5797 - val_acc: 0.8897\n",
      "Epoch 388/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6188 - acc: 0.8730 - val_loss: 0.5656 - val_acc: 0.8949\n",
      "Epoch 389/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6148 - acc: 0.8730 - val_loss: 0.5565 - val_acc: 0.8984\n",
      "Epoch 390/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6177 - acc: 0.8714 - val_loss: 0.5677 - val_acc: 0.8925\n",
      "Epoch 391/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6181 - acc: 0.8731 - val_loss: 0.5869 - val_acc: 0.8866\n",
      "Epoch 392/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6180 - acc: 0.8715 - val_loss: 0.5418 - val_acc: 0.9020\n",
      "Epoch 393/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6188 - acc: 0.8718 - val_loss: 0.5633 - val_acc: 0.8965\n",
      "Epoch 394/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6174 - acc: 0.8707 - val_loss: 0.5465 - val_acc: 0.9010\n",
      "Epoch 395/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6209 - acc: 0.8712 - val_loss: 0.5436 - val_acc: 0.9029\n",
      "Epoch 396/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6219 - acc: 0.8710 - val_loss: 0.5503 - val_acc: 0.8985\n",
      "Epoch 397/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6157 - acc: 0.8726 - val_loss: 0.5660 - val_acc: 0.8926\n",
      "Epoch 398/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6173 - acc: 0.8717 - val_loss: 0.5646 - val_acc: 0.8944\n",
      "Epoch 399/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6189 - acc: 0.8719 - val_loss: 0.5659 - val_acc: 0.8946\n",
      "Epoch 400/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6192 - acc: 0.8709 - val_loss: 0.5471 - val_acc: 0.8999\n",
      "Epoch 401/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6210 - acc: 0.8716 - val_loss: 0.5689 - val_acc: 0.8939\n",
      "Epoch 402/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6180 - acc: 0.8710 - val_loss: 0.5453 - val_acc: 0.9006\n",
      "Epoch 403/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6156 - acc: 0.8723 - val_loss: 0.5495 - val_acc: 0.8995\n",
      "Epoch 404/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6178 - acc: 0.8709 - val_loss: 0.5527 - val_acc: 0.8968\n",
      "Epoch 405/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6215 - acc: 0.8691 - val_loss: 0.5759 - val_acc: 0.8910\n",
      "Epoch 406/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6152 - acc: 0.8718 - val_loss: 0.5647 - val_acc: 0.8913\n",
      "Epoch 407/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6150 - acc: 0.8724 - val_loss: 0.5573 - val_acc: 0.8969\n",
      "Epoch 408/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6188 - acc: 0.8712 - val_loss: 0.5524 - val_acc: 0.8980\n",
      "Epoch 409/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6191 - acc: 0.8712 - val_loss: 0.5571 - val_acc: 0.8969\n",
      "Epoch 410/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6161 - acc: 0.8721 - val_loss: 0.5526 - val_acc: 0.8980\n",
      "Epoch 411/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6161 - acc: 0.8732 - val_loss: 0.5403 - val_acc: 0.9017\n",
      "Epoch 412/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6152 - acc: 0.8725 - val_loss: 0.5516 - val_acc: 0.8991\n",
      "Epoch 413/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6184 - acc: 0.8714 - val_loss: 0.5860 - val_acc: 0.8900\n",
      "Epoch 414/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6162 - acc: 0.8717 - val_loss: 0.5571 - val_acc: 0.8964\n",
      "Epoch 415/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6186 - acc: 0.8718 - val_loss: 0.5481 - val_acc: 0.8996\n",
      "Epoch 416/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6166 - acc: 0.8720 - val_loss: 0.5472 - val_acc: 0.9000\n",
      "Epoch 417/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6131 - acc: 0.8739 - val_loss: 0.5628 - val_acc: 0.8943\n",
      "Epoch 418/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6165 - acc: 0.8729 - val_loss: 0.5640 - val_acc: 0.8941\n",
      "Epoch 419/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6135 - acc: 0.8729 - val_loss: 0.5537 - val_acc: 0.8959\n",
      "Epoch 420/500\n",
      "71400/71400 [==============================] - 5s 67us/sample - loss: 0.6153 - acc: 0.8722 - val_loss: 0.5670 - val_acc: 0.8917\n",
      "Epoch 421/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6136 - acc: 0.8730 - val_loss: 0.5530 - val_acc: 0.8996\n",
      "Epoch 422/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6140 - acc: 0.8725 - val_loss: 0.5706 - val_acc: 0.8884\n",
      "Epoch 423/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6162 - acc: 0.8717 - val_loss: 0.5435 - val_acc: 0.9017\n",
      "Epoch 424/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6161 - acc: 0.8712 - val_loss: 0.5490 - val_acc: 0.8981\n",
      "Epoch 425/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6158 - acc: 0.8726 - val_loss: 0.5490 - val_acc: 0.8990\n",
      "Epoch 426/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6146 - acc: 0.8729 - val_loss: 0.5504 - val_acc: 0.8982\n",
      "Epoch 427/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6167 - acc: 0.8728 - val_loss: 0.5637 - val_acc: 0.8929\n",
      "Epoch 428/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6120 - acc: 0.8732 - val_loss: 0.5528 - val_acc: 0.8970\n",
      "Epoch 429/500\n",
      "71400/71400 [==============================] - 4s 60us/sample - loss: 0.6119 - acc: 0.8743 - val_loss: 0.5575 - val_acc: 0.8979\n",
      "Epoch 430/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6148 - acc: 0.8722 - val_loss: 0.5353 - val_acc: 0.9029\n",
      "Epoch 431/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6106 - acc: 0.8740 - val_loss: 0.5695 - val_acc: 0.8925\n",
      "Epoch 432/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6164 - acc: 0.8718 - val_loss: 0.5651 - val_acc: 0.8921\n",
      "Epoch 433/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6142 - acc: 0.8733 - val_loss: 0.5568 - val_acc: 0.8975\n",
      "Epoch 434/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6141 - acc: 0.8716 - val_loss: 0.5454 - val_acc: 0.8997\n",
      "Epoch 435/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6074 - acc: 0.8747 - val_loss: 0.5610 - val_acc: 0.8958\n",
      "Epoch 436/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6115 - acc: 0.8718 - val_loss: 0.5589 - val_acc: 0.8970\n",
      "Epoch 437/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6111 - acc: 0.8734 - val_loss: 0.5457 - val_acc: 0.9011\n",
      "Epoch 438/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6138 - acc: 0.8731 - val_loss: 0.5541 - val_acc: 0.8967\n",
      "Epoch 439/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6147 - acc: 0.8730 - val_loss: 0.5440 - val_acc: 0.9003\n",
      "Epoch 440/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6133 - acc: 0.8733 - val_loss: 0.5566 - val_acc: 0.8974\n",
      "Epoch 441/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6106 - acc: 0.8738 - val_loss: 0.5518 - val_acc: 0.8959\n",
      "Epoch 442/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6123 - acc: 0.8739 - val_loss: 0.5530 - val_acc: 0.8986\n",
      "Epoch 443/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6110 - acc: 0.8733 - val_loss: 0.5487 - val_acc: 0.8993\n",
      "Epoch 444/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6138 - acc: 0.8720 - val_loss: 0.5448 - val_acc: 0.8998\n",
      "Epoch 445/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6152 - acc: 0.8726 - val_loss: 0.5520 - val_acc: 0.9002\n",
      "Epoch 446/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6063 - acc: 0.8758 - val_loss: 0.5417 - val_acc: 0.9018\n",
      "Epoch 447/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6104 - acc: 0.8749 - val_loss: 0.5696 - val_acc: 0.8930\n",
      "Epoch 448/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6134 - acc: 0.8730 - val_loss: 0.5534 - val_acc: 0.8974\n",
      "Epoch 449/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6121 - acc: 0.8729 - val_loss: 0.5630 - val_acc: 0.8944\n",
      "Epoch 450/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6128 - acc: 0.8730 - val_loss: 0.5486 - val_acc: 0.8982\n",
      "Epoch 451/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6103 - acc: 0.8745 - val_loss: 0.5528 - val_acc: 0.8957\n",
      "Epoch 452/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6128 - acc: 0.8736 - val_loss: 0.5567 - val_acc: 0.8947\n",
      "Epoch 453/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6118 - acc: 0.8733 - val_loss: 0.5566 - val_acc: 0.8960\n",
      "Epoch 454/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6069 - acc: 0.8745 - val_loss: 0.5477 - val_acc: 0.9011\n",
      "Epoch 455/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6121 - acc: 0.8719 - val_loss: 0.5518 - val_acc: 0.8970\n",
      "Epoch 456/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6096 - acc: 0.8735 - val_loss: 0.5572 - val_acc: 0.8963\n",
      "Epoch 457/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6110 - acc: 0.8735 - val_loss: 0.5534 - val_acc: 0.8986\n",
      "Epoch 458/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6159 - acc: 0.8719 - val_loss: 0.5637 - val_acc: 0.8922\n",
      "Epoch 459/500\n",
      "71400/71400 [==============================] - 4s 60us/sample - loss: 0.6072 - acc: 0.8758 - val_loss: 0.5657 - val_acc: 0.8926\n",
      "Epoch 460/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6100 - acc: 0.8735 - val_loss: 0.5541 - val_acc: 0.8978\n",
      "Epoch 461/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6127 - acc: 0.8732 - val_loss: 0.5397 - val_acc: 0.9021\n",
      "Epoch 462/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6066 - acc: 0.8754 - val_loss: 0.5586 - val_acc: 0.8956\n",
      "Epoch 463/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6077 - acc: 0.8727 - val_loss: 0.5569 - val_acc: 0.8965\n",
      "Epoch 464/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6052 - acc: 0.8747 - val_loss: 0.5483 - val_acc: 0.8980\n",
      "Epoch 465/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6116 - acc: 0.8730 - val_loss: 0.5509 - val_acc: 0.8990\n",
      "Epoch 466/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6098 - acc: 0.8739 - val_loss: 0.5607 - val_acc: 0.8920\n",
      "Epoch 467/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6104 - acc: 0.8737 - val_loss: 0.5486 - val_acc: 0.8995\n",
      "Epoch 468/500\n",
      "71400/71400 [==============================] - 4s 60us/sample - loss: 0.6096 - acc: 0.8738 - val_loss: 0.5712 - val_acc: 0.8914\n",
      "Epoch 469/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6114 - acc: 0.8739 - val_loss: 0.5489 - val_acc: 0.8985\n",
      "Epoch 470/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6098 - acc: 0.8745 - val_loss: 0.5643 - val_acc: 0.8903\n",
      "Epoch 471/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6116 - acc: 0.8735 - val_loss: 0.5648 - val_acc: 0.8933\n",
      "Epoch 472/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6105 - acc: 0.8736 - val_loss: 0.5585 - val_acc: 0.8958\n",
      "Epoch 473/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6071 - acc: 0.8746 - val_loss: 0.5576 - val_acc: 0.8959\n",
      "Epoch 474/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6089 - acc: 0.8740 - val_loss: 0.5512 - val_acc: 0.9000\n",
      "Epoch 475/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6081 - acc: 0.8743 - val_loss: 0.5499 - val_acc: 0.8992\n",
      "Epoch 476/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6113 - acc: 0.8733 - val_loss: 0.5499 - val_acc: 0.8955\n",
      "Epoch 477/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6089 - acc: 0.8734 - val_loss: 0.5538 - val_acc: 0.8971\n",
      "Epoch 478/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6093 - acc: 0.8738 - val_loss: 0.5559 - val_acc: 0.8979\n",
      "Epoch 479/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6111 - acc: 0.8733 - val_loss: 0.5409 - val_acc: 0.9019\n",
      "Epoch 480/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6101 - acc: 0.8743 - val_loss: 0.5494 - val_acc: 0.8989\n",
      "Epoch 481/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6068 - acc: 0.8750 - val_loss: 0.5422 - val_acc: 0.9007\n",
      "Epoch 482/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6070 - acc: 0.8755 - val_loss: 0.5532 - val_acc: 0.8971\n",
      "Epoch 483/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6060 - acc: 0.8745 - val_loss: 0.5659 - val_acc: 0.8924\n",
      "Epoch 484/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6082 - acc: 0.8739 - val_loss: 0.5593 - val_acc: 0.8950\n",
      "Epoch 485/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6104 - acc: 0.8732 - val_loss: 0.5595 - val_acc: 0.8949\n",
      "Epoch 486/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6088 - acc: 0.8746 - val_loss: 0.5509 - val_acc: 0.8982\n",
      "Epoch 487/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6084 - acc: 0.8732 - val_loss: 0.5427 - val_acc: 0.9011\n",
      "Epoch 488/500\n",
      "71400/71400 [==============================] - 5s 63us/sample - loss: 0.6060 - acc: 0.8760 - val_loss: 0.5548 - val_acc: 0.8974\n",
      "Epoch 489/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6064 - acc: 0.8757 - val_loss: 0.5638 - val_acc: 0.8925\n",
      "Epoch 490/500\n",
      "71400/71400 [==============================] - 5s 67us/sample - loss: 0.6073 - acc: 0.8761 - val_loss: 0.5534 - val_acc: 0.8970\n",
      "Epoch 491/500\n",
      "71400/71400 [==============================] - 5s 64us/sample - loss: 0.6034 - acc: 0.8755 - val_loss: 0.5422 - val_acc: 0.9014\n",
      "Epoch 492/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6093 - acc: 0.8734 - val_loss: 0.5497 - val_acc: 0.8965\n",
      "Epoch 493/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6045 - acc: 0.8752 - val_loss: 0.5589 - val_acc: 0.8930\n",
      "Epoch 494/500\n",
      "71400/71400 [==============================] - 4s 63us/sample - loss: 0.6061 - acc: 0.8744 - val_loss: 0.5451 - val_acc: 0.8993\n",
      "Epoch 495/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6046 - acc: 0.8755 - val_loss: 0.5520 - val_acc: 0.8966\n",
      "Epoch 496/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6076 - acc: 0.8746 - val_loss: 0.5721 - val_acc: 0.8914\n",
      "Epoch 497/500\n",
      "71400/71400 [==============================] - 4s 61us/sample - loss: 0.6063 - acc: 0.8737 - val_loss: 0.5410 - val_acc: 0.9009\n",
      "Epoch 498/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6083 - acc: 0.8744 - val_loss: 0.5451 - val_acc: 0.8992\n",
      "Epoch 499/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6043 - acc: 0.8759 - val_loss: 0.5456 - val_acc: 0.8998\n",
      "Epoch 500/500\n",
      "71400/71400 [==============================] - 4s 62us/sample - loss: 0.6064 - acc: 0.8750 - val_loss: 0.5494 - val_acc: 0.8995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f135a8637b8>"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Learning_rate= 0.0004992788270625213\n",
    "neurons=[350,250,150,10]\n",
    "Regularize= 0.0010370185781516902\n",
    "Activation=['relu','relu','relu','softmax']\n",
    "do=0.22800119704264665\n",
    "Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "\n",
    "nmodel_1=tf.keras.models.Sequential()\n",
    "nmodel_1.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "for i in range(0,len(neurons)):\n",
    "  nmodel_1.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "  if i != (len(neurons)-1):\n",
    "    nmodel_1.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "nmodel_1.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "nmodel_1.fit(nnfX_train,HHy_train,validation_data=(nnfX_val,HHy_val),batch_size=128,epochs=500,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4uKX5J605L1m"
   },
   "source": [
    "The Val accuracy is 0.8995 which is less than our model 1,2,3\n",
    "\n",
    "Lets train this data set in same hyperparameters as model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CNEyXvmG_MWE",
    "outputId": "54d09ba8-8fe8-48a3-a606-5a2d1204b72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71400 samples, validate on 30600 samples\n",
      "Epoch 1/500\n",
      "71400/71400 [==============================] - 11s 158us/sample - loss: 1.2705 - acc: 0.5897 - val_loss: 0.8659 - val_acc: 0.7279\n",
      "Epoch 2/500\n",
      "71400/71400 [==============================] - 7s 92us/sample - loss: 0.7897 - acc: 0.7514 - val_loss: 0.6746 - val_acc: 0.7941\n",
      "Epoch 3/500\n",
      "71400/71400 [==============================] - 7s 91us/sample - loss: 0.6578 - acc: 0.7950 - val_loss: 0.6154 - val_acc: 0.8130\n",
      "Epoch 4/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.5806 - acc: 0.8178 - val_loss: 0.5564 - val_acc: 0.8290\n",
      "Epoch 5/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.5322 - acc: 0.8334 - val_loss: 0.5122 - val_acc: 0.8461\n",
      "Epoch 6/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.4965 - acc: 0.8435 - val_loss: 0.4798 - val_acc: 0.8527\n",
      "Epoch 7/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.4646 - acc: 0.8541 - val_loss: 0.4735 - val_acc: 0.8577\n",
      "Epoch 8/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.4312 - acc: 0.8635 - val_loss: 0.4368 - val_acc: 0.8701\n",
      "Epoch 9/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.4094 - acc: 0.8715 - val_loss: 0.4357 - val_acc: 0.8689\n",
      "Epoch 10/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.3840 - acc: 0.8800 - val_loss: 0.4193 - val_acc: 0.8750\n",
      "Epoch 11/500\n",
      "71400/71400 [==============================] - 7s 91us/sample - loss: 0.3650 - acc: 0.8838 - val_loss: 0.4241 - val_acc: 0.8712\n",
      "Epoch 12/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.3506 - acc: 0.8881 - val_loss: 0.4015 - val_acc: 0.8810\n",
      "Epoch 13/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.3384 - acc: 0.8915 - val_loss: 0.3928 - val_acc: 0.8854\n",
      "Epoch 14/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.3202 - acc: 0.8976 - val_loss: 0.3826 - val_acc: 0.8869\n",
      "Epoch 15/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.3046 - acc: 0.9015 - val_loss: 0.3649 - val_acc: 0.8935\n",
      "Epoch 16/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2989 - acc: 0.9040 - val_loss: 0.3534 - val_acc: 0.8975\n",
      "Epoch 17/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2861 - acc: 0.9068 - val_loss: 0.3643 - val_acc: 0.8923\n",
      "Epoch 18/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.2736 - acc: 0.9117 - val_loss: 0.3530 - val_acc: 0.9003\n",
      "Epoch 19/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2646 - acc: 0.9160 - val_loss: 0.3450 - val_acc: 0.8988\n",
      "Epoch 20/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2536 - acc: 0.9176 - val_loss: 0.3239 - val_acc: 0.9089\n",
      "Epoch 21/500\n",
      "71400/71400 [==============================] - 7s 92us/sample - loss: 0.2453 - acc: 0.9198 - val_loss: 0.3373 - val_acc: 0.9040\n",
      "Epoch 22/500\n",
      "71400/71400 [==============================] - 7s 93us/sample - loss: 0.2371 - acc: 0.9229 - val_loss: 0.3208 - val_acc: 0.9098\n",
      "Epoch 23/500\n",
      "71400/71400 [==============================] - 7s 94us/sample - loss: 0.2320 - acc: 0.9247 - val_loss: 0.3323 - val_acc: 0.9066\n",
      "Epoch 24/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2267 - acc: 0.9268 - val_loss: 0.3259 - val_acc: 0.9102\n",
      "Epoch 25/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.2141 - acc: 0.9301 - val_loss: 0.3149 - val_acc: 0.9136\n",
      "Epoch 26/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.2086 - acc: 0.9323 - val_loss: 0.3170 - val_acc: 0.9123\n",
      "Epoch 27/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2081 - acc: 0.9324 - val_loss: 0.3280 - val_acc: 0.9104\n",
      "Epoch 28/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.2024 - acc: 0.9340 - val_loss: 0.3298 - val_acc: 0.9089\n",
      "Epoch 29/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1966 - acc: 0.9364 - val_loss: 0.3014 - val_acc: 0.9192\n",
      "Epoch 30/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1918 - acc: 0.9370 - val_loss: 0.3000 - val_acc: 0.9185\n",
      "Epoch 31/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1833 - acc: 0.9404 - val_loss: 0.2940 - val_acc: 0.9210\n",
      "Epoch 32/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1765 - acc: 0.9427 - val_loss: 0.3014 - val_acc: 0.9196\n",
      "Epoch 33/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1762 - acc: 0.9418 - val_loss: 0.3006 - val_acc: 0.9239\n",
      "Epoch 34/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.1693 - acc: 0.9450 - val_loss: 0.3200 - val_acc: 0.9163\n",
      "Epoch 35/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1724 - acc: 0.9430 - val_loss: 0.2946 - val_acc: 0.9251\n",
      "Epoch 36/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1665 - acc: 0.9451 - val_loss: 0.2965 - val_acc: 0.9228\n",
      "Epoch 37/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.1602 - acc: 0.9474 - val_loss: 0.2891 - val_acc: 0.9277\n",
      "Epoch 38/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.1570 - acc: 0.9478 - val_loss: 0.2957 - val_acc: 0.9251\n",
      "Epoch 39/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1565 - acc: 0.9482 - val_loss: 0.2871 - val_acc: 0.9274\n",
      "Epoch 40/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.1472 - acc: 0.9515 - val_loss: 0.2937 - val_acc: 0.9252\n",
      "Epoch 41/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.1496 - acc: 0.9509 - val_loss: 0.2971 - val_acc: 0.9253\n",
      "Epoch 42/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1452 - acc: 0.9523 - val_loss: 0.2911 - val_acc: 0.9282\n",
      "Epoch 43/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1429 - acc: 0.9524 - val_loss: 0.2904 - val_acc: 0.9283\n",
      "Epoch 44/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.1409 - acc: 0.9534 - val_loss: 0.2787 - val_acc: 0.9322\n",
      "Epoch 45/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1385 - acc: 0.9533 - val_loss: 0.2868 - val_acc: 0.9301\n",
      "Epoch 46/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1360 - acc: 0.9545 - val_loss: 0.2921 - val_acc: 0.9293\n",
      "Epoch 47/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.1294 - acc: 0.9571 - val_loss: 0.3008 - val_acc: 0.9276\n",
      "Epoch 48/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1334 - acc: 0.9559 - val_loss: 0.2887 - val_acc: 0.9306\n",
      "Epoch 49/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1290 - acc: 0.9579 - val_loss: 0.2783 - val_acc: 0.9338\n",
      "Epoch 50/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1263 - acc: 0.9575 - val_loss: 0.2861 - val_acc: 0.9310\n",
      "Epoch 51/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1253 - acc: 0.9587 - val_loss: 0.2848 - val_acc: 0.9311\n",
      "Epoch 52/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1224 - acc: 0.9591 - val_loss: 0.2833 - val_acc: 0.9339\n",
      "Epoch 53/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1211 - acc: 0.9595 - val_loss: 0.2859 - val_acc: 0.9324\n",
      "Epoch 54/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1199 - acc: 0.9605 - val_loss: 0.2907 - val_acc: 0.9325\n",
      "Epoch 55/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1185 - acc: 0.9606 - val_loss: 0.2822 - val_acc: 0.9340\n",
      "Epoch 56/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1152 - acc: 0.9611 - val_loss: 0.2855 - val_acc: 0.9332\n",
      "Epoch 57/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1150 - acc: 0.9613 - val_loss: 0.2812 - val_acc: 0.9328\n",
      "Epoch 58/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1119 - acc: 0.9635 - val_loss: 0.2820 - val_acc: 0.9351\n",
      "Epoch 59/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1098 - acc: 0.9636 - val_loss: 0.2921 - val_acc: 0.9322\n",
      "Epoch 60/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1119 - acc: 0.9621 - val_loss: 0.2899 - val_acc: 0.9345\n",
      "Epoch 61/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1071 - acc: 0.9643 - val_loss: 0.2779 - val_acc: 0.9363\n",
      "Epoch 62/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1095 - acc: 0.9633 - val_loss: 0.2946 - val_acc: 0.9341\n",
      "Epoch 63/500\n",
      "71400/71400 [==============================] - 6s 91us/sample - loss: 0.1057 - acc: 0.9649 - val_loss: 0.2804 - val_acc: 0.9370\n",
      "Epoch 64/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.1068 - acc: 0.9644 - val_loss: 0.2824 - val_acc: 0.9382\n",
      "Epoch 65/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1014 - acc: 0.9658 - val_loss: 0.2888 - val_acc: 0.9358\n",
      "Epoch 66/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.1011 - acc: 0.9668 - val_loss: 0.2839 - val_acc: 0.9380\n",
      "Epoch 67/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.1034 - acc: 0.9652 - val_loss: 0.2884 - val_acc: 0.9367\n",
      "Epoch 68/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0982 - acc: 0.9675 - val_loss: 0.2893 - val_acc: 0.9366\n",
      "Epoch 69/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0977 - acc: 0.9674 - val_loss: 0.2792 - val_acc: 0.9404\n",
      "Epoch 70/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0959 - acc: 0.9685 - val_loss: 0.2922 - val_acc: 0.9369\n",
      "Epoch 71/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0983 - acc: 0.9671 - val_loss: 0.2951 - val_acc: 0.9365\n",
      "Epoch 72/500\n",
      "71400/71400 [==============================] - 7s 92us/sample - loss: 0.0994 - acc: 0.9668 - val_loss: 0.2929 - val_acc: 0.9371\n",
      "Epoch 73/500\n",
      "71400/71400 [==============================] - 6s 91us/sample - loss: 0.0952 - acc: 0.9687 - val_loss: 0.3037 - val_acc: 0.9335\n",
      "Epoch 74/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0944 - acc: 0.9688 - val_loss: 0.2799 - val_acc: 0.9405\n",
      "Epoch 75/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0905 - acc: 0.9695 - val_loss: 0.2888 - val_acc: 0.9383\n",
      "Epoch 76/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0918 - acc: 0.9693 - val_loss: 0.2822 - val_acc: 0.9399\n",
      "Epoch 77/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0937 - acc: 0.9680 - val_loss: 0.2822 - val_acc: 0.9413\n",
      "Epoch 78/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0929 - acc: 0.9689 - val_loss: 0.2947 - val_acc: 0.9372\n",
      "Epoch 79/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0920 - acc: 0.9692 - val_loss: 0.2837 - val_acc: 0.9413\n",
      "Epoch 80/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0905 - acc: 0.9702 - val_loss: 0.2867 - val_acc: 0.9387\n",
      "Epoch 81/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0869 - acc: 0.9710 - val_loss: 0.2940 - val_acc: 0.9377\n",
      "Epoch 82/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0877 - acc: 0.9704 - val_loss: 0.2793 - val_acc: 0.9427\n",
      "Epoch 83/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0866 - acc: 0.9710 - val_loss: 0.2961 - val_acc: 0.9373\n",
      "Epoch 84/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0858 - acc: 0.9711 - val_loss: 0.2935 - val_acc: 0.9399\n",
      "Epoch 85/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0876 - acc: 0.9702 - val_loss: 0.2895 - val_acc: 0.9396\n",
      "Epoch 86/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0834 - acc: 0.9716 - val_loss: 0.2976 - val_acc: 0.9380\n",
      "Epoch 87/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0817 - acc: 0.9728 - val_loss: 0.2947 - val_acc: 0.9389\n",
      "Epoch 88/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0805 - acc: 0.9732 - val_loss: 0.2850 - val_acc: 0.9424\n",
      "Epoch 89/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0802 - acc: 0.9731 - val_loss: 0.3001 - val_acc: 0.9384\n",
      "Epoch 90/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0799 - acc: 0.9730 - val_loss: 0.2865 - val_acc: 0.9413\n",
      "Epoch 91/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0828 - acc: 0.9720 - val_loss: 0.2916 - val_acc: 0.9414\n",
      "Epoch 92/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0810 - acc: 0.9730 - val_loss: 0.2946 - val_acc: 0.9413\n",
      "Epoch 93/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0789 - acc: 0.9736 - val_loss: 0.2954 - val_acc: 0.9416\n",
      "Epoch 94/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0779 - acc: 0.9741 - val_loss: 0.3010 - val_acc: 0.9389\n",
      "Epoch 95/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0758 - acc: 0.9747 - val_loss: 0.3017 - val_acc: 0.9407\n",
      "Epoch 96/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0798 - acc: 0.9728 - val_loss: 0.2925 - val_acc: 0.9394\n",
      "Epoch 97/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0765 - acc: 0.9750 - val_loss: 0.2927 - val_acc: 0.9407\n",
      "Epoch 98/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0800 - acc: 0.9729 - val_loss: 0.3039 - val_acc: 0.9369\n",
      "Epoch 99/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0748 - acc: 0.9755 - val_loss: 0.2906 - val_acc: 0.9402\n",
      "Epoch 100/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0754 - acc: 0.9746 - val_loss: 0.2987 - val_acc: 0.9413\n",
      "Epoch 101/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0754 - acc: 0.9746 - val_loss: 0.3000 - val_acc: 0.9400\n",
      "Epoch 102/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0709 - acc: 0.9761 - val_loss: 0.3019 - val_acc: 0.9392\n",
      "Epoch 103/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0709 - acc: 0.9769 - val_loss: 0.2964 - val_acc: 0.9415\n",
      "Epoch 104/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0752 - acc: 0.9745 - val_loss: 0.2946 - val_acc: 0.9407\n",
      "Epoch 105/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0710 - acc: 0.9758 - val_loss: 0.2955 - val_acc: 0.9420\n",
      "Epoch 106/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0740 - acc: 0.9756 - val_loss: 0.2981 - val_acc: 0.9415\n",
      "Epoch 107/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0702 - acc: 0.9769 - val_loss: 0.2931 - val_acc: 0.9424\n",
      "Epoch 108/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0721 - acc: 0.9756 - val_loss: 0.2990 - val_acc: 0.9401\n",
      "Epoch 109/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0694 - acc: 0.9768 - val_loss: 0.2945 - val_acc: 0.9426\n",
      "Epoch 110/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0673 - acc: 0.9776 - val_loss: 0.3060 - val_acc: 0.9400\n",
      "Epoch 111/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0701 - acc: 0.9764 - val_loss: 0.2944 - val_acc: 0.9419\n",
      "Epoch 112/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0696 - acc: 0.9765 - val_loss: 0.2964 - val_acc: 0.9438\n",
      "Epoch 113/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0690 - acc: 0.9770 - val_loss: 0.2974 - val_acc: 0.9409\n",
      "Epoch 114/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0685 - acc: 0.9768 - val_loss: 0.2986 - val_acc: 0.9418\n",
      "Epoch 115/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0711 - acc: 0.9766 - val_loss: 0.2983 - val_acc: 0.9420\n",
      "Epoch 116/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0670 - acc: 0.9777 - val_loss: 0.2997 - val_acc: 0.9403\n",
      "Epoch 117/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0704 - acc: 0.9759 - val_loss: 0.2996 - val_acc: 0.9422\n",
      "Epoch 118/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0632 - acc: 0.9789 - val_loss: 0.3032 - val_acc: 0.9406\n",
      "Epoch 119/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0623 - acc: 0.9791 - val_loss: 0.2943 - val_acc: 0.9448\n",
      "Epoch 120/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0666 - acc: 0.9776 - val_loss: 0.3134 - val_acc: 0.9403\n",
      "Epoch 121/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0687 - acc: 0.9768 - val_loss: 0.3069 - val_acc: 0.9407\n",
      "Epoch 122/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0685 - acc: 0.9775 - val_loss: 0.3031 - val_acc: 0.9430\n",
      "Epoch 123/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0648 - acc: 0.9783 - val_loss: 0.3100 - val_acc: 0.9413\n",
      "Epoch 124/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0648 - acc: 0.9784 - val_loss: 0.2954 - val_acc: 0.9456\n",
      "Epoch 125/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0612 - acc: 0.9792 - val_loss: 0.3055 - val_acc: 0.9417\n",
      "Epoch 126/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0649 - acc: 0.9778 - val_loss: 0.2985 - val_acc: 0.9445\n",
      "Epoch 127/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0626 - acc: 0.9789 - val_loss: 0.3002 - val_acc: 0.9433\n",
      "Epoch 128/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0606 - acc: 0.9796 - val_loss: 0.2997 - val_acc: 0.9435\n",
      "Epoch 129/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0604 - acc: 0.9800 - val_loss: 0.3058 - val_acc: 0.9410\n",
      "Epoch 130/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0633 - acc: 0.9782 - val_loss: 0.3050 - val_acc: 0.9419\n",
      "Epoch 131/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0624 - acc: 0.9797 - val_loss: 0.3120 - val_acc: 0.9407\n",
      "Epoch 132/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0605 - acc: 0.9795 - val_loss: 0.3114 - val_acc: 0.9420\n",
      "Epoch 133/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0613 - acc: 0.9797 - val_loss: 0.3101 - val_acc: 0.9413\n",
      "Epoch 134/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0646 - acc: 0.9778 - val_loss: 0.3036 - val_acc: 0.9444\n",
      "Epoch 135/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0609 - acc: 0.9796 - val_loss: 0.3033 - val_acc: 0.9426\n",
      "Epoch 136/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0620 - acc: 0.9796 - val_loss: 0.3060 - val_acc: 0.9428\n",
      "Epoch 137/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0628 - acc: 0.9791 - val_loss: 0.3060 - val_acc: 0.9425\n",
      "Epoch 138/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0607 - acc: 0.9793 - val_loss: 0.3003 - val_acc: 0.9436\n",
      "Epoch 139/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0571 - acc: 0.9804 - val_loss: 0.3004 - val_acc: 0.9435\n",
      "Epoch 140/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0583 - acc: 0.9804 - val_loss: 0.3004 - val_acc: 0.9436\n",
      "Epoch 141/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0590 - acc: 0.9802 - val_loss: 0.3047 - val_acc: 0.9440\n",
      "Epoch 142/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0594 - acc: 0.9807 - val_loss: 0.3080 - val_acc: 0.9440\n",
      "Epoch 143/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0587 - acc: 0.9804 - val_loss: 0.3047 - val_acc: 0.9451\n",
      "Epoch 144/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0545 - acc: 0.9820 - val_loss: 0.3043 - val_acc: 0.9437\n",
      "Epoch 145/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0594 - acc: 0.9798 - val_loss: 0.2976 - val_acc: 0.9450\n",
      "Epoch 146/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0560 - acc: 0.9814 - val_loss: 0.3057 - val_acc: 0.9441\n",
      "Epoch 147/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0583 - acc: 0.9808 - val_loss: 0.3091 - val_acc: 0.9423\n",
      "Epoch 148/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0541 - acc: 0.9814 - val_loss: 0.3071 - val_acc: 0.9428\n",
      "Epoch 149/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0584 - acc: 0.9806 - val_loss: 0.3034 - val_acc: 0.9424\n",
      "Epoch 150/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0570 - acc: 0.9815 - val_loss: 0.3089 - val_acc: 0.9419\n",
      "Epoch 151/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0581 - acc: 0.9804 - val_loss: 0.3112 - val_acc: 0.9438\n",
      "Epoch 152/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0576 - acc: 0.9806 - val_loss: 0.3211 - val_acc: 0.9415\n",
      "Epoch 153/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0569 - acc: 0.9806 - val_loss: 0.3045 - val_acc: 0.9450\n",
      "Epoch 154/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0557 - acc: 0.9811 - val_loss: 0.3114 - val_acc: 0.9428\n",
      "Epoch 155/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0574 - acc: 0.9805 - val_loss: 0.3111 - val_acc: 0.9419\n",
      "Epoch 156/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0516 - acc: 0.9828 - val_loss: 0.3037 - val_acc: 0.9447\n",
      "Epoch 157/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0569 - acc: 0.9808 - val_loss: 0.3144 - val_acc: 0.9441\n",
      "Epoch 158/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0515 - acc: 0.9830 - val_loss: 0.3172 - val_acc: 0.9423\n",
      "Epoch 159/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0547 - acc: 0.9815 - val_loss: 0.3071 - val_acc: 0.9443\n",
      "Epoch 160/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0553 - acc: 0.9822 - val_loss: 0.3107 - val_acc: 0.9425\n",
      "Epoch 161/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0540 - acc: 0.9823 - val_loss: 0.3189 - val_acc: 0.9425\n",
      "Epoch 162/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0521 - acc: 0.9825 - val_loss: 0.3139 - val_acc: 0.9422\n",
      "Epoch 163/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0538 - acc: 0.9821 - val_loss: 0.3091 - val_acc: 0.9438\n",
      "Epoch 164/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0549 - acc: 0.9817 - val_loss: 0.3175 - val_acc: 0.9419\n",
      "Epoch 165/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0500 - acc: 0.9829 - val_loss: 0.3105 - val_acc: 0.9429\n",
      "Epoch 166/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0528 - acc: 0.9822 - val_loss: 0.3151 - val_acc: 0.9435\n",
      "Epoch 167/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0520 - acc: 0.9829 - val_loss: 0.3113 - val_acc: 0.9439\n",
      "Epoch 168/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0507 - acc: 0.9831 - val_loss: 0.3188 - val_acc: 0.9439\n",
      "Epoch 169/500\n",
      "71400/71400 [==============================] - 6s 91us/sample - loss: 0.0499 - acc: 0.9831 - val_loss: 0.2995 - val_acc: 0.9462\n",
      "Epoch 170/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0523 - acc: 0.9824 - val_loss: 0.3160 - val_acc: 0.9427\n",
      "Epoch 171/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0533 - acc: 0.9818 - val_loss: 0.3055 - val_acc: 0.9444\n",
      "Epoch 172/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0477 - acc: 0.9846 - val_loss: 0.3129 - val_acc: 0.9430\n",
      "Epoch 173/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0541 - acc: 0.9828 - val_loss: 0.3190 - val_acc: 0.9433\n",
      "Epoch 174/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0500 - acc: 0.9832 - val_loss: 0.3081 - val_acc: 0.9447\n",
      "Epoch 175/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0524 - acc: 0.9827 - val_loss: 0.3233 - val_acc: 0.9440\n",
      "Epoch 176/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0507 - acc: 0.9832 - val_loss: 0.3155 - val_acc: 0.9442\n",
      "Epoch 177/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0508 - acc: 0.9827 - val_loss: 0.3192 - val_acc: 0.9437\n",
      "Epoch 178/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0507 - acc: 0.9833 - val_loss: 0.3118 - val_acc: 0.9431\n",
      "Epoch 179/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0538 - acc: 0.9820 - val_loss: 0.3137 - val_acc: 0.9448\n",
      "Epoch 180/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0486 - acc: 0.9834 - val_loss: 0.3096 - val_acc: 0.9449\n",
      "Epoch 181/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0479 - acc: 0.9840 - val_loss: 0.3135 - val_acc: 0.9438\n",
      "Epoch 182/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0491 - acc: 0.9835 - val_loss: 0.3150 - val_acc: 0.9419\n",
      "Epoch 183/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0517 - acc: 0.9825 - val_loss: 0.3189 - val_acc: 0.9427\n",
      "Epoch 184/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0506 - acc: 0.9834 - val_loss: 0.3169 - val_acc: 0.9456\n",
      "Epoch 185/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0505 - acc: 0.9837 - val_loss: 0.3139 - val_acc: 0.9448\n",
      "Epoch 186/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0488 - acc: 0.9841 - val_loss: 0.3100 - val_acc: 0.9463\n",
      "Epoch 187/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0469 - acc: 0.9842 - val_loss: 0.3083 - val_acc: 0.9456\n",
      "Epoch 188/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0483 - acc: 0.9842 - val_loss: 0.3122 - val_acc: 0.9445\n",
      "Epoch 189/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0490 - acc: 0.9838 - val_loss: 0.3153 - val_acc: 0.9441\n",
      "Epoch 190/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0492 - acc: 0.9835 - val_loss: 0.3132 - val_acc: 0.9436\n",
      "Epoch 191/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0490 - acc: 0.9843 - val_loss: 0.3150 - val_acc: 0.9438\n",
      "Epoch 192/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0491 - acc: 0.9835 - val_loss: 0.3163 - val_acc: 0.9439\n",
      "Epoch 193/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0474 - acc: 0.9848 - val_loss: 0.3149 - val_acc: 0.9432\n",
      "Epoch 194/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0461 - acc: 0.9850 - val_loss: 0.3182 - val_acc: 0.9442\n",
      "Epoch 195/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0489 - acc: 0.9836 - val_loss: 0.3220 - val_acc: 0.9429\n",
      "Epoch 196/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0491 - acc: 0.9836 - val_loss: 0.3123 - val_acc: 0.9446\n",
      "Epoch 197/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0474 - acc: 0.9844 - val_loss: 0.3222 - val_acc: 0.9425\n",
      "Epoch 198/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0502 - acc: 0.9837 - val_loss: 0.3169 - val_acc: 0.9445\n",
      "Epoch 199/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0454 - acc: 0.9855 - val_loss: 0.3151 - val_acc: 0.9456\n",
      "Epoch 200/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0471 - acc: 0.9841 - val_loss: 0.3148 - val_acc: 0.9456\n",
      "Epoch 201/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0477 - acc: 0.9843 - val_loss: 0.3214 - val_acc: 0.9427\n",
      "Epoch 202/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0466 - acc: 0.9843 - val_loss: 0.3167 - val_acc: 0.9438\n",
      "Epoch 203/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0427 - acc: 0.9863 - val_loss: 0.3123 - val_acc: 0.9451\n",
      "Epoch 204/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0453 - acc: 0.9852 - val_loss: 0.3151 - val_acc: 0.9443\n",
      "Epoch 205/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0453 - acc: 0.9852 - val_loss: 0.3183 - val_acc: 0.9435\n",
      "Epoch 206/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0447 - acc: 0.9849 - val_loss: 0.3181 - val_acc: 0.9446\n",
      "Epoch 207/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0442 - acc: 0.9851 - val_loss: 0.3239 - val_acc: 0.9432\n",
      "Epoch 208/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0473 - acc: 0.9842 - val_loss: 0.3164 - val_acc: 0.9440\n",
      "Epoch 209/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0469 - acc: 0.9847 - val_loss: 0.3156 - val_acc: 0.9447\n",
      "Epoch 210/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0439 - acc: 0.9851 - val_loss: 0.3207 - val_acc: 0.9431\n",
      "Epoch 211/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0436 - acc: 0.9850 - val_loss: 0.3275 - val_acc: 0.9430\n",
      "Epoch 212/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0438 - acc: 0.9859 - val_loss: 0.3216 - val_acc: 0.9448\n",
      "Epoch 213/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0451 - acc: 0.9851 - val_loss: 0.3202 - val_acc: 0.9433\n",
      "Epoch 214/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0446 - acc: 0.9851 - val_loss: 0.3192 - val_acc: 0.9456\n",
      "Epoch 215/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0438 - acc: 0.9851 - val_loss: 0.3242 - val_acc: 0.9439\n",
      "Epoch 216/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0460 - acc: 0.9849 - val_loss: 0.3322 - val_acc: 0.9415\n",
      "Epoch 217/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0401 - acc: 0.9864 - val_loss: 0.3221 - val_acc: 0.9435\n",
      "Epoch 218/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0436 - acc: 0.9852 - val_loss: 0.3290 - val_acc: 0.9436\n",
      "Epoch 219/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0440 - acc: 0.9856 - val_loss: 0.3259 - val_acc: 0.9447\n",
      "Epoch 220/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0431 - acc: 0.9858 - val_loss: 0.3289 - val_acc: 0.9435\n",
      "Epoch 221/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0458 - acc: 0.9851 - val_loss: 0.3218 - val_acc: 0.9452\n",
      "Epoch 222/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0408 - acc: 0.9868 - val_loss: 0.3278 - val_acc: 0.9441\n",
      "Epoch 223/500\n",
      "71400/71400 [==============================] - 6s 91us/sample - loss: 0.0444 - acc: 0.9848 - val_loss: 0.3173 - val_acc: 0.9444\n",
      "Epoch 224/500\n",
      "71400/71400 [==============================] - 6s 84us/sample - loss: 0.0450 - acc: 0.9848 - val_loss: 0.3271 - val_acc: 0.9430\n",
      "Epoch 225/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0425 - acc: 0.9859 - val_loss: 0.3161 - val_acc: 0.9449\n",
      "Epoch 226/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0432 - acc: 0.9856 - val_loss: 0.3272 - val_acc: 0.9441\n",
      "Epoch 227/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0431 - acc: 0.9859 - val_loss: 0.3209 - val_acc: 0.9455\n",
      "Epoch 228/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0428 - acc: 0.9857 - val_loss: 0.3267 - val_acc: 0.9442\n",
      "Epoch 229/500\n",
      "71400/71400 [==============================] - 6s 84us/sample - loss: 0.0425 - acc: 0.9861 - val_loss: 0.3222 - val_acc: 0.9444\n",
      "Epoch 230/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0447 - acc: 0.9852 - val_loss: 0.3317 - val_acc: 0.9438\n",
      "Epoch 231/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0409 - acc: 0.9869 - val_loss: 0.3224 - val_acc: 0.9440\n",
      "Epoch 232/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0430 - acc: 0.9864 - val_loss: 0.3272 - val_acc: 0.9451\n",
      "Epoch 233/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0412 - acc: 0.9868 - val_loss: 0.3254 - val_acc: 0.9430\n",
      "Epoch 234/500\n",
      "71400/71400 [==============================] - 6s 84us/sample - loss: 0.0406 - acc: 0.9868 - val_loss: 0.3163 - val_acc: 0.9454\n",
      "Epoch 235/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0398 - acc: 0.9870 - val_loss: 0.3233 - val_acc: 0.9457\n",
      "Epoch 236/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0413 - acc: 0.9867 - val_loss: 0.3244 - val_acc: 0.9430\n",
      "Epoch 237/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0428 - acc: 0.9859 - val_loss: 0.3331 - val_acc: 0.9432\n",
      "Epoch 238/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0411 - acc: 0.9864 - val_loss: 0.3191 - val_acc: 0.9451\n",
      "Epoch 239/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0406 - acc: 0.9864 - val_loss: 0.3213 - val_acc: 0.9444\n",
      "Epoch 240/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0409 - acc: 0.9867 - val_loss: 0.3326 - val_acc: 0.9444\n",
      "Epoch 241/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0419 - acc: 0.9861 - val_loss: 0.3296 - val_acc: 0.9439\n",
      "Epoch 242/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0403 - acc: 0.9866 - val_loss: 0.3350 - val_acc: 0.9435\n",
      "Epoch 243/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0418 - acc: 0.9860 - val_loss: 0.3345 - val_acc: 0.9436\n",
      "Epoch 244/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0411 - acc: 0.9867 - val_loss: 0.3392 - val_acc: 0.9422\n",
      "Epoch 245/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0390 - acc: 0.9868 - val_loss: 0.3445 - val_acc: 0.9426\n",
      "Epoch 246/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0399 - acc: 0.9865 - val_loss: 0.3290 - val_acc: 0.9453\n",
      "Epoch 247/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0413 - acc: 0.9864 - val_loss: 0.3337 - val_acc: 0.9460\n",
      "Epoch 248/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0437 - acc: 0.9857 - val_loss: 0.3308 - val_acc: 0.9444\n",
      "Epoch 249/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0388 - acc: 0.9870 - val_loss: 0.3255 - val_acc: 0.9450\n",
      "Epoch 250/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0347 - acc: 0.9888 - val_loss: 0.3303 - val_acc: 0.9455\n",
      "Epoch 251/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0418 - acc: 0.9862 - val_loss: 0.3330 - val_acc: 0.9441\n",
      "Epoch 252/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0402 - acc: 0.9865 - val_loss: 0.3280 - val_acc: 0.9456\n",
      "Epoch 253/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0384 - acc: 0.9872 - val_loss: 0.3331 - val_acc: 0.9449\n",
      "Epoch 254/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0410 - acc: 0.9869 - val_loss: 0.3290 - val_acc: 0.9449\n",
      "Epoch 255/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0435 - acc: 0.9853 - val_loss: 0.3356 - val_acc: 0.9446\n",
      "Epoch 256/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0386 - acc: 0.9873 - val_loss: 0.3330 - val_acc: 0.9433\n",
      "Epoch 257/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0409 - acc: 0.9866 - val_loss: 0.3405 - val_acc: 0.9440\n",
      "Epoch 258/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0396 - acc: 0.9870 - val_loss: 0.3343 - val_acc: 0.9458\n",
      "Epoch 259/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0364 - acc: 0.9878 - val_loss: 0.3279 - val_acc: 0.9455\n",
      "Epoch 260/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0369 - acc: 0.9879 - val_loss: 0.3326 - val_acc: 0.9463\n",
      "Epoch 261/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0375 - acc: 0.9876 - val_loss: 0.3382 - val_acc: 0.9449\n",
      "Epoch 262/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0393 - acc: 0.9872 - val_loss: 0.3273 - val_acc: 0.9445\n",
      "Epoch 263/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0389 - acc: 0.9873 - val_loss: 0.3366 - val_acc: 0.9456\n",
      "Epoch 264/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0415 - acc: 0.9864 - val_loss: 0.3298 - val_acc: 0.9446\n",
      "Epoch 265/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0396 - acc: 0.9875 - val_loss: 0.3326 - val_acc: 0.9452\n",
      "Epoch 266/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0367 - acc: 0.9877 - val_loss: 0.3280 - val_acc: 0.9462\n",
      "Epoch 267/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0398 - acc: 0.9870 - val_loss: 0.3296 - val_acc: 0.9448\n",
      "Epoch 268/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0371 - acc: 0.9877 - val_loss: 0.3281 - val_acc: 0.9445\n",
      "Epoch 269/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0372 - acc: 0.9881 - val_loss: 0.3319 - val_acc: 0.9443\n",
      "Epoch 270/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0398 - acc: 0.9870 - val_loss: 0.3307 - val_acc: 0.9430\n",
      "Epoch 271/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0373 - acc: 0.9874 - val_loss: 0.3348 - val_acc: 0.9438\n",
      "Epoch 272/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0388 - acc: 0.9872 - val_loss: 0.3345 - val_acc: 0.9443\n",
      "Epoch 273/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0361 - acc: 0.9880 - val_loss: 0.3268 - val_acc: 0.9457\n",
      "Epoch 274/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0394 - acc: 0.9866 - val_loss: 0.3299 - val_acc: 0.9435\n",
      "Epoch 275/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0404 - acc: 0.9869 - val_loss: 0.3304 - val_acc: 0.9435\n",
      "Epoch 276/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0373 - acc: 0.9880 - val_loss: 0.3385 - val_acc: 0.9439\n",
      "Epoch 277/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0376 - acc: 0.9877 - val_loss: 0.3271 - val_acc: 0.9459\n",
      "Epoch 278/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.3322 - val_acc: 0.9457\n",
      "Epoch 279/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0373 - acc: 0.9876 - val_loss: 0.3374 - val_acc: 0.9430\n",
      "Epoch 280/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0390 - acc: 0.9871 - val_loss: 0.3245 - val_acc: 0.9459\n",
      "Epoch 281/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0367 - acc: 0.9881 - val_loss: 0.3371 - val_acc: 0.9453\n",
      "Epoch 282/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0374 - acc: 0.9875 - val_loss: 0.3347 - val_acc: 0.9443\n",
      "Epoch 283/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0385 - acc: 0.9876 - val_loss: 0.3280 - val_acc: 0.9458\n",
      "Epoch 284/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0384 - acc: 0.9873 - val_loss: 0.3420 - val_acc: 0.9433\n",
      "Epoch 285/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0360 - acc: 0.9882 - val_loss: 0.3387 - val_acc: 0.9446\n",
      "Epoch 286/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.3322 - val_acc: 0.9452\n",
      "Epoch 287/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0343 - acc: 0.9896 - val_loss: 0.3343 - val_acc: 0.9455\n",
      "Epoch 288/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0366 - acc: 0.9884 - val_loss: 0.3293 - val_acc: 0.9465\n",
      "Epoch 289/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0375 - acc: 0.9878 - val_loss: 0.3359 - val_acc: 0.9457\n",
      "Epoch 290/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0385 - acc: 0.9871 - val_loss: 0.3334 - val_acc: 0.9442\n",
      "Epoch 291/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0351 - acc: 0.9887 - val_loss: 0.3344 - val_acc: 0.9458\n",
      "Epoch 292/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0396 - acc: 0.9869 - val_loss: 0.3311 - val_acc: 0.9455\n",
      "Epoch 293/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0379 - acc: 0.9878 - val_loss: 0.3364 - val_acc: 0.9451\n",
      "Epoch 294/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0331 - acc: 0.9894 - val_loss: 0.3363 - val_acc: 0.9441\n",
      "Epoch 295/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0367 - acc: 0.9885 - val_loss: 0.3399 - val_acc: 0.9437\n",
      "Epoch 296/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0379 - acc: 0.9873 - val_loss: 0.3312 - val_acc: 0.9453\n",
      "Epoch 297/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0356 - acc: 0.9892 - val_loss: 0.3322 - val_acc: 0.9451\n",
      "Epoch 298/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0370 - acc: 0.9878 - val_loss: 0.3293 - val_acc: 0.9451\n",
      "Epoch 299/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0362 - acc: 0.9882 - val_loss: 0.3298 - val_acc: 0.9451\n",
      "Epoch 300/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0352 - acc: 0.9886 - val_loss: 0.3387 - val_acc: 0.9449\n",
      "Epoch 301/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0372 - acc: 0.9880 - val_loss: 0.3352 - val_acc: 0.9457\n",
      "Epoch 302/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0362 - acc: 0.9883 - val_loss: 0.3348 - val_acc: 0.9457\n",
      "Epoch 303/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0342 - acc: 0.9888 - val_loss: 0.3328 - val_acc: 0.9468\n",
      "Epoch 304/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0345 - acc: 0.9893 - val_loss: 0.3380 - val_acc: 0.9445\n",
      "Epoch 305/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0351 - acc: 0.9887 - val_loss: 0.3476 - val_acc: 0.9440\n",
      "Epoch 306/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0367 - acc: 0.9876 - val_loss: 0.3353 - val_acc: 0.9449\n",
      "Epoch 307/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0377 - acc: 0.9882 - val_loss: 0.3381 - val_acc: 0.9456\n",
      "Epoch 308/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0341 - acc: 0.9887 - val_loss: 0.3319 - val_acc: 0.9461\n",
      "Epoch 309/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0339 - acc: 0.9889 - val_loss: 0.3478 - val_acc: 0.9429\n",
      "Epoch 310/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0363 - acc: 0.9886 - val_loss: 0.3453 - val_acc: 0.9428\n",
      "Epoch 311/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0329 - acc: 0.9897 - val_loss: 0.3449 - val_acc: 0.9434\n",
      "Epoch 312/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0326 - acc: 0.9898 - val_loss: 0.3392 - val_acc: 0.9457\n",
      "Epoch 313/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0360 - acc: 0.9883 - val_loss: 0.3393 - val_acc: 0.9452\n",
      "Epoch 314/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0333 - acc: 0.9889 - val_loss: 0.3400 - val_acc: 0.9454\n",
      "Epoch 315/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0374 - acc: 0.9878 - val_loss: 0.3446 - val_acc: 0.9459\n",
      "Epoch 316/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0343 - acc: 0.9890 - val_loss: 0.3378 - val_acc: 0.9440\n",
      "Epoch 317/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0345 - acc: 0.9890 - val_loss: 0.3446 - val_acc: 0.9447\n",
      "Epoch 318/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0338 - acc: 0.9891 - val_loss: 0.3401 - val_acc: 0.9457\n",
      "Epoch 319/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0363 - acc: 0.9881 - val_loss: 0.3385 - val_acc: 0.9459\n",
      "Epoch 320/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0335 - acc: 0.9890 - val_loss: 0.3390 - val_acc: 0.9464\n",
      "Epoch 321/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0325 - acc: 0.9896 - val_loss: 0.3347 - val_acc: 0.9463\n",
      "Epoch 322/500\n",
      "71400/71400 [==============================] - 7s 91us/sample - loss: 0.0353 - acc: 0.9886 - val_loss: 0.3456 - val_acc: 0.9437\n",
      "Epoch 323/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0334 - acc: 0.9891 - val_loss: 0.3300 - val_acc: 0.9472\n",
      "Epoch 324/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0340 - acc: 0.9892 - val_loss: 0.3339 - val_acc: 0.9461\n",
      "Epoch 325/500\n",
      "71400/71400 [==============================] - 6s 84us/sample - loss: 0.0373 - acc: 0.9879 - val_loss: 0.3399 - val_acc: 0.9458\n",
      "Epoch 326/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0349 - acc: 0.9890 - val_loss: 0.3378 - val_acc: 0.9468\n",
      "Epoch 327/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0332 - acc: 0.9886 - val_loss: 0.3401 - val_acc: 0.9456\n",
      "Epoch 328/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0336 - acc: 0.9891 - val_loss: 0.3421 - val_acc: 0.9447\n",
      "Epoch 329/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0354 - acc: 0.9887 - val_loss: 0.3370 - val_acc: 0.9450\n",
      "Epoch 330/500\n",
      "71400/71400 [==============================] - 6s 84us/sample - loss: 0.0331 - acc: 0.9892 - val_loss: 0.3360 - val_acc: 0.9453\n",
      "Epoch 331/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0350 - acc: 0.9890 - val_loss: 0.3311 - val_acc: 0.9457\n",
      "Epoch 332/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0343 - acc: 0.9894 - val_loss: 0.3362 - val_acc: 0.9460\n",
      "Epoch 333/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0316 - acc: 0.9898 - val_loss: 0.3378 - val_acc: 0.9448\n",
      "Epoch 334/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0344 - acc: 0.9887 - val_loss: 0.3451 - val_acc: 0.9449\n",
      "Epoch 335/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0335 - acc: 0.9889 - val_loss: 0.3375 - val_acc: 0.9473\n",
      "Epoch 336/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0333 - acc: 0.9894 - val_loss: 0.3379 - val_acc: 0.9462\n",
      "Epoch 337/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0326 - acc: 0.9896 - val_loss: 0.3528 - val_acc: 0.9439\n",
      "Epoch 338/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0359 - acc: 0.9884 - val_loss: 0.3440 - val_acc: 0.9459\n",
      "Epoch 339/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0321 - acc: 0.9895 - val_loss: 0.3322 - val_acc: 0.9465\n",
      "Epoch 340/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0327 - acc: 0.9896 - val_loss: 0.3362 - val_acc: 0.9474\n",
      "Epoch 341/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0340 - acc: 0.9889 - val_loss: 0.3475 - val_acc: 0.9431\n",
      "Epoch 342/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0352 - acc: 0.9884 - val_loss: 0.3405 - val_acc: 0.9451\n",
      "Epoch 343/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0334 - acc: 0.9896 - val_loss: 0.3359 - val_acc: 0.9457\n",
      "Epoch 344/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0307 - acc: 0.9901 - val_loss: 0.3382 - val_acc: 0.9460\n",
      "Epoch 345/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0363 - acc: 0.9880 - val_loss: 0.3533 - val_acc: 0.9428\n",
      "Epoch 346/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0361 - acc: 0.9884 - val_loss: 0.3442 - val_acc: 0.9441\n",
      "Epoch 347/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0317 - acc: 0.9901 - val_loss: 0.3378 - val_acc: 0.9456\n",
      "Epoch 348/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0307 - acc: 0.9901 - val_loss: 0.3376 - val_acc: 0.9455\n",
      "Epoch 349/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0314 - acc: 0.9899 - val_loss: 0.3451 - val_acc: 0.9437\n",
      "Epoch 350/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0322 - acc: 0.9897 - val_loss: 0.3415 - val_acc: 0.9461\n",
      "Epoch 351/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0305 - acc: 0.9900 - val_loss: 0.3419 - val_acc: 0.9455\n",
      "Epoch 352/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0334 - acc: 0.9892 - val_loss: 0.3398 - val_acc: 0.9458\n",
      "Epoch 353/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0342 - acc: 0.9892 - val_loss: 0.3433 - val_acc: 0.9451\n",
      "Epoch 354/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0288 - acc: 0.9911 - val_loss: 0.3383 - val_acc: 0.9459\n",
      "Epoch 355/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0315 - acc: 0.9899 - val_loss: 0.3377 - val_acc: 0.9462\n",
      "Epoch 356/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0344 - acc: 0.9887 - val_loss: 0.3483 - val_acc: 0.9432\n",
      "Epoch 357/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0320 - acc: 0.9897 - val_loss: 0.3414 - val_acc: 0.9458\n",
      "Epoch 358/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0329 - acc: 0.9891 - val_loss: 0.3524 - val_acc: 0.9441\n",
      "Epoch 359/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0309 - acc: 0.9900 - val_loss: 0.3458 - val_acc: 0.9451\n",
      "Epoch 360/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0308 - acc: 0.9895 - val_loss: 0.3421 - val_acc: 0.9457\n",
      "Epoch 361/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.3422 - val_acc: 0.9459\n",
      "Epoch 362/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0327 - acc: 0.9892 - val_loss: 0.3430 - val_acc: 0.9458\n",
      "Epoch 363/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0318 - acc: 0.9895 - val_loss: 0.3383 - val_acc: 0.9445\n",
      "Epoch 364/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0304 - acc: 0.9905 - val_loss: 0.3355 - val_acc: 0.9471\n",
      "Epoch 365/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0323 - acc: 0.9898 - val_loss: 0.3476 - val_acc: 0.9458\n",
      "Epoch 366/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0312 - acc: 0.9903 - val_loss: 0.3285 - val_acc: 0.9468\n",
      "Epoch 367/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0308 - acc: 0.9897 - val_loss: 0.3386 - val_acc: 0.9442\n",
      "Epoch 368/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0326 - acc: 0.9893 - val_loss: 0.3443 - val_acc: 0.9446\n",
      "Epoch 369/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0288 - acc: 0.9907 - val_loss: 0.3472 - val_acc: 0.9450\n",
      "Epoch 370/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0328 - acc: 0.9893 - val_loss: 0.3373 - val_acc: 0.9458\n",
      "Epoch 371/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0308 - acc: 0.9898 - val_loss: 0.3475 - val_acc: 0.9453\n",
      "Epoch 372/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0297 - acc: 0.9906 - val_loss: 0.3527 - val_acc: 0.9437\n",
      "Epoch 373/500\n",
      "71400/71400 [==============================] - 7s 91us/sample - loss: 0.0299 - acc: 0.9900 - val_loss: 0.3501 - val_acc: 0.9466\n",
      "Epoch 374/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0298 - acc: 0.9904 - val_loss: 0.3440 - val_acc: 0.9455\n",
      "Epoch 375/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0337 - acc: 0.9891 - val_loss: 0.3463 - val_acc: 0.9450\n",
      "Epoch 376/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0305 - acc: 0.9905 - val_loss: 0.3425 - val_acc: 0.9460\n",
      "Epoch 377/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0315 - acc: 0.9899 - val_loss: 0.3474 - val_acc: 0.9435\n",
      "Epoch 378/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0323 - acc: 0.9897 - val_loss: 0.3406 - val_acc: 0.9459\n",
      "Epoch 379/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0281 - acc: 0.9909 - val_loss: 0.3475 - val_acc: 0.9449\n",
      "Epoch 380/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0313 - acc: 0.9896 - val_loss: 0.3430 - val_acc: 0.9456\n",
      "Epoch 381/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0324 - acc: 0.9898 - val_loss: 0.3444 - val_acc: 0.9458\n",
      "Epoch 382/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0330 - acc: 0.9892 - val_loss: 0.3431 - val_acc: 0.9448\n",
      "Epoch 383/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0294 - acc: 0.9909 - val_loss: 0.3376 - val_acc: 0.9464\n",
      "Epoch 384/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0296 - acc: 0.9905 - val_loss: 0.3484 - val_acc: 0.9467\n",
      "Epoch 385/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0331 - acc: 0.9892 - val_loss: 0.3409 - val_acc: 0.9466\n",
      "Epoch 386/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0304 - acc: 0.9903 - val_loss: 0.3444 - val_acc: 0.9448\n",
      "Epoch 387/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0324 - acc: 0.9897 - val_loss: 0.3463 - val_acc: 0.9455\n",
      "Epoch 388/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0305 - acc: 0.9903 - val_loss: 0.3401 - val_acc: 0.9460\n",
      "Epoch 389/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0316 - acc: 0.9900 - val_loss: 0.3547 - val_acc: 0.9447\n",
      "Epoch 390/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0312 - acc: 0.9902 - val_loss: 0.3475 - val_acc: 0.9466\n",
      "Epoch 391/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0291 - acc: 0.9906 - val_loss: 0.3424 - val_acc: 0.9472\n",
      "Epoch 392/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0310 - acc: 0.9898 - val_loss: 0.3417 - val_acc: 0.9458\n",
      "Epoch 393/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0320 - acc: 0.9894 - val_loss: 0.3457 - val_acc: 0.9459\n",
      "Epoch 394/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0306 - acc: 0.9903 - val_loss: 0.3486 - val_acc: 0.9443\n",
      "Epoch 395/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0326 - acc: 0.9894 - val_loss: 0.3502 - val_acc: 0.9442\n",
      "Epoch 396/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0296 - acc: 0.9908 - val_loss: 0.3451 - val_acc: 0.9457\n",
      "Epoch 397/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0325 - acc: 0.9893 - val_loss: 0.3436 - val_acc: 0.9468\n",
      "Epoch 398/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0307 - acc: 0.9899 - val_loss: 0.3440 - val_acc: 0.9459\n",
      "Epoch 399/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0318 - acc: 0.9897 - val_loss: 0.3377 - val_acc: 0.9456\n",
      "Epoch 400/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0312 - acc: 0.9901 - val_loss: 0.3410 - val_acc: 0.9449\n",
      "Epoch 401/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0285 - acc: 0.9912 - val_loss: 0.3381 - val_acc: 0.9479\n",
      "Epoch 402/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0309 - acc: 0.9909 - val_loss: 0.3452 - val_acc: 0.9446\n",
      "Epoch 403/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0307 - acc: 0.9903 - val_loss: 0.3354 - val_acc: 0.9467\n",
      "Epoch 404/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0301 - acc: 0.9907 - val_loss: 0.3467 - val_acc: 0.9447\n",
      "Epoch 405/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0280 - acc: 0.9911 - val_loss: 0.3427 - val_acc: 0.9481\n",
      "Epoch 406/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0294 - acc: 0.9907 - val_loss: 0.3474 - val_acc: 0.9455\n",
      "Epoch 407/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0337 - acc: 0.9894 - val_loss: 0.3491 - val_acc: 0.9452\n",
      "Epoch 408/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0305 - acc: 0.9905 - val_loss: 0.3441 - val_acc: 0.9460\n",
      "Epoch 409/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0294 - acc: 0.9906 - val_loss: 0.3406 - val_acc: 0.9475\n",
      "Epoch 410/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0290 - acc: 0.9906 - val_loss: 0.3431 - val_acc: 0.9460\n",
      "Epoch 411/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0285 - acc: 0.9910 - val_loss: 0.3418 - val_acc: 0.9459\n",
      "Epoch 412/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0280 - acc: 0.9908 - val_loss: 0.3413 - val_acc: 0.9470\n",
      "Epoch 413/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0314 - acc: 0.9896 - val_loss: 0.3434 - val_acc: 0.9454\n",
      "Epoch 414/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0290 - acc: 0.9908 - val_loss: 0.3404 - val_acc: 0.9463\n",
      "Epoch 415/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0300 - acc: 0.9904 - val_loss: 0.3401 - val_acc: 0.9468\n",
      "Epoch 416/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0294 - acc: 0.9906 - val_loss: 0.3495 - val_acc: 0.9458\n",
      "Epoch 417/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0297 - acc: 0.9909 - val_loss: 0.3460 - val_acc: 0.9461\n",
      "Epoch 418/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0292 - acc: 0.9906 - val_loss: 0.3470 - val_acc: 0.9463\n",
      "Epoch 419/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0287 - acc: 0.9907 - val_loss: 0.3477 - val_acc: 0.9459\n",
      "Epoch 420/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0271 - acc: 0.9911 - val_loss: 0.3524 - val_acc: 0.9452\n",
      "Epoch 421/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0298 - acc: 0.9907 - val_loss: 0.3506 - val_acc: 0.9453\n",
      "Epoch 422/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0324 - acc: 0.9900 - val_loss: 0.3510 - val_acc: 0.9471\n",
      "Epoch 423/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0297 - acc: 0.9905 - val_loss: 0.3548 - val_acc: 0.9454\n",
      "Epoch 424/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0289 - acc: 0.9909 - val_loss: 0.3617 - val_acc: 0.9449\n",
      "Epoch 425/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0321 - acc: 0.9897 - val_loss: 0.3507 - val_acc: 0.9453\n",
      "Epoch 426/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0288 - acc: 0.9908 - val_loss: 0.3482 - val_acc: 0.9456\n",
      "Epoch 427/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0294 - acc: 0.9906 - val_loss: 0.3556 - val_acc: 0.9453\n",
      "Epoch 428/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0268 - acc: 0.9917 - val_loss: 0.3520 - val_acc: 0.9458\n",
      "Epoch 429/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0274 - acc: 0.9916 - val_loss: 0.3476 - val_acc: 0.9454\n",
      "Epoch 430/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0298 - acc: 0.9908 - val_loss: 0.3520 - val_acc: 0.9451\n",
      "Epoch 431/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0266 - acc: 0.9913 - val_loss: 0.3540 - val_acc: 0.9465\n",
      "Epoch 432/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0296 - acc: 0.9908 - val_loss: 0.3538 - val_acc: 0.9445\n",
      "Epoch 433/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0321 - acc: 0.9899 - val_loss: 0.3513 - val_acc: 0.9457\n",
      "Epoch 434/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0305 - acc: 0.9906 - val_loss: 0.3541 - val_acc: 0.9450\n",
      "Epoch 435/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0311 - acc: 0.9896 - val_loss: 0.3450 - val_acc: 0.9461\n",
      "Epoch 436/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0298 - acc: 0.9904 - val_loss: 0.3524 - val_acc: 0.9443\n",
      "Epoch 437/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0299 - acc: 0.9905 - val_loss: 0.3517 - val_acc: 0.9451\n",
      "Epoch 438/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0282 - acc: 0.9912 - val_loss: 0.3482 - val_acc: 0.9456\n",
      "Epoch 439/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0258 - acc: 0.9917 - val_loss: 0.3475 - val_acc: 0.9448\n",
      "Epoch 440/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0278 - acc: 0.9913 - val_loss: 0.3526 - val_acc: 0.9460\n",
      "Epoch 441/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0285 - acc: 0.9909 - val_loss: 0.3507 - val_acc: 0.9457\n",
      "Epoch 442/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0273 - acc: 0.9913 - val_loss: 0.3477 - val_acc: 0.9465\n",
      "Epoch 443/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0299 - acc: 0.9904 - val_loss: 0.3525 - val_acc: 0.9448\n",
      "Epoch 444/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0294 - acc: 0.9910 - val_loss: 0.3610 - val_acc: 0.9449\n",
      "Epoch 445/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0318 - acc: 0.9902 - val_loss: 0.3536 - val_acc: 0.9458\n",
      "Epoch 446/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0260 - acc: 0.9918 - val_loss: 0.3571 - val_acc: 0.9463\n",
      "Epoch 447/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0305 - acc: 0.9901 - val_loss: 0.3477 - val_acc: 0.9456\n",
      "Epoch 448/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0288 - acc: 0.9906 - val_loss: 0.3558 - val_acc: 0.9463\n",
      "Epoch 449/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0273 - acc: 0.9915 - val_loss: 0.3472 - val_acc: 0.9463\n",
      "Epoch 450/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0271 - acc: 0.9914 - val_loss: 0.3577 - val_acc: 0.9439\n",
      "Epoch 451/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0271 - acc: 0.9915 - val_loss: 0.3523 - val_acc: 0.9445\n",
      "Epoch 452/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0281 - acc: 0.9908 - val_loss: 0.3563 - val_acc: 0.9449\n",
      "Epoch 453/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0283 - acc: 0.9909 - val_loss: 0.3507 - val_acc: 0.9469\n",
      "Epoch 454/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0290 - acc: 0.9906 - val_loss: 0.3480 - val_acc: 0.9457\n",
      "Epoch 455/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0271 - acc: 0.9910 - val_loss: 0.3511 - val_acc: 0.9451\n",
      "Epoch 456/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0260 - acc: 0.9915 - val_loss: 0.3571 - val_acc: 0.9432\n",
      "Epoch 457/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0281 - acc: 0.9911 - val_loss: 0.3581 - val_acc: 0.9441\n",
      "Epoch 458/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0261 - acc: 0.9915 - val_loss: 0.3556 - val_acc: 0.9460\n",
      "Epoch 459/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.3581 - val_acc: 0.9441\n",
      "Epoch 460/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0267 - acc: 0.9918 - val_loss: 0.3648 - val_acc: 0.9449\n",
      "Epoch 461/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0295 - acc: 0.9909 - val_loss: 0.3591 - val_acc: 0.9458\n",
      "Epoch 462/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0292 - acc: 0.9910 - val_loss: 0.3557 - val_acc: 0.9442\n",
      "Epoch 463/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0278 - acc: 0.9914 - val_loss: 0.3514 - val_acc: 0.9445\n",
      "Epoch 464/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0282 - acc: 0.9907 - val_loss: 0.3448 - val_acc: 0.9465\n",
      "Epoch 465/500\n",
      "71400/71400 [==============================] - 6s 91us/sample - loss: 0.0273 - acc: 0.9913 - val_loss: 0.3514 - val_acc: 0.9465\n",
      "Epoch 466/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0272 - acc: 0.9909 - val_loss: 0.3490 - val_acc: 0.9464\n",
      "Epoch 467/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0281 - acc: 0.9915 - val_loss: 0.3516 - val_acc: 0.9458\n",
      "Epoch 468/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0264 - acc: 0.9916 - val_loss: 0.3423 - val_acc: 0.9462\n",
      "Epoch 469/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0250 - acc: 0.9918 - val_loss: 0.3449 - val_acc: 0.9470\n",
      "Epoch 470/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0310 - acc: 0.9904 - val_loss: 0.3463 - val_acc: 0.9458\n",
      "Epoch 471/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0255 - acc: 0.9921 - val_loss: 0.3548 - val_acc: 0.9451\n",
      "Epoch 472/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0285 - acc: 0.9909 - val_loss: 0.3516 - val_acc: 0.9470\n",
      "Epoch 473/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0291 - acc: 0.9910 - val_loss: 0.3555 - val_acc: 0.9465\n",
      "Epoch 474/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0272 - acc: 0.9915 - val_loss: 0.3494 - val_acc: 0.9478\n",
      "Epoch 475/500\n",
      "71400/71400 [==============================] - 6s 90us/sample - loss: 0.0276 - acc: 0.9910 - val_loss: 0.3570 - val_acc: 0.9457\n",
      "Epoch 476/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0282 - acc: 0.9907 - val_loss: 0.3520 - val_acc: 0.9459\n",
      "Epoch 477/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0260 - acc: 0.9919 - val_loss: 0.3582 - val_acc: 0.9454\n",
      "Epoch 478/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0255 - acc: 0.9919 - val_loss: 0.3560 - val_acc: 0.9461\n",
      "Epoch 479/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0283 - acc: 0.9912 - val_loss: 0.3487 - val_acc: 0.9462\n",
      "Epoch 480/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0254 - acc: 0.9923 - val_loss: 0.3532 - val_acc: 0.9456\n",
      "Epoch 481/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0302 - acc: 0.9903 - val_loss: 0.3508 - val_acc: 0.9460\n",
      "Epoch 482/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0253 - acc: 0.9923 - val_loss: 0.3471 - val_acc: 0.9467\n",
      "Epoch 483/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0263 - acc: 0.9916 - val_loss: 0.3614 - val_acc: 0.9453\n",
      "Epoch 484/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0296 - acc: 0.9909 - val_loss: 0.3617 - val_acc: 0.9459\n",
      "Epoch 485/500\n",
      "71400/71400 [==============================] - 6s 88us/sample - loss: 0.0295 - acc: 0.9904 - val_loss: 0.3583 - val_acc: 0.9455\n",
      "Epoch 486/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0289 - acc: 0.9907 - val_loss: 0.3499 - val_acc: 0.9464\n",
      "Epoch 487/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0253 - acc: 0.9920 - val_loss: 0.3515 - val_acc: 0.9468\n",
      "Epoch 488/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0292 - acc: 0.9907 - val_loss: 0.3488 - val_acc: 0.9476\n",
      "Epoch 489/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0271 - acc: 0.9917 - val_loss: 0.3665 - val_acc: 0.9454\n",
      "Epoch 490/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0256 - acc: 0.9921 - val_loss: 0.3534 - val_acc: 0.9465\n",
      "Epoch 491/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0270 - acc: 0.9915 - val_loss: 0.3571 - val_acc: 0.9460\n",
      "Epoch 492/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0286 - acc: 0.9909 - val_loss: 0.3667 - val_acc: 0.9448\n",
      "Epoch 493/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0263 - acc: 0.9915 - val_loss: 0.3611 - val_acc: 0.9454\n",
      "Epoch 494/500\n",
      "71400/71400 [==============================] - 6s 87us/sample - loss: 0.0278 - acc: 0.9910 - val_loss: 0.3510 - val_acc: 0.9470\n",
      "Epoch 495/500\n",
      "71400/71400 [==============================] - 6s 89us/sample - loss: 0.0240 - acc: 0.9928 - val_loss: 0.3532 - val_acc: 0.9470\n",
      "Epoch 496/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0262 - acc: 0.9918 - val_loss: 0.3616 - val_acc: 0.9448\n",
      "Epoch 497/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0275 - acc: 0.9911 - val_loss: 0.3575 - val_acc: 0.9458\n",
      "Epoch 498/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0262 - acc: 0.9917 - val_loss: 0.3622 - val_acc: 0.9458\n",
      "Epoch 499/500\n",
      "71400/71400 [==============================] - 6s 86us/sample - loss: 0.0274 - acc: 0.9916 - val_loss: 0.3564 - val_acc: 0.9448\n",
      "Epoch 500/500\n",
      "71400/71400 [==============================] - 6s 85us/sample - loss: 0.0254 - acc: 0.9920 - val_loss: 0.3549 - val_acc: 0.9461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f135a1bc940>"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Learning_rate=0.0003221410197800375\n",
    "neurons=[350,250,150,10]\n",
    "Regularize= 1.3083277528087247e-07\n",
    "Activation=['relu','relu','relu','softmax']\n",
    "do=0.03572196350233002\n",
    "Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "\n",
    "nmodel_2=tf.keras.models.Sequential()\n",
    "nmodel_2.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "for i in range(0,len(neurons)):\n",
    "  nmodel_2.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "  if i != (len(neurons)-1):\n",
    "    nmodel_2.add(tf.keras.layers.BatchNormalization()) \n",
    "    nmodel_2.add(tf.keras.layers.Dropout(do))   \n",
    "    \n",
    "nmodel_2.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "nmodel_2.fit(nnfX_train,HHy_train,validation_data=(nnfX_val,HHy_val),batch_size=128,epochs=500,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51xF8JXNkhCn"
   },
   "source": [
    "**With Dataset: Same as the H5 file (Validation higher than training**\n",
    "\n",
    "**SGD model**\n",
    "\n",
    "**Model 1**\n",
    "\n",
    "Layers : [350,250,150,10],\n",
    "Activation:['relu','relu','relu','softmax'],\n",
    "Lr:0.019560946340911864,\n",
    "Lambda:4.715676601623284e-07\n",
    "\n",
    "**Training Accuracy : 0.9827 ** \n",
    "\n",
    "** val_acc: 0.9572 ** \n",
    "\n",
    "\n",
    "\n",
    "**ADAM model**\n",
    "\n",
    "**Model 2**\n",
    "\n",
    "Layers : [350,250,150,10],\n",
    "Activation:['relu','relu','relu','softmax'],\n",
    "lr: 0.0003221410197800375, \n",
    "Lambda: 1.3083277528087247e-07, \n",
    "Dropout: 0.03572196350233002\n",
    "\n",
    " \n",
    "**Training Accuracy : 0.9904 ** \n",
    "\n",
    "** val_acc: 0.9607 ** \n",
    "\n",
    "\n",
    "**Model 3**\n",
    "\n",
    "Layers : [350,250,150,10],\n",
    "Activation:['relu','relu','relu','softmax'],\n",
    "lr: 0.0005028916162997343, Lambda: 1.427453007116601e-06, Dropout: 0.2285388887981482\n",
    "\n",
    " **Training Accuracy : 0.9510 ** \n",
    "\n",
    " ** val_acc:0.9620 ** \n",
    "\n",
    "  The learning rate parameters for SGD and Adams is different and Lambda is same (Very small in both the cases). we have introduced dropout in Adams. But accuracy is not much different so lets keep it as such\n",
    "\n",
    " **With modified Dataset: Train and val combined and split in 70:30 ratio**\n",
    " \n",
    " **nmodel_1**\n",
    "\n",
    "Layers : [350,250,150,10],\n",
    "Activation:['relu','relu','relu','softmax'],\n",
    " lr: 0.0004992788270625213, \n",
    " Lambda: 0.0010370185781516902,\n",
    " Dropout: 0.22800119704264665\n",
    "\n",
    "**Training Accuracy : 0.8750 ** \n",
    "\n",
    "** val_acc:0.8995**\n",
    "\n",
    "\n",
    " **nmodel_2**\n",
    "\n",
    "Layers : [350,250,150,10],\n",
    "Activation:['relu','relu','relu','softmax'],\n",
    "lr: 0.0003221410197800375, \n",
    "Lambda: 1.3083277528087247e-07, \n",
    "Dropout: 0.03572196350233002\n",
    "\n",
    " \n",
    "**Training Accuracy : 0.9920 **  \n",
    "\n",
    "** val_acc: 0.9461 **\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fF7Py2F108-"
   },
   "source": [
    "Models - 1,2,3,4  seem to be doing reasonably good in validation. \n",
    "\n",
    "Lets try adding batch normalisation to each hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YdBMd8Oy2m0_"
   },
   "source": [
    "Bmodel_2: model 2 parameter with new batch norm at each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rNLYiSR02l9s",
    "outputId": "a41e28ff-3ed0-495c-e6b6-ca88108c463b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/500\n",
      "42000/42000 [==============================] - 10s 245us/sample - loss: 1.5059 - acc: 0.5090 - val_loss: 1.0734 - val_acc: 0.6808\n",
      "Epoch 2/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.9486 - acc: 0.7003 - val_loss: 0.7833 - val_acc: 0.7588\n",
      "Epoch 3/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.7998 - acc: 0.7484 - val_loss: 0.6705 - val_acc: 0.7939\n",
      "Epoch 4/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.7178 - acc: 0.7748 - val_loss: 0.6081 - val_acc: 0.8133\n",
      "Epoch 5/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.6558 - acc: 0.7937 - val_loss: 0.5569 - val_acc: 0.8296\n",
      "Epoch 6/500\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.6038 - acc: 0.8102 - val_loss: 0.5118 - val_acc: 0.8442\n",
      "Epoch 7/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.5731 - acc: 0.8204 - val_loss: 0.4950 - val_acc: 0.8486\n",
      "Epoch 8/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.5403 - acc: 0.8282 - val_loss: 0.4698 - val_acc: 0.8571\n",
      "Epoch 9/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.5117 - acc: 0.8372 - val_loss: 0.4720 - val_acc: 0.8559\n",
      "Epoch 10/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.4930 - acc: 0.8433 - val_loss: 0.4452 - val_acc: 0.8628\n",
      "Epoch 11/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.4701 - acc: 0.8506 - val_loss: 0.4318 - val_acc: 0.8686\n",
      "Epoch 12/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.4533 - acc: 0.8545 - val_loss: 0.3907 - val_acc: 0.8819\n",
      "Epoch 13/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.4370 - acc: 0.8593 - val_loss: 0.3846 - val_acc: 0.8822\n",
      "Epoch 14/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.4153 - acc: 0.8673 - val_loss: 0.3651 - val_acc: 0.8910\n",
      "Epoch 15/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.3948 - acc: 0.8742 - val_loss: 0.3657 - val_acc: 0.8905\n",
      "Epoch 16/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.3956 - acc: 0.8734 - val_loss: 0.3482 - val_acc: 0.8970\n",
      "Epoch 17/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.3667 - acc: 0.8818 - val_loss: 0.3330 - val_acc: 0.9014\n",
      "Epoch 18/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.3619 - acc: 0.8837 - val_loss: 0.3197 - val_acc: 0.9057\n",
      "Epoch 19/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.3518 - acc: 0.8858 - val_loss: 0.3403 - val_acc: 0.8978\n",
      "Epoch 20/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.3456 - acc: 0.8877 - val_loss: 0.3211 - val_acc: 0.9049\n",
      "Epoch 21/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.3261 - acc: 0.8958 - val_loss: 0.3027 - val_acc: 0.9114\n",
      "Epoch 22/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.3223 - acc: 0.8939 - val_loss: 0.3231 - val_acc: 0.9043\n",
      "Epoch 23/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.3193 - acc: 0.8968 - val_loss: 0.2894 - val_acc: 0.9174\n",
      "Epoch 24/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.3188 - acc: 0.8955 - val_loss: 0.2910 - val_acc: 0.9151\n",
      "Epoch 25/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.2944 - acc: 0.9039 - val_loss: 0.2754 - val_acc: 0.9217\n",
      "Epoch 26/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.2801 - acc: 0.9094 - val_loss: 0.3109 - val_acc: 0.9084\n",
      "Epoch 27/500\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.2817 - acc: 0.9085 - val_loss: 0.2631 - val_acc: 0.9257\n",
      "Epoch 28/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.2741 - acc: 0.9102 - val_loss: 0.2782 - val_acc: 0.9199\n",
      "Epoch 29/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.2735 - acc: 0.9108 - val_loss: 0.2751 - val_acc: 0.9222\n",
      "Epoch 30/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.2596 - acc: 0.9154 - val_loss: 0.2608 - val_acc: 0.9257\n",
      "Epoch 31/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.2611 - acc: 0.9151 - val_loss: 0.2562 - val_acc: 0.9273\n",
      "Epoch 32/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.2464 - acc: 0.9203 - val_loss: 0.2635 - val_acc: 0.9249\n",
      "Epoch 33/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.2465 - acc: 0.9180 - val_loss: 0.2655 - val_acc: 0.9257\n",
      "Epoch 34/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.2445 - acc: 0.9198 - val_loss: 0.2532 - val_acc: 0.9298\n",
      "Epoch 35/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.2364 - acc: 0.9213 - val_loss: 0.2449 - val_acc: 0.9319\n",
      "Epoch 36/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.2282 - acc: 0.9242 - val_loss: 0.2449 - val_acc: 0.9330\n",
      "Epoch 37/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.2244 - acc: 0.9259 - val_loss: 0.2438 - val_acc: 0.9332\n",
      "Epoch 38/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.2192 - acc: 0.9265 - val_loss: 0.2492 - val_acc: 0.9311\n",
      "Epoch 39/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.2138 - acc: 0.9295 - val_loss: 0.2362 - val_acc: 0.9364\n",
      "Epoch 40/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.2091 - acc: 0.9316 - val_loss: 0.2312 - val_acc: 0.9383\n",
      "Epoch 41/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.2038 - acc: 0.9324 - val_loss: 0.2415 - val_acc: 0.9360\n",
      "Epoch 42/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.2103 - acc: 0.9308 - val_loss: 0.2367 - val_acc: 0.9364\n",
      "Epoch 43/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1919 - acc: 0.9375 - val_loss: 0.2331 - val_acc: 0.9384\n",
      "Epoch 44/500\n",
      "42000/42000 [==============================] - 6s 134us/sample - loss: 0.1912 - acc: 0.9374 - val_loss: 0.2364 - val_acc: 0.9371\n",
      "Epoch 45/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1932 - acc: 0.9369 - val_loss: 0.2334 - val_acc: 0.9380\n",
      "Epoch 46/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1944 - acc: 0.9354 - val_loss: 0.2344 - val_acc: 0.9378\n",
      "Epoch 47/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1916 - acc: 0.9353 - val_loss: 0.2440 - val_acc: 0.9354\n",
      "Epoch 48/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1753 - acc: 0.9426 - val_loss: 0.2248 - val_acc: 0.9414\n",
      "Epoch 49/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1861 - acc: 0.9395 - val_loss: 0.2217 - val_acc: 0.9420\n",
      "Epoch 50/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1765 - acc: 0.9408 - val_loss: 0.2379 - val_acc: 0.9380\n",
      "Epoch 51/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1743 - acc: 0.9421 - val_loss: 0.2260 - val_acc: 0.9414\n",
      "Epoch 52/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.1711 - acc: 0.9425 - val_loss: 0.2251 - val_acc: 0.9424\n",
      "Epoch 53/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1779 - acc: 0.9400 - val_loss: 0.2194 - val_acc: 0.9440\n",
      "Epoch 54/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1662 - acc: 0.9458 - val_loss: 0.2278 - val_acc: 0.9417\n",
      "Epoch 55/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1647 - acc: 0.9449 - val_loss: 0.2168 - val_acc: 0.9451\n",
      "Epoch 56/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1614 - acc: 0.9462 - val_loss: 0.2146 - val_acc: 0.9457\n",
      "Epoch 57/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1586 - acc: 0.9469 - val_loss: 0.2126 - val_acc: 0.9475\n",
      "Epoch 58/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1539 - acc: 0.9485 - val_loss: 0.2151 - val_acc: 0.9455\n",
      "Epoch 59/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1554 - acc: 0.9478 - val_loss: 0.2132 - val_acc: 0.9470\n",
      "Epoch 60/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1528 - acc: 0.9487 - val_loss: 0.2180 - val_acc: 0.9460\n",
      "Epoch 61/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1452 - acc: 0.9512 - val_loss: 0.2156 - val_acc: 0.9463\n",
      "Epoch 62/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1447 - acc: 0.9526 - val_loss: 0.2201 - val_acc: 0.9448\n",
      "Epoch 63/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1482 - acc: 0.9505 - val_loss: 0.2321 - val_acc: 0.9419\n",
      "Epoch 64/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1540 - acc: 0.9473 - val_loss: 0.2215 - val_acc: 0.9446\n",
      "Epoch 65/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1387 - acc: 0.9526 - val_loss: 0.2060 - val_acc: 0.9508\n",
      "Epoch 66/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1411 - acc: 0.9524 - val_loss: 0.2034 - val_acc: 0.9517\n",
      "Epoch 67/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1337 - acc: 0.9561 - val_loss: 0.2323 - val_acc: 0.9419\n",
      "Epoch 68/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1483 - acc: 0.9487 - val_loss: 0.2073 - val_acc: 0.9507\n",
      "Epoch 69/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.1455 - acc: 0.9511 - val_loss: 0.2107 - val_acc: 0.9499\n",
      "Epoch 70/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1343 - acc: 0.9555 - val_loss: 0.2209 - val_acc: 0.9463\n",
      "Epoch 71/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1324 - acc: 0.9556 - val_loss: 0.2117 - val_acc: 0.9495\n",
      "Epoch 72/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.1302 - acc: 0.9566 - val_loss: 0.2133 - val_acc: 0.9487\n",
      "Epoch 73/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1272 - acc: 0.9578 - val_loss: 0.2093 - val_acc: 0.9497\n",
      "Epoch 74/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1284 - acc: 0.9567 - val_loss: 0.2022 - val_acc: 0.9528\n",
      "Epoch 75/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1262 - acc: 0.9566 - val_loss: 0.2065 - val_acc: 0.9513\n",
      "Epoch 76/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1350 - acc: 0.9541 - val_loss: 0.2087 - val_acc: 0.9513\n",
      "Epoch 77/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.1275 - acc: 0.9573 - val_loss: 0.2099 - val_acc: 0.9504\n",
      "Epoch 78/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1344 - acc: 0.9552 - val_loss: 0.2031 - val_acc: 0.9525\n",
      "Epoch 79/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1203 - acc: 0.9589 - val_loss: 0.2087 - val_acc: 0.9518\n",
      "Epoch 80/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1299 - acc: 0.9563 - val_loss: 0.2076 - val_acc: 0.9527\n",
      "Epoch 81/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1351 - acc: 0.9549 - val_loss: 0.2084 - val_acc: 0.9515\n",
      "Epoch 82/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.1169 - acc: 0.9606 - val_loss: 0.2179 - val_acc: 0.9492\n",
      "Epoch 83/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1120 - acc: 0.9626 - val_loss: 0.2101 - val_acc: 0.9508\n",
      "Epoch 84/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1390 - acc: 0.9530 - val_loss: 0.2064 - val_acc: 0.9529\n",
      "Epoch 85/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.1158 - acc: 0.9610 - val_loss: 0.2127 - val_acc: 0.9497\n",
      "Epoch 86/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.1275 - acc: 0.9566 - val_loss: 0.2191 - val_acc: 0.9488\n",
      "Epoch 87/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1161 - acc: 0.9615 - val_loss: 0.1991 - val_acc: 0.9551\n",
      "Epoch 88/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1099 - acc: 0.9637 - val_loss: 0.2111 - val_acc: 0.9509\n",
      "Epoch 89/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.1127 - acc: 0.9625 - val_loss: 0.2010 - val_acc: 0.9546\n",
      "Epoch 90/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.1086 - acc: 0.9639 - val_loss: 0.2079 - val_acc: 0.9520\n",
      "Epoch 91/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1214 - acc: 0.9590 - val_loss: 0.2072 - val_acc: 0.9524\n",
      "Epoch 92/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1205 - acc: 0.9596 - val_loss: 0.2005 - val_acc: 0.9544\n",
      "Epoch 93/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1053 - acc: 0.9652 - val_loss: 0.2046 - val_acc: 0.9536\n",
      "Epoch 94/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1063 - acc: 0.9647 - val_loss: 0.2145 - val_acc: 0.9523\n",
      "Epoch 95/500\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.1288 - acc: 0.9561 - val_loss: 0.2051 - val_acc: 0.9542\n",
      "Epoch 96/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1144 - acc: 0.9622 - val_loss: 0.2057 - val_acc: 0.9531\n",
      "Epoch 97/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1011 - acc: 0.9659 - val_loss: 0.2049 - val_acc: 0.9556\n",
      "Epoch 98/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1108 - acc: 0.9621 - val_loss: 0.2089 - val_acc: 0.9534\n",
      "Epoch 99/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1074 - acc: 0.9654 - val_loss: 0.2068 - val_acc: 0.9553\n",
      "Epoch 100/500\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.0996 - acc: 0.9657 - val_loss: 0.2021 - val_acc: 0.9557\n",
      "Epoch 101/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0956 - acc: 0.9682 - val_loss: 0.2108 - val_acc: 0.9532\n",
      "Epoch 102/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1018 - acc: 0.9661 - val_loss: 0.2027 - val_acc: 0.9559\n",
      "Epoch 103/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0904 - acc: 0.9686 - val_loss: 0.2072 - val_acc: 0.9542\n",
      "Epoch 104/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1056 - acc: 0.9646 - val_loss: 0.2126 - val_acc: 0.9528\n",
      "Epoch 105/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1000 - acc: 0.9666 - val_loss: 0.1979 - val_acc: 0.9568\n",
      "Epoch 106/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.1007 - acc: 0.9654 - val_loss: 0.2153 - val_acc: 0.9520\n",
      "Epoch 107/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.1012 - acc: 0.9655 - val_loss: 0.2148 - val_acc: 0.9519\n",
      "Epoch 108/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0977 - acc: 0.9670 - val_loss: 0.2042 - val_acc: 0.9551\n",
      "Epoch 109/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0882 - acc: 0.9706 - val_loss: 0.2087 - val_acc: 0.9537\n",
      "Epoch 110/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0962 - acc: 0.9671 - val_loss: 0.2138 - val_acc: 0.9536\n",
      "Epoch 111/500\n",
      "42000/42000 [==============================] - 5s 131us/sample - loss: 0.1173 - acc: 0.9600 - val_loss: 0.2166 - val_acc: 0.9531\n",
      "Epoch 112/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.1302 - acc: 0.9558 - val_loss: 0.2099 - val_acc: 0.9546\n",
      "Epoch 113/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0934 - acc: 0.9680 - val_loss: 0.2027 - val_acc: 0.9569\n",
      "Epoch 114/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0958 - acc: 0.9670 - val_loss: 0.2031 - val_acc: 0.9563\n",
      "Epoch 115/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0891 - acc: 0.9700 - val_loss: 0.2038 - val_acc: 0.9565\n",
      "Epoch 116/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0979 - acc: 0.9670 - val_loss: 0.2096 - val_acc: 0.9548\n",
      "Epoch 117/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0830 - acc: 0.9720 - val_loss: 0.2142 - val_acc: 0.9536\n",
      "Epoch 118/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0931 - acc: 0.9683 - val_loss: 0.2094 - val_acc: 0.9547\n",
      "Epoch 119/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0831 - acc: 0.9722 - val_loss: 0.2030 - val_acc: 0.9574\n",
      "Epoch 120/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0945 - acc: 0.9682 - val_loss: 0.2261 - val_acc: 0.9509\n",
      "Epoch 121/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.1293 - acc: 0.9568 - val_loss: 0.2090 - val_acc: 0.9554\n",
      "Epoch 122/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.1091 - acc: 0.9640 - val_loss: 0.2022 - val_acc: 0.9574\n",
      "Epoch 123/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0837 - acc: 0.9722 - val_loss: 0.2048 - val_acc: 0.9567\n",
      "Epoch 124/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0945 - acc: 0.9684 - val_loss: 0.2022 - val_acc: 0.9566\n",
      "Epoch 125/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0888 - acc: 0.9704 - val_loss: 0.2000 - val_acc: 0.9580\n",
      "Epoch 126/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0775 - acc: 0.9748 - val_loss: 0.2022 - val_acc: 0.9571\n",
      "Epoch 127/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0812 - acc: 0.9726 - val_loss: 0.2104 - val_acc: 0.9556\n",
      "Epoch 128/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0818 - acc: 0.9718 - val_loss: 0.2161 - val_acc: 0.9533\n",
      "Epoch 129/500\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.0849 - acc: 0.9712 - val_loss: 0.2122 - val_acc: 0.9552\n",
      "Epoch 130/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0860 - acc: 0.9710 - val_loss: 0.2086 - val_acc: 0.9567\n",
      "Epoch 131/500\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.0905 - acc: 0.9689 - val_loss: 0.2147 - val_acc: 0.9551\n",
      "Epoch 132/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0852 - acc: 0.9716 - val_loss: 0.2014 - val_acc: 0.9584\n",
      "Epoch 133/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0875 - acc: 0.9713 - val_loss: 0.2102 - val_acc: 0.9561\n",
      "Epoch 134/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0871 - acc: 0.9704 - val_loss: 0.2089 - val_acc: 0.9567\n",
      "Epoch 135/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0828 - acc: 0.9720 - val_loss: 0.2084 - val_acc: 0.9569\n",
      "Epoch 136/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0781 - acc: 0.9744 - val_loss: 0.2184 - val_acc: 0.9534\n",
      "Epoch 137/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0865 - acc: 0.9713 - val_loss: 0.2162 - val_acc: 0.9557\n",
      "Epoch 138/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0974 - acc: 0.9672 - val_loss: 0.2129 - val_acc: 0.9552\n",
      "Epoch 139/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0741 - acc: 0.9750 - val_loss: 0.2070 - val_acc: 0.9577\n",
      "Epoch 140/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0736 - acc: 0.9755 - val_loss: 0.2137 - val_acc: 0.9553\n",
      "Epoch 141/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0748 - acc: 0.9749 - val_loss: 0.2101 - val_acc: 0.9570\n",
      "Epoch 142/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0729 - acc: 0.9761 - val_loss: 0.2153 - val_acc: 0.9543\n",
      "Epoch 143/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0827 - acc: 0.9712 - val_loss: 0.2222 - val_acc: 0.9540\n",
      "Epoch 144/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0826 - acc: 0.9721 - val_loss: 0.2098 - val_acc: 0.9567\n",
      "Epoch 145/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0787 - acc: 0.9725 - val_loss: 0.2137 - val_acc: 0.9565\n",
      "Epoch 146/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0795 - acc: 0.9735 - val_loss: 0.2105 - val_acc: 0.9575\n",
      "Epoch 147/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0773 - acc: 0.9735 - val_loss: 0.2126 - val_acc: 0.9573\n",
      "Epoch 148/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0694 - acc: 0.9772 - val_loss: 0.2131 - val_acc: 0.9570\n",
      "Epoch 149/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0757 - acc: 0.9752 - val_loss: 0.2182 - val_acc: 0.9565\n",
      "Epoch 150/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0864 - acc: 0.9706 - val_loss: 0.2162 - val_acc: 0.9567\n",
      "Epoch 151/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0641 - acc: 0.9791 - val_loss: 0.2102 - val_acc: 0.9581\n",
      "Epoch 152/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0796 - acc: 0.9732 - val_loss: 0.2163 - val_acc: 0.9555\n",
      "Epoch 153/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0710 - acc: 0.9760 - val_loss: 0.2128 - val_acc: 0.9571\n",
      "Epoch 154/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0736 - acc: 0.9753 - val_loss: 0.2123 - val_acc: 0.9576\n",
      "Epoch 155/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0830 - acc: 0.9727 - val_loss: 0.2130 - val_acc: 0.9569\n",
      "Epoch 156/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0839 - acc: 0.9712 - val_loss: 0.2109 - val_acc: 0.9578\n",
      "Epoch 157/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0741 - acc: 0.9751 - val_loss: 0.2084 - val_acc: 0.9575\n",
      "Epoch 158/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0659 - acc: 0.9779 - val_loss: 0.2117 - val_acc: 0.9584\n",
      "Epoch 159/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0831 - acc: 0.9721 - val_loss: 0.2051 - val_acc: 0.9589\n",
      "Epoch 160/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0743 - acc: 0.9751 - val_loss: 0.2135 - val_acc: 0.9578\n",
      "Epoch 161/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0681 - acc: 0.9769 - val_loss: 0.2159 - val_acc: 0.9568\n",
      "Epoch 162/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0655 - acc: 0.9780 - val_loss: 0.2156 - val_acc: 0.9567\n",
      "Epoch 163/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0733 - acc: 0.9744 - val_loss: 0.2150 - val_acc: 0.9571\n",
      "Epoch 164/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0747 - acc: 0.9751 - val_loss: 0.2117 - val_acc: 0.9567\n",
      "Epoch 165/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0680 - acc: 0.9776 - val_loss: 0.2085 - val_acc: 0.9591\n",
      "Epoch 166/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0640 - acc: 0.9785 - val_loss: 0.2233 - val_acc: 0.9564\n",
      "Epoch 167/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0740 - acc: 0.9763 - val_loss: 0.2167 - val_acc: 0.9568\n",
      "Epoch 168/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0739 - acc: 0.9751 - val_loss: 0.2145 - val_acc: 0.9572\n",
      "Epoch 169/500\n",
      "42000/42000 [==============================] - 5s 131us/sample - loss: 0.0743 - acc: 0.9750 - val_loss: 0.2153 - val_acc: 0.9571\n",
      "Epoch 170/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0786 - acc: 0.9742 - val_loss: 0.2110 - val_acc: 0.9581\n",
      "Epoch 171/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0835 - acc: 0.9721 - val_loss: 0.2082 - val_acc: 0.9591\n",
      "Epoch 172/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0741 - acc: 0.9750 - val_loss: 0.2123 - val_acc: 0.9570\n",
      "Epoch 173/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0626 - acc: 0.9793 - val_loss: 0.2142 - val_acc: 0.9573\n",
      "Epoch 174/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0684 - acc: 0.9775 - val_loss: 0.2110 - val_acc: 0.9585\n",
      "Epoch 175/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0746 - acc: 0.9755 - val_loss: 0.2112 - val_acc: 0.9582\n",
      "Epoch 176/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0621 - acc: 0.9791 - val_loss: 0.2127 - val_acc: 0.9580\n",
      "Epoch 177/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0645 - acc: 0.9785 - val_loss: 0.2104 - val_acc: 0.9589\n",
      "Epoch 178/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0726 - acc: 0.9752 - val_loss: 0.2129 - val_acc: 0.9587\n",
      "Epoch 179/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0695 - acc: 0.9767 - val_loss: 0.2144 - val_acc: 0.9577\n",
      "Epoch 180/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0641 - acc: 0.9788 - val_loss: 0.2162 - val_acc: 0.9565\n",
      "Epoch 181/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0636 - acc: 0.9785 - val_loss: 0.2200 - val_acc: 0.9555\n",
      "Epoch 182/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0597 - acc: 0.9803 - val_loss: 0.2200 - val_acc: 0.9559\n",
      "Epoch 183/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0966 - acc: 0.9680 - val_loss: 0.2150 - val_acc: 0.9584\n",
      "Epoch 184/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0628 - acc: 0.9791 - val_loss: 0.2151 - val_acc: 0.9579\n",
      "Epoch 185/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0636 - acc: 0.9787 - val_loss: 0.2181 - val_acc: 0.9579\n",
      "Epoch 186/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0564 - acc: 0.9814 - val_loss: 0.2162 - val_acc: 0.9578\n",
      "Epoch 187/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0628 - acc: 0.9788 - val_loss: 0.2131 - val_acc: 0.9584\n",
      "Epoch 188/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0640 - acc: 0.9788 - val_loss: 0.2174 - val_acc: 0.9578\n",
      "Epoch 189/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0711 - acc: 0.9765 - val_loss: 0.2189 - val_acc: 0.9570\n",
      "Epoch 190/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0817 - acc: 0.9735 - val_loss: 0.2181 - val_acc: 0.9572\n",
      "Epoch 191/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0650 - acc: 0.9784 - val_loss: 0.2144 - val_acc: 0.9582\n",
      "Epoch 192/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0600 - acc: 0.9800 - val_loss: 0.2172 - val_acc: 0.9583\n",
      "Epoch 193/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0576 - acc: 0.9813 - val_loss: 0.2220 - val_acc: 0.9568\n",
      "Epoch 194/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0582 - acc: 0.9803 - val_loss: 0.2295 - val_acc: 0.9559\n",
      "Epoch 195/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0705 - acc: 0.9763 - val_loss: 0.2189 - val_acc: 0.9584\n",
      "Epoch 196/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0619 - acc: 0.9798 - val_loss: 0.2215 - val_acc: 0.9577\n",
      "Epoch 197/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0664 - acc: 0.9767 - val_loss: 0.2180 - val_acc: 0.9585\n",
      "Epoch 198/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0570 - acc: 0.9810 - val_loss: 0.2171 - val_acc: 0.9581\n",
      "Epoch 199/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0611 - acc: 0.9792 - val_loss: 0.2144 - val_acc: 0.9592\n",
      "Epoch 200/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0644 - acc: 0.9777 - val_loss: 0.2232 - val_acc: 0.9568\n",
      "Epoch 201/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0692 - acc: 0.9775 - val_loss: 0.2166 - val_acc: 0.9586\n",
      "Epoch 202/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0746 - acc: 0.9758 - val_loss: 0.2183 - val_acc: 0.9588\n",
      "Epoch 203/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0840 - acc: 0.9720 - val_loss: 0.2150 - val_acc: 0.9585\n",
      "Epoch 204/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0604 - acc: 0.9797 - val_loss: 0.2171 - val_acc: 0.9589\n",
      "Epoch 205/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0625 - acc: 0.9795 - val_loss: 0.2097 - val_acc: 0.9602\n",
      "Epoch 206/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0577 - acc: 0.9810 - val_loss: 0.2107 - val_acc: 0.9597\n",
      "Epoch 207/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0656 - acc: 0.9781 - val_loss: 0.2176 - val_acc: 0.9580\n",
      "Epoch 208/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0650 - acc: 0.9784 - val_loss: 0.2159 - val_acc: 0.9595\n",
      "Epoch 209/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0604 - acc: 0.9801 - val_loss: 0.2183 - val_acc: 0.9586\n",
      "Epoch 210/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0750 - acc: 0.9761 - val_loss: 0.2131 - val_acc: 0.9590\n",
      "Epoch 211/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0564 - acc: 0.9811 - val_loss: 0.2178 - val_acc: 0.9580\n",
      "Epoch 212/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0669 - acc: 0.9779 - val_loss: 0.2210 - val_acc: 0.9581\n",
      "Epoch 213/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0585 - acc: 0.9803 - val_loss: 0.2150 - val_acc: 0.9603\n",
      "Epoch 214/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0517 - acc: 0.9834 - val_loss: 0.2170 - val_acc: 0.9594\n",
      "Epoch 215/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0563 - acc: 0.9812 - val_loss: 0.2222 - val_acc: 0.9574\n",
      "Epoch 216/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0567 - acc: 0.9812 - val_loss: 0.2170 - val_acc: 0.9596\n",
      "Epoch 217/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0688 - acc: 0.9775 - val_loss: 0.2235 - val_acc: 0.9583\n",
      "Epoch 218/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0712 - acc: 0.9768 - val_loss: 0.2163 - val_acc: 0.9593\n",
      "Epoch 219/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0558 - acc: 0.9816 - val_loss: 0.2156 - val_acc: 0.9600\n",
      "Epoch 220/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0587 - acc: 0.9808 - val_loss: 0.2202 - val_acc: 0.9589\n",
      "Epoch 221/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0623 - acc: 0.9786 - val_loss: 0.2153 - val_acc: 0.9589\n",
      "Epoch 222/500\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.0582 - acc: 0.9813 - val_loss: 0.2181 - val_acc: 0.9579\n",
      "Epoch 223/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0674 - acc: 0.9780 - val_loss: 0.2163 - val_acc: 0.9588\n",
      "Epoch 224/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0558 - acc: 0.9811 - val_loss: 0.2163 - val_acc: 0.9595\n",
      "Epoch 225/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0612 - acc: 0.9792 - val_loss: 0.2168 - val_acc: 0.9582\n",
      "Epoch 226/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0581 - acc: 0.9806 - val_loss: 0.2163 - val_acc: 0.9592\n",
      "Epoch 227/500\n",
      "42000/42000 [==============================] - 6s 132us/sample - loss: 0.0642 - acc: 0.9790 - val_loss: 0.2172 - val_acc: 0.9584\n",
      "Epoch 228/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0497 - acc: 0.9832 - val_loss: 0.2166 - val_acc: 0.9596\n",
      "Epoch 229/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0533 - acc: 0.9825 - val_loss: 0.2159 - val_acc: 0.9596\n",
      "Epoch 230/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0506 - acc: 0.9824 - val_loss: 0.2234 - val_acc: 0.9579\n",
      "Epoch 231/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0727 - acc: 0.9765 - val_loss: 0.2199 - val_acc: 0.9589\n",
      "Epoch 232/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0544 - acc: 0.9813 - val_loss: 0.2179 - val_acc: 0.9590\n",
      "Epoch 233/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0537 - acc: 0.9823 - val_loss: 0.2197 - val_acc: 0.9598\n",
      "Epoch 234/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0618 - acc: 0.9793 - val_loss: 0.2240 - val_acc: 0.9571\n",
      "Epoch 235/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0549 - acc: 0.9813 - val_loss: 0.2157 - val_acc: 0.9595\n",
      "Epoch 236/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0514 - acc: 0.9830 - val_loss: 0.2183 - val_acc: 0.9594\n",
      "Epoch 237/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0610 - acc: 0.9796 - val_loss: 0.2215 - val_acc: 0.9592\n",
      "Epoch 238/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0535 - acc: 0.9820 - val_loss: 0.2199 - val_acc: 0.9590\n",
      "Epoch 239/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0594 - acc: 0.9800 - val_loss: 0.2236 - val_acc: 0.9586\n",
      "Epoch 240/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0567 - acc: 0.9805 - val_loss: 0.2232 - val_acc: 0.9584\n",
      "Epoch 241/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0600 - acc: 0.9797 - val_loss: 0.2240 - val_acc: 0.9585\n",
      "Epoch 242/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0519 - acc: 0.9830 - val_loss: 0.2210 - val_acc: 0.9594\n",
      "Epoch 243/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0684 - acc: 0.9776 - val_loss: 0.2212 - val_acc: 0.9591\n",
      "Epoch 244/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0566 - acc: 0.9818 - val_loss: 0.2197 - val_acc: 0.9595\n",
      "Epoch 245/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0670 - acc: 0.9778 - val_loss: 0.2188 - val_acc: 0.9601\n",
      "Epoch 246/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0520 - acc: 0.9827 - val_loss: 0.2176 - val_acc: 0.9597\n",
      "Epoch 247/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0521 - acc: 0.9829 - val_loss: 0.2167 - val_acc: 0.9599\n",
      "Epoch 248/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0601 - acc: 0.9793 - val_loss: 0.2184 - val_acc: 0.9601\n",
      "Epoch 249/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0563 - acc: 0.9815 - val_loss: 0.2188 - val_acc: 0.9596\n",
      "Epoch 250/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0459 - acc: 0.9846 - val_loss: 0.2163 - val_acc: 0.9603\n",
      "Epoch 251/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0517 - acc: 0.9828 - val_loss: 0.2161 - val_acc: 0.9599\n",
      "Epoch 252/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0571 - acc: 0.9810 - val_loss: 0.2212 - val_acc: 0.9595\n",
      "Epoch 253/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0448 - acc: 0.9852 - val_loss: 0.2252 - val_acc: 0.9579\n",
      "Epoch 254/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0523 - acc: 0.9816 - val_loss: 0.2213 - val_acc: 0.9592\n",
      "Epoch 255/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0614 - acc: 0.9798 - val_loss: 0.2232 - val_acc: 0.9594\n",
      "Epoch 256/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0521 - acc: 0.9820 - val_loss: 0.2262 - val_acc: 0.9582\n",
      "Epoch 257/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0526 - acc: 0.9824 - val_loss: 0.2216 - val_acc: 0.9596\n",
      "Epoch 258/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0530 - acc: 0.9818 - val_loss: 0.2243 - val_acc: 0.9591\n",
      "Epoch 259/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0564 - acc: 0.9810 - val_loss: 0.2256 - val_acc: 0.9589\n",
      "Epoch 260/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0443 - acc: 0.9854 - val_loss: 0.2244 - val_acc: 0.9586\n",
      "Epoch 261/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0486 - acc: 0.9842 - val_loss: 0.2228 - val_acc: 0.9591\n",
      "Epoch 262/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0583 - acc: 0.9803 - val_loss: 0.2238 - val_acc: 0.9589\n",
      "Epoch 263/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0531 - acc: 0.9821 - val_loss: 0.2194 - val_acc: 0.9596\n",
      "Epoch 264/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0530 - acc: 0.9818 - val_loss: 0.2215 - val_acc: 0.9589\n",
      "Epoch 265/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0517 - acc: 0.9823 - val_loss: 0.2232 - val_acc: 0.9591\n",
      "Epoch 266/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0593 - acc: 0.9800 - val_loss: 0.2225 - val_acc: 0.9599\n",
      "Epoch 267/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0488 - acc: 0.9832 - val_loss: 0.2185 - val_acc: 0.9602\n",
      "Epoch 268/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0465 - acc: 0.9845 - val_loss: 0.2272 - val_acc: 0.9583\n",
      "Epoch 269/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0503 - acc: 0.9835 - val_loss: 0.2248 - val_acc: 0.9593\n",
      "Epoch 270/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0525 - acc: 0.9829 - val_loss: 0.2233 - val_acc: 0.9590\n",
      "Epoch 271/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0770 - acc: 0.9751 - val_loss: 0.2214 - val_acc: 0.9596\n",
      "Epoch 272/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0485 - acc: 0.9842 - val_loss: 0.2237 - val_acc: 0.9592\n",
      "Epoch 273/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0503 - acc: 0.9831 - val_loss: 0.2250 - val_acc: 0.9595\n",
      "Epoch 274/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0579 - acc: 0.9809 - val_loss: 0.2232 - val_acc: 0.9592\n",
      "Epoch 275/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0560 - acc: 0.9810 - val_loss: 0.2214 - val_acc: 0.9603\n",
      "Epoch 276/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0432 - acc: 0.9854 - val_loss: 0.2255 - val_acc: 0.9582\n",
      "Epoch 277/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0510 - acc: 0.9838 - val_loss: 0.2234 - val_acc: 0.9601\n",
      "Epoch 278/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0486 - acc: 0.9847 - val_loss: 0.2249 - val_acc: 0.9593\n",
      "Epoch 279/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0508 - acc: 0.9831 - val_loss: 0.2278 - val_acc: 0.9579\n",
      "Epoch 280/500\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.0592 - acc: 0.9801 - val_loss: 0.2272 - val_acc: 0.9589\n",
      "Epoch 281/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0563 - acc: 0.9817 - val_loss: 0.2231 - val_acc: 0.9595\n",
      "Epoch 282/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0501 - acc: 0.9828 - val_loss: 0.2243 - val_acc: 0.9587\n",
      "Epoch 283/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0430 - acc: 0.9858 - val_loss: 0.2242 - val_acc: 0.9600\n",
      "Epoch 284/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0590 - acc: 0.9803 - val_loss: 0.2284 - val_acc: 0.9590\n",
      "Epoch 285/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0419 - acc: 0.9855 - val_loss: 0.2251 - val_acc: 0.9592\n",
      "Epoch 286/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0508 - acc: 0.9831 - val_loss: 0.2296 - val_acc: 0.9578\n",
      "Epoch 287/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0614 - acc: 0.9804 - val_loss: 0.2242 - val_acc: 0.9590\n",
      "Epoch 288/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0510 - acc: 0.9830 - val_loss: 0.2248 - val_acc: 0.9591\n",
      "Epoch 289/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0659 - acc: 0.9783 - val_loss: 0.2222 - val_acc: 0.9590\n",
      "Epoch 290/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0448 - acc: 0.9851 - val_loss: 0.2235 - val_acc: 0.9591\n",
      "Epoch 291/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0466 - acc: 0.9845 - val_loss: 0.2280 - val_acc: 0.9579\n",
      "Epoch 292/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0387 - acc: 0.9877 - val_loss: 0.2313 - val_acc: 0.9575\n",
      "Epoch 293/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0551 - acc: 0.9823 - val_loss: 0.2235 - val_acc: 0.9597\n",
      "Epoch 294/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0413 - acc: 0.9867 - val_loss: 0.2312 - val_acc: 0.9585\n",
      "Epoch 295/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0455 - acc: 0.9851 - val_loss: 0.2307 - val_acc: 0.9587\n",
      "Epoch 296/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0430 - acc: 0.9860 - val_loss: 0.2307 - val_acc: 0.9593\n",
      "Epoch 297/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0544 - acc: 0.9817 - val_loss: 0.2258 - val_acc: 0.9600\n",
      "Epoch 298/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0480 - acc: 0.9845 - val_loss: 0.2274 - val_acc: 0.9590\n",
      "Epoch 299/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0486 - acc: 0.9836 - val_loss: 0.2262 - val_acc: 0.9602\n",
      "Epoch 300/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0507 - acc: 0.9842 - val_loss: 0.2242 - val_acc: 0.9593\n",
      "Epoch 301/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0416 - acc: 0.9873 - val_loss: 0.2270 - val_acc: 0.9600\n",
      "Epoch 302/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0440 - acc: 0.9858 - val_loss: 0.2287 - val_acc: 0.9596\n",
      "Epoch 303/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0535 - acc: 0.9827 - val_loss: 0.2262 - val_acc: 0.9595\n",
      "Epoch 304/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0514 - acc: 0.9831 - val_loss: 0.2266 - val_acc: 0.9597\n",
      "Epoch 305/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0479 - acc: 0.9845 - val_loss: 0.2246 - val_acc: 0.9599\n",
      "Epoch 306/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0469 - acc: 0.9843 - val_loss: 0.2281 - val_acc: 0.9599\n",
      "Epoch 307/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0428 - acc: 0.9863 - val_loss: 0.2301 - val_acc: 0.9591\n",
      "Epoch 308/500\n",
      "42000/42000 [==============================] - 5s 120us/sample - loss: 0.0645 - acc: 0.9789 - val_loss: 0.2276 - val_acc: 0.9601\n",
      "Epoch 309/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0533 - acc: 0.9826 - val_loss: 0.2267 - val_acc: 0.9605\n",
      "Epoch 310/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0424 - acc: 0.9859 - val_loss: 0.2287 - val_acc: 0.9589\n",
      "Epoch 311/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0413 - acc: 0.9869 - val_loss: 0.2293 - val_acc: 0.9594\n",
      "Epoch 312/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0422 - acc: 0.9860 - val_loss: 0.2351 - val_acc: 0.9583\n",
      "Epoch 313/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0503 - acc: 0.9840 - val_loss: 0.2323 - val_acc: 0.9589\n",
      "Epoch 314/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0450 - acc: 0.9851 - val_loss: 0.2360 - val_acc: 0.9589\n",
      "Epoch 315/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0612 - acc: 0.9803 - val_loss: 0.2329 - val_acc: 0.9590\n",
      "Epoch 316/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0497 - acc: 0.9831 - val_loss: 0.2266 - val_acc: 0.9600\n",
      "Epoch 317/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0515 - acc: 0.9825 - val_loss: 0.2280 - val_acc: 0.9593\n",
      "Epoch 318/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0487 - acc: 0.9840 - val_loss: 0.2266 - val_acc: 0.9598\n",
      "Epoch 319/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0581 - acc: 0.9816 - val_loss: 0.2316 - val_acc: 0.9578\n",
      "Epoch 320/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0696 - acc: 0.9772 - val_loss: 0.2242 - val_acc: 0.9600\n",
      "Epoch 321/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0475 - acc: 0.9840 - val_loss: 0.2241 - val_acc: 0.9601\n",
      "Epoch 322/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0489 - acc: 0.9838 - val_loss: 0.2253 - val_acc: 0.9599\n",
      "Epoch 323/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0369 - acc: 0.9877 - val_loss: 0.2278 - val_acc: 0.9596\n",
      "Epoch 324/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0401 - acc: 0.9862 - val_loss: 0.2286 - val_acc: 0.9593\n",
      "Epoch 325/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0358 - acc: 0.9880 - val_loss: 0.2276 - val_acc: 0.9598\n",
      "Epoch 326/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0422 - acc: 0.9860 - val_loss: 0.2341 - val_acc: 0.9592\n",
      "Epoch 327/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0577 - acc: 0.9810 - val_loss: 0.2307 - val_acc: 0.9596\n",
      "Epoch 328/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0458 - acc: 0.9852 - val_loss: 0.2319 - val_acc: 0.9586\n",
      "Epoch 329/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0456 - acc: 0.9853 - val_loss: 0.2244 - val_acc: 0.9604\n",
      "Epoch 330/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0519 - acc: 0.9823 - val_loss: 0.2340 - val_acc: 0.9587\n",
      "Epoch 331/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0412 - acc: 0.9865 - val_loss: 0.2299 - val_acc: 0.9599\n",
      "Epoch 332/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0516 - acc: 0.9831 - val_loss: 0.2281 - val_acc: 0.9595\n",
      "Epoch 333/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0507 - acc: 0.9840 - val_loss: 0.2254 - val_acc: 0.9601\n",
      "Epoch 334/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0451 - acc: 0.9849 - val_loss: 0.2269 - val_acc: 0.9600\n",
      "Epoch 335/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0398 - acc: 0.9868 - val_loss: 0.2270 - val_acc: 0.9603\n",
      "Epoch 336/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0358 - acc: 0.9880 - val_loss: 0.2323 - val_acc: 0.9598\n",
      "Epoch 337/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0380 - acc: 0.9878 - val_loss: 0.2354 - val_acc: 0.9596\n",
      "Epoch 338/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0559 - acc: 0.9817 - val_loss: 0.2291 - val_acc: 0.9598\n",
      "Epoch 339/500\n",
      "42000/42000 [==============================] - 5s 130us/sample - loss: 0.0453 - acc: 0.9844 - val_loss: 0.2329 - val_acc: 0.9591\n",
      "Epoch 340/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0426 - acc: 0.9861 - val_loss: 0.2275 - val_acc: 0.9599\n",
      "Epoch 341/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0453 - acc: 0.9847 - val_loss: 0.2267 - val_acc: 0.9599\n",
      "Epoch 342/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0399 - acc: 0.9866 - val_loss: 0.2313 - val_acc: 0.9595\n",
      "Epoch 343/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0385 - acc: 0.9874 - val_loss: 0.2328 - val_acc: 0.9592\n",
      "Epoch 344/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 0.2298 - val_acc: 0.9601\n",
      "Epoch 345/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0392 - acc: 0.9873 - val_loss: 0.2350 - val_acc: 0.9587\n",
      "Epoch 346/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0495 - acc: 0.9837 - val_loss: 0.2289 - val_acc: 0.9603\n",
      "Epoch 347/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0404 - acc: 0.9865 - val_loss: 0.2274 - val_acc: 0.9604\n",
      "Epoch 348/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0506 - acc: 0.9832 - val_loss: 0.2311 - val_acc: 0.9592\n",
      "Epoch 349/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0488 - acc: 0.9842 - val_loss: 0.2339 - val_acc: 0.9594\n",
      "Epoch 350/500\n",
      "42000/42000 [==============================] - 5s 120us/sample - loss: 0.0375 - acc: 0.9880 - val_loss: 0.2321 - val_acc: 0.9597\n",
      "Epoch 351/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0429 - acc: 0.9858 - val_loss: 0.2328 - val_acc: 0.9598\n",
      "Epoch 352/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0351 - acc: 0.9883 - val_loss: 0.2353 - val_acc: 0.9594\n",
      "Epoch 353/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0478 - acc: 0.9839 - val_loss: 0.2326 - val_acc: 0.9601\n",
      "Epoch 354/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0382 - acc: 0.9872 - val_loss: 0.2394 - val_acc: 0.9586\n",
      "Epoch 355/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0434 - acc: 0.9860 - val_loss: 0.2343 - val_acc: 0.9593\n",
      "Epoch 356/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0514 - acc: 0.9833 - val_loss: 0.2390 - val_acc: 0.9588\n",
      "Epoch 357/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0483 - acc: 0.9843 - val_loss: 0.2327 - val_acc: 0.9597\n",
      "Epoch 358/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0359 - acc: 0.9884 - val_loss: 0.2354 - val_acc: 0.9600\n",
      "Epoch 359/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0436 - acc: 0.9857 - val_loss: 0.2347 - val_acc: 0.9598\n",
      "Epoch 360/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0322 - acc: 0.9889 - val_loss: 0.2365 - val_acc: 0.9593\n",
      "Epoch 361/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0468 - acc: 0.9843 - val_loss: 0.2363 - val_acc: 0.9597\n",
      "Epoch 362/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0498 - acc: 0.9836 - val_loss: 0.2327 - val_acc: 0.9596\n",
      "Epoch 363/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0442 - acc: 0.9859 - val_loss: 0.2355 - val_acc: 0.9601\n",
      "Epoch 364/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0728 - acc: 0.9771 - val_loss: 0.2368 - val_acc: 0.9592\n",
      "Epoch 365/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0443 - acc: 0.9848 - val_loss: 0.2318 - val_acc: 0.9597\n",
      "Epoch 366/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0409 - acc: 0.9864 - val_loss: 0.2305 - val_acc: 0.9605\n",
      "Epoch 367/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0419 - acc: 0.9866 - val_loss: 0.2308 - val_acc: 0.9606\n",
      "Epoch 368/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0456 - acc: 0.9848 - val_loss: 0.2287 - val_acc: 0.9613\n",
      "Epoch 369/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0474 - acc: 0.9845 - val_loss: 0.2316 - val_acc: 0.9600\n",
      "Epoch 370/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0481 - acc: 0.9844 - val_loss: 0.2294 - val_acc: 0.9601\n",
      "Epoch 371/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0363 - acc: 0.9879 - val_loss: 0.2328 - val_acc: 0.9595\n",
      "Epoch 372/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0469 - acc: 0.9841 - val_loss: 0.2324 - val_acc: 0.9601\n",
      "Epoch 373/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0373 - acc: 0.9881 - val_loss: 0.2337 - val_acc: 0.9594\n",
      "Epoch 374/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0446 - acc: 0.9856 - val_loss: 0.2303 - val_acc: 0.9610\n",
      "Epoch 375/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0417 - acc: 0.9863 - val_loss: 0.2314 - val_acc: 0.9601\n",
      "Epoch 376/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0439 - acc: 0.9852 - val_loss: 0.2321 - val_acc: 0.9605\n",
      "Epoch 377/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0386 - acc: 0.9874 - val_loss: 0.2324 - val_acc: 0.9606\n",
      "Epoch 378/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0369 - acc: 0.9877 - val_loss: 0.2315 - val_acc: 0.9601\n",
      "Epoch 379/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0345 - acc: 0.9886 - val_loss: 0.2324 - val_acc: 0.9603\n",
      "Epoch 380/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0405 - acc: 0.9864 - val_loss: 0.2405 - val_acc: 0.9585\n",
      "Epoch 381/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0511 - acc: 0.9840 - val_loss: 0.2336 - val_acc: 0.9597\n",
      "Epoch 382/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0464 - acc: 0.9854 - val_loss: 0.2304 - val_acc: 0.9597\n",
      "Epoch 383/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0359 - acc: 0.9876 - val_loss: 0.2301 - val_acc: 0.9602\n",
      "Epoch 384/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0390 - acc: 0.9866 - val_loss: 0.2312 - val_acc: 0.9600\n",
      "Epoch 385/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0470 - acc: 0.9844 - val_loss: 0.2321 - val_acc: 0.9600\n",
      "Epoch 386/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0462 - acc: 0.9852 - val_loss: 0.2325 - val_acc: 0.9602\n",
      "Epoch 387/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0530 - acc: 0.9825 - val_loss: 0.2316 - val_acc: 0.9603\n",
      "Epoch 388/500\n",
      "42000/42000 [==============================] - 5s 120us/sample - loss: 0.0387 - acc: 0.9873 - val_loss: 0.2314 - val_acc: 0.9597\n",
      "Epoch 389/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0449 - acc: 0.9844 - val_loss: 0.2295 - val_acc: 0.9599\n",
      "Epoch 390/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0334 - acc: 0.9889 - val_loss: 0.2325 - val_acc: 0.9597\n",
      "Epoch 391/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0397 - acc: 0.9868 - val_loss: 0.2314 - val_acc: 0.9605\n",
      "Epoch 392/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0455 - acc: 0.9850 - val_loss: 0.2316 - val_acc: 0.9601\n",
      "Epoch 393/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0389 - acc: 0.9870 - val_loss: 0.2296 - val_acc: 0.9609\n",
      "Epoch 394/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0434 - acc: 0.9862 - val_loss: 0.2308 - val_acc: 0.9604\n",
      "Epoch 395/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0372 - acc: 0.9880 - val_loss: 0.2310 - val_acc: 0.9609\n",
      "Epoch 396/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0393 - acc: 0.9870 - val_loss: 0.2359 - val_acc: 0.9595\n",
      "Epoch 397/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0455 - acc: 0.9856 - val_loss: 0.2323 - val_acc: 0.9604\n",
      "Epoch 398/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0349 - acc: 0.9890 - val_loss: 0.2329 - val_acc: 0.9596\n",
      "Epoch 399/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0573 - acc: 0.9812 - val_loss: 0.2315 - val_acc: 0.9604\n",
      "Epoch 400/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0380 - acc: 0.9872 - val_loss: 0.2336 - val_acc: 0.9600\n",
      "Epoch 401/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0380 - acc: 0.9879 - val_loss: 0.2329 - val_acc: 0.9600\n",
      "Epoch 402/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0307 - acc: 0.9902 - val_loss: 0.2332 - val_acc: 0.9601\n",
      "Epoch 403/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0426 - acc: 0.9861 - val_loss: 0.2356 - val_acc: 0.9595\n",
      "Epoch 404/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0463 - acc: 0.9852 - val_loss: 0.2363 - val_acc: 0.9599\n",
      "Epoch 405/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0460 - acc: 0.9846 - val_loss: 0.2304 - val_acc: 0.9604\n",
      "Epoch 406/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0419 - acc: 0.9869 - val_loss: 0.2364 - val_acc: 0.9593\n",
      "Epoch 407/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0517 - acc: 0.9838 - val_loss: 0.2333 - val_acc: 0.9603\n",
      "Epoch 408/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0444 - acc: 0.9851 - val_loss: 0.2320 - val_acc: 0.9597\n",
      "Epoch 409/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0370 - acc: 0.9873 - val_loss: 0.2329 - val_acc: 0.9599\n",
      "Epoch 410/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0370 - acc: 0.9882 - val_loss: 0.2359 - val_acc: 0.9602\n",
      "Epoch 411/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0401 - acc: 0.9865 - val_loss: 0.2372 - val_acc: 0.9597\n",
      "Epoch 412/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0398 - acc: 0.9873 - val_loss: 0.2368 - val_acc: 0.9597\n",
      "Epoch 413/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0378 - acc: 0.9874 - val_loss: 0.2346 - val_acc: 0.9595\n",
      "Epoch 414/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0364 - acc: 0.9880 - val_loss: 0.2338 - val_acc: 0.9604\n",
      "Epoch 415/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0495 - acc: 0.9838 - val_loss: 0.2323 - val_acc: 0.9605\n",
      "Epoch 416/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0417 - acc: 0.9872 - val_loss: 0.2375 - val_acc: 0.9599\n",
      "Epoch 417/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0406 - acc: 0.9869 - val_loss: 0.2384 - val_acc: 0.9593\n",
      "Epoch 418/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0375 - acc: 0.9881 - val_loss: 0.2370 - val_acc: 0.9599\n",
      "Epoch 419/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0328 - acc: 0.9887 - val_loss: 0.2401 - val_acc: 0.9590\n",
      "Epoch 420/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0333 - acc: 0.9893 - val_loss: 0.2418 - val_acc: 0.9593\n",
      "Epoch 421/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0523 - acc: 0.9833 - val_loss: 0.2368 - val_acc: 0.9599\n",
      "Epoch 422/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0405 - acc: 0.9869 - val_loss: 0.2345 - val_acc: 0.9590\n",
      "Epoch 423/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0344 - acc: 0.9894 - val_loss: 0.2352 - val_acc: 0.9589\n",
      "Epoch 424/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0472 - acc: 0.9841 - val_loss: 0.2352 - val_acc: 0.9599\n",
      "Epoch 425/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0330 - acc: 0.9894 - val_loss: 0.2337 - val_acc: 0.9596\n",
      "Epoch 426/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0363 - acc: 0.9879 - val_loss: 0.2354 - val_acc: 0.9596\n",
      "Epoch 427/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0406 - acc: 0.9867 - val_loss: 0.2379 - val_acc: 0.9596\n",
      "Epoch 428/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0309 - acc: 0.9897 - val_loss: 0.2373 - val_acc: 0.9596\n",
      "Epoch 429/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0310 - acc: 0.9898 - val_loss: 0.2332 - val_acc: 0.9594\n",
      "Epoch 430/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0339 - acc: 0.9889 - val_loss: 0.2391 - val_acc: 0.9596\n",
      "Epoch 431/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0387 - acc: 0.9877 - val_loss: 0.2405 - val_acc: 0.9590\n",
      "Epoch 432/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0352 - acc: 0.9882 - val_loss: 0.2355 - val_acc: 0.9597\n",
      "Epoch 433/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0466 - acc: 0.9844 - val_loss: 0.2366 - val_acc: 0.9602\n",
      "Epoch 434/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0374 - acc: 0.9881 - val_loss: 0.2365 - val_acc: 0.9598\n",
      "Epoch 435/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0351 - acc: 0.9888 - val_loss: 0.2364 - val_acc: 0.9596\n",
      "Epoch 436/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0407 - acc: 0.9864 - val_loss: 0.2375 - val_acc: 0.9595\n",
      "Epoch 437/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0372 - acc: 0.9883 - val_loss: 0.2364 - val_acc: 0.9601\n",
      "Epoch 438/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0355 - acc: 0.9884 - val_loss: 0.2452 - val_acc: 0.9589\n",
      "Epoch 439/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0418 - acc: 0.9860 - val_loss: 0.2382 - val_acc: 0.9595\n",
      "Epoch 440/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0455 - acc: 0.9851 - val_loss: 0.2382 - val_acc: 0.9598\n",
      "Epoch 441/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0455 - acc: 0.9846 - val_loss: 0.2392 - val_acc: 0.9601\n",
      "Epoch 442/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0383 - acc: 0.9871 - val_loss: 0.2385 - val_acc: 0.9604\n",
      "Epoch 443/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0330 - acc: 0.9887 - val_loss: 0.2388 - val_acc: 0.9602\n",
      "Epoch 444/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0342 - acc: 0.9882 - val_loss: 0.2337 - val_acc: 0.9602\n",
      "Epoch 445/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0400 - acc: 0.9865 - val_loss: 0.2352 - val_acc: 0.9602\n",
      "Epoch 446/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0420 - acc: 0.9860 - val_loss: 0.2366 - val_acc: 0.9603\n",
      "Epoch 447/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0431 - acc: 0.9860 - val_loss: 0.2339 - val_acc: 0.9606\n",
      "Epoch 448/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0328 - acc: 0.9891 - val_loss: 0.2376 - val_acc: 0.9600\n",
      "Epoch 449/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0330 - acc: 0.9898 - val_loss: 0.2348 - val_acc: 0.9604\n",
      "Epoch 450/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0366 - acc: 0.9877 - val_loss: 0.2388 - val_acc: 0.9602\n",
      "Epoch 451/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0388 - acc: 0.9878 - val_loss: 0.2338 - val_acc: 0.9606\n",
      "Epoch 452/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0436 - acc: 0.9859 - val_loss: 0.2370 - val_acc: 0.9597\n",
      "Epoch 453/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0374 - acc: 0.9880 - val_loss: 0.2340 - val_acc: 0.9605\n",
      "Epoch 454/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0358 - acc: 0.9881 - val_loss: 0.2358 - val_acc: 0.9598\n",
      "Epoch 455/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0350 - acc: 0.9885 - val_loss: 0.2374 - val_acc: 0.9604\n",
      "Epoch 456/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0450 - acc: 0.9848 - val_loss: 0.2394 - val_acc: 0.9598\n",
      "Epoch 457/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0325 - acc: 0.9896 - val_loss: 0.2360 - val_acc: 0.9597\n",
      "Epoch 458/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0376 - acc: 0.9882 - val_loss: 0.2370 - val_acc: 0.9599\n",
      "Epoch 459/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0345 - acc: 0.9888 - val_loss: 0.2384 - val_acc: 0.9604\n",
      "Epoch 460/500\n",
      "42000/42000 [==============================] - 5s 129us/sample - loss: 0.0423 - acc: 0.9864 - val_loss: 0.2387 - val_acc: 0.9594\n",
      "Epoch 461/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0491 - acc: 0.9845 - val_loss: 0.2337 - val_acc: 0.9604\n",
      "Epoch 462/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0303 - acc: 0.9900 - val_loss: 0.2359 - val_acc: 0.9605\n",
      "Epoch 463/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0334 - acc: 0.9893 - val_loss: 0.2377 - val_acc: 0.9597\n",
      "Epoch 464/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0434 - acc: 0.9857 - val_loss: 0.2366 - val_acc: 0.9606\n",
      "Epoch 465/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0306 - acc: 0.9905 - val_loss: 0.2347 - val_acc: 0.9608\n",
      "Epoch 466/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0299 - acc: 0.9902 - val_loss: 0.2357 - val_acc: 0.9609\n",
      "Epoch 467/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0344 - acc: 0.9883 - val_loss: 0.2389 - val_acc: 0.9602\n",
      "Epoch 468/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0411 - acc: 0.9866 - val_loss: 0.2371 - val_acc: 0.9608\n",
      "Epoch 469/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0442 - acc: 0.9853 - val_loss: 0.2382 - val_acc: 0.9604\n",
      "Epoch 470/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0347 - acc: 0.9886 - val_loss: 0.2395 - val_acc: 0.9608\n",
      "Epoch 471/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0393 - acc: 0.9879 - val_loss: 0.2400 - val_acc: 0.9608\n",
      "Epoch 472/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0363 - acc: 0.9881 - val_loss: 0.2385 - val_acc: 0.9603\n",
      "Epoch 473/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0364 - acc: 0.9883 - val_loss: 0.2354 - val_acc: 0.9603\n",
      "Epoch 474/500\n",
      "42000/42000 [==============================] - 5s 127us/sample - loss: 0.0440 - acc: 0.9858 - val_loss: 0.2403 - val_acc: 0.9600\n",
      "Epoch 475/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0408 - acc: 0.9871 - val_loss: 0.2379 - val_acc: 0.9606\n",
      "Epoch 476/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0292 - acc: 0.9906 - val_loss: 0.2412 - val_acc: 0.9594\n",
      "Epoch 477/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0390 - acc: 0.9873 - val_loss: 0.2396 - val_acc: 0.9602\n",
      "Epoch 478/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0415 - acc: 0.9864 - val_loss: 0.2404 - val_acc: 0.9598\n",
      "Epoch 479/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0458 - acc: 0.9854 - val_loss: 0.2375 - val_acc: 0.9602\n",
      "Epoch 480/500\n",
      "42000/42000 [==============================] - 5s 121us/sample - loss: 0.0359 - acc: 0.9890 - val_loss: 0.2357 - val_acc: 0.9604\n",
      "Epoch 481/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0304 - acc: 0.9902 - val_loss: 0.2344 - val_acc: 0.9604\n",
      "Epoch 482/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0308 - acc: 0.9902 - val_loss: 0.2351 - val_acc: 0.9607\n",
      "Epoch 483/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0281 - acc: 0.9906 - val_loss: 0.2325 - val_acc: 0.9612\n",
      "Epoch 484/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0304 - acc: 0.9897 - val_loss: 0.2406 - val_acc: 0.9599\n",
      "Epoch 485/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0420 - acc: 0.9857 - val_loss: 0.2369 - val_acc: 0.9608\n",
      "Epoch 486/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0364 - acc: 0.9882 - val_loss: 0.2370 - val_acc: 0.9602\n",
      "Epoch 487/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0314 - acc: 0.9899 - val_loss: 0.2396 - val_acc: 0.9600\n",
      "Epoch 488/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0443 - acc: 0.9857 - val_loss: 0.2397 - val_acc: 0.9600\n",
      "Epoch 489/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0331 - acc: 0.9892 - val_loss: 0.2332 - val_acc: 0.9607\n",
      "Epoch 490/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0278 - acc: 0.9912 - val_loss: 0.2369 - val_acc: 0.9601\n",
      "Epoch 491/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0419 - acc: 0.9869 - val_loss: 0.2355 - val_acc: 0.9610\n",
      "Epoch 492/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0376 - acc: 0.9879 - val_loss: 0.2373 - val_acc: 0.9605\n",
      "Epoch 493/500\n",
      "42000/42000 [==============================] - 5s 122us/sample - loss: 0.0285 - acc: 0.9908 - val_loss: 0.2369 - val_acc: 0.9604\n",
      "Epoch 494/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0373 - acc: 0.9880 - val_loss: 0.2384 - val_acc: 0.9601\n",
      "Epoch 495/500\n",
      "42000/42000 [==============================] - 5s 126us/sample - loss: 0.0336 - acc: 0.9889 - val_loss: 0.2386 - val_acc: 0.9604\n",
      "Epoch 496/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0432 - acc: 0.9859 - val_loss: 0.2397 - val_acc: 0.9607\n",
      "Epoch 497/500\n",
      "42000/42000 [==============================] - 5s 125us/sample - loss: 0.0302 - acc: 0.9901 - val_loss: 0.2371 - val_acc: 0.9605\n",
      "Epoch 498/500\n",
      "42000/42000 [==============================] - 5s 124us/sample - loss: 0.0403 - acc: 0.9870 - val_loss: 0.2403 - val_acc: 0.9603\n",
      "Epoch 499/500\n",
      "42000/42000 [==============================] - 5s 123us/sample - loss: 0.0346 - acc: 0.9897 - val_loss: 0.2398 - val_acc: 0.9604\n",
      "Epoch 500/500\n",
      "42000/42000 [==============================] - 5s 128us/sample - loss: 0.0350 - acc: 0.9886 - val_loss: 0.2390 - val_acc: 0.9606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f135a552630>"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Learning_rate=0.0003221410197800375\n",
    "neurons=[350,250,150,10]\n",
    "Regularize=1.3083277528087247e-07\n",
    "Activation=['relu','relu','relu','softmax']\n",
    "do=0.03572196350233002\n",
    "Optimizer=tf.keras.optimizers.Adam(lr=Learning_rate)\n",
    "\n",
    "\n",
    "Bmodel_2=tf.keras.models.Sequential()\n",
    "Bmodel_2.add(tf.keras.layers.BatchNormalization(input_shape=(1024,)))\n",
    "    \n",
    "for i in range(0,len(neurons)):\n",
    "  Bmodel_2.add(tf.keras.layers.Dense(neurons[i],activation=Activation[i],kernel_regularizer=keras.regularizers.l2(l=Regularize)))\n",
    "  if i != (len(neurons)-1):\n",
    "    Bmodel_2.add(tf.keras.layers.BatchNormalization()) \n",
    "    Bmodel_2.add(tf.keras.layers.Dropout(do))\n",
    "      \n",
    "    \n",
    "Bmodel_2.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "Bmodel_2.fit(nfX_train,HY_train,validation_data=(nfX_val,HY_val),batch_size=128,epochs=500,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DaEXpOmg2Rwm"
   },
   "source": [
    "No accuracy Variation with Hidden batch normalisation in comparision to model 2 in which input layer alone batch normalised . lets drop at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8VY3sST4SI2T"
   },
   "source": [
    "# Evaluation of models in test data.\n",
    "\n",
    "We have created few different models. on SGD, ADAMs with different hyperparameters, and different dataset.\n",
    "\n",
    "models cannot be finetuned by testset as test set is the data set that arrives in production(not known during model builting) and it is the true evaluation of model. so model cannot be biased.Lets evaluate all the proposed models in test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00CXUgE6S5Ig"
   },
   "source": [
    "# Original dataset models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WueAq53rS29p"
   },
   "source": [
    "*Model_1*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yAssAW8iS1xQ",
    "outputId": "362af711-d84e-483c-b41f-a0a1882e1f62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 67us/sample - loss: 1.1014 - acc: 0.8590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1013920131557517, 0.859]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(nfX_test,HY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAl0xmdYU1Jk"
   },
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "WYeV9oUhU1Y6",
    "outputId": "c7a823d0-76c9-4659-b8aa-011cdfcaea7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 140us/sample - loss: 1.1834 - acc: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1833977907043365, 0.86983335]"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(nfX_test,HY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Csdi4f8EUg1u"
   },
   "source": [
    "Model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v6O2-mmLUhVE",
    "outputId": "24d9c1b9-6ab9-403f-cb91-23c28ec79ad3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 68us/sample - loss: 0.5568 - acc: 0.8842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5568494799733162, 0.88416666]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(nfX_test,HY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzbrgGmmT69O"
   },
   "source": [
    "**Hidden layers Batch normalized models**\n",
    "\n",
    "**Bmodel_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "UpFfEMOhUFO9",
    "outputId": "3b761a92-8ecc-479a-becd-ffd9ddeda4b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 147us/sample - loss: 0.7906 - acc: 0.8694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7905780554281341, 0.8693889]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bmodel_2.evaluate(nfX_test,HY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bf7HE_VlTMiH"
   },
   "source": [
    "# Modified Dataset model\n",
    "\n",
    "**nmode1_1**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5Vdr6sFWTkjA",
    "outputId": "647201cb-c527-4fff-840a-c31c02af7de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 145us/sample - loss: 0.5280 - acc: 0.9052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5280279262595706, 0.90522224]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmodel_1.evaluate(nfX_test,HY_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaurpT8oTrt_"
   },
   "source": [
    "**nmode1_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RTMj-PivTpKN",
    "outputId": "3762ec00-cb40-4aa7-b3a5-3485b959d248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 150us/sample - loss: 0.2599 - acc: 0.9608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25985204540048207, 0.96077776]"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmodel_2.evaluate(nfX_test,HY_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S16Wu_yAWDrR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "Model1=['Original','No','SGD',0.0195,4.7e-07,0,0.9837,0.9558,0.859]\n",
    "Model2=['Original','No','ADAMS',0.00032,1.3e-07,0.035,0.9898,0.9607,0.8698]\n",
    "Model3=['Original','No','ADAMS',0.00050,1.4e-06,0.228,0.9510,0.9620,0.8842]\n",
    "bmodel2=['Original','No','ADAMS',0.00032,1.3e-07,0.035,0.9886,9606,0.8694]\n",
    "Nmodel1=['Modified','No','ADAMS', 0.000499,0.00103,0.228,0.8750,0.8995,0.905222]\n",
    "Nmodel2=['Modified','No','ADAMS',0.00032,1.3e-07,0.035,0.9920,0.9461,0.9608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWUks2-Nd_K5"
   },
   "outputs": [],
   "source": [
    "Results=pd.DataFrame({})\n",
    "Results['model1']=Model1\n",
    "Results['model2']=Model2\n",
    "Results['model3']=Model3\n",
    "Results['bmodel2']=bmodel2\n",
    "Results['Nmodel1']=Nmodel1\n",
    "Results['Nmodel2']=Nmodel2\n",
    "Results.set_index(pd.Index(['Data','Hidden batchnormalised','OPtimizer','Learning Rate','L2_reg','Dropout','Train_acc','Val_acc','Test_acc']),inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "cjneXAkMfOSU",
    "outputId": "f645d3bf-8e0d-4b81-bff5-26e687260b38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Hidden batchnormalised</th>\n",
       "      <th>OPtimizer</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>L2_reg</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Train_acc</th>\n",
       "      <th>Val_acc</th>\n",
       "      <th>Test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>Original</td>\n",
       "      <td>No</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>4.7e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9837</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>Original</td>\n",
       "      <td>No</td>\n",
       "      <td>ADAMS</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>1.3e-07</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>0.9607</td>\n",
       "      <td>0.8698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>Original</td>\n",
       "      <td>No</td>\n",
       "      <td>ADAMS</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.4e-06</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.8842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmodel2</th>\n",
       "      <td>Original</td>\n",
       "      <td>No</td>\n",
       "      <td>ADAMS</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>1.3e-07</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.9886</td>\n",
       "      <td>9606</td>\n",
       "      <td>0.8694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nmodel1</th>\n",
       "      <td>Modified</td>\n",
       "      <td>No</td>\n",
       "      <td>ADAMS</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>0.905222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nmodel2</th>\n",
       "      <td>Modified</td>\n",
       "      <td>No</td>\n",
       "      <td>ADAMS</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>1.3e-07</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.9461</td>\n",
       "      <td>0.9608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Data Hidden batchnormalised OPtimizer  ... Train_acc Val_acc  Test_acc\n",
       "model1   Original                     No       SGD  ...    0.9837  0.9558     0.859\n",
       "model2   Original                     No     ADAMS  ...    0.9898  0.9607    0.8698\n",
       "model3   Original                     No     ADAMS  ...     0.951   0.962    0.8842\n",
       "bmodel2  Original                     No     ADAMS  ...    0.9886    9606    0.8694\n",
       "Nmodel1  Modified                     No     ADAMS  ...     0.875  0.8995  0.905222\n",
       "Nmodel2  Modified                     No     ADAMS  ...     0.992  0.9461    0.9608\n",
       "\n",
       "[6 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKQ9WUWWVSbm"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgVqnuaWVY_t"
   },
   "source": [
    "Performance of different models on different datasets shown above\n",
    "\n",
    "\n",
    "*   Models 1,2 and bmodel2 has high accuray in training data nearing 99% and Validation accuracy is also high around 96%. This shows the model is not overfitted. But its performance on the test data is poor (86%). This provide us the conclusion that the training data is not sufficient to capture all the variane in the testdata. Data to be collected for training and we know that validation data has more samples than training, this results could be due to that, Less number of training data\n",
    "\n",
    "*   Model 3 has high dropout regularization than above models, thus training accuracy is low on camparision(95%). but the validaiton(96%) and the test accuracy(88%) is higher on comparision. but still this too suffers from data problem\n",
    "\n",
    "\n",
    "*   nmodel_1 : Modified datamodel (Train data and Val data are combined and split in 70:30 ratio).This model has lower training accuracy(87.5%) and Validation accuracy(89.95%) but higher test accuracy(90.5%) on comparision. Seems like this address the insufficient data problem\n",
    "*   nmodel_3: Modified datamodel with parameters same as model_2. This model has good training accuracy(99.2%) and Validation accuracy(94.6%) but higher test accuracy(96.08%).\n",
    "This shows clearly that there is a issue in data split up. i.e., Insufficient data for training\n",
    "\n",
    "Thus models are built and hyperparameters are fine tuned iteratively based on validation results and its performance on test data is studied. and it is clearly understood that the there is issue in data split which makes it insufficient for training. This issue is over came by combining train and validation set and the split it at 70:30 ratio\n",
    "\n",
    "Depending on which model is chosen for production. the production accuracy would have varied\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_5nd1cqVUlY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Projectsvhn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
